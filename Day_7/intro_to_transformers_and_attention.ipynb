{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98258b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78e21f",
   "metadata": {},
   "source": [
    "### **0. What Are Transformers and Why They Brought the AI Revolution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a96ef",
   "metadata": {},
   "source": [
    "**The Dark Ages Before Transformers (Pre-2017)**\n",
    "\n",
    "Before 2017, the AI landscape looked very different. Let's understand the problems that held back progress:\n",
    "\n",
    "**The RNN Era (1990s-2017):**\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their variants (LSTMs, GRUs) were the go-to architecture for sequential data. But they had **fundamental limitations**:\n",
    "\n",
    "| Problem | Impact | Example |\n",
    "|---------|--------|---------|\n",
    "| **Sequential Processing** | No parallelization, slow training | Processing a 1000-word document takes 1000 sequential steps |\n",
    "| **Vanishing Gradients** | Can't learn long-range dependencies | Struggles to connect \"The cat\" with \"it\" 50 words later |\n",
    "| **Memory Bottleneck** | Information compressed into fixed-size hidden state | Early context gets \"forgotten\" in long sequences |\n",
    "| **Training Time** | Days or weeks on powerful GPUs | A large model might take 2-3 weeks to train |\n",
    "\n",
    "**The Breakthrough: \"Attention Is All You Need\" (2017)**\n",
    "\n",
    "In June 2017, researchers at Google published a paper with a bold claim: **\"Attention Is All You Need\"**. They introduced the **Transformer architecture**, which threw away recurrence entirely and relied solely on attention mechanisms.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*BgYbFWLPOwqk7peFHnPpQw.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Why Transformers Are Revolutionary:**\n",
    "\n",
    "**1. Parallel Processing ‚ö°**\n",
    "\n",
    "Unlike RNNs that process words one-by-one, Transformers process ALL words simultaneously:\n",
    "\n",
    "```\n",
    "RNN approach (SLOW ‚ùå):\n",
    "Step 1: Process word 1 ‚Üí h‚ÇÅ\n",
    "Step 2: Process word 2 ‚Üí h‚ÇÇ (waits for h‚ÇÅ)\n",
    "Step 3: Process word 3 ‚Üí h‚ÇÉ (waits for h‚ÇÇ)\n",
    "...1000 sequential steps...\n",
    "\n",
    "Transformer approach (FAST ‚úÖ):\n",
    "Step 1: Process ALL 1000 words in parallel!\n",
    "```\n",
    "\n",
    "**Impact:** Training time reduced from weeks to days or even hours!\n",
    "\n",
    "**2. Direct Long-Range Connections üîó**\n",
    "\n",
    "Every word can directly attend to every other word, regardless of distance:\n",
    "\n",
    "```\n",
    "RNN: Word 1 ‚Üí Word 50 requires 49 hops (information decays)\n",
    "Transformer: Word 1 ‚Üí Word 50 is just 1 hop (direct connection!)\n",
    "```\n",
    "\n",
    "**Impact:** Models can finally understand long-range dependencies like humans do!\n",
    "\n",
    "**3. Scalability üìà**\n",
    "\n",
    "Transformers scale beautifully with more data and compute:\n",
    "\n",
    "| Model | Parameters | Training Data | Performance |\n",
    "|-------|-----------|---------------|-------------|\n",
    "| GPT-1 (2018) | 117M | 5GB text | Good |\n",
    "| GPT-2 (2019) | 1.5B | 40GB text | Better |\n",
    "| GPT-3 (2020) | 175B | 570GB text | Amazing |\n",
    "| GPT-4 (2023) | ~1.8T | Unknown | Mind-blowing ü§Ø |\n",
    "\n",
    "**The Law:** More parameters + more data = better performance (this didn't work well for RNNs!)\n",
    "\n",
    "**4. Transfer Learning üéì**\n",
    "\n",
    "Pre-train once, fine-tune for many tasks:\n",
    "\n",
    "```\n",
    "Pre-training: Learn language from billions of words\n",
    "    ‚Üì\n",
    "Fine-tuning: Adapt to specific tasks with small datasets\n",
    "    ‚Üì\n",
    "Tasks: Translation, summarization, Q&A, code generation, etc.\n",
    "```\n",
    "\n",
    "**The AI Revolution: What Transformers Enabled**\n",
    "\n",
    "**Before Transformers (2016):**\n",
    "- ‚ùå Machine translation was mediocre\n",
    "- ‚ùå Text generation was incoherent\n",
    "- ‚ùå Chatbots were rule-based and rigid\n",
    "- ‚ùå Code generation didn't exist\n",
    "- ‚ùå Few-shot learning was impossible\n",
    "\n",
    "**After Transformers (2017+):**\n",
    "- ‚úÖ **2017**: Transformer achieves state-of-the-art translation\n",
    "- ‚úÖ **2018**: BERT revolutionizes NLP tasks (question answering, sentiment analysis)\n",
    "- ‚úÖ **2019**: GPT-2 generates surprisingly coherent text\n",
    "- ‚úÖ **2020**: GPT-3 enables few-shot learning (learns from examples without fine-tuning!)\n",
    "- ‚úÖ **2021**: Codex powers GitHub Copilot (AI pair programmer)\n",
    "- ‚úÖ **2022**: ChatGPT becomes mainstream (100M users in 2 months!)\n",
    "- ‚úÖ **2023**: GPT-4, Claude, LLaMA - AI assistants everywhere\n",
    "- ‚úÖ **2024+**: Transformers power vision (ViT), multimodal models (GPT-4V), protein folding (AlphaFold2)\n",
    "\n",
    "**Beyond Text: Transformers Everywhere**\n",
    "\n",
    "The Transformer architecture isn't just for language anymore:\n",
    "\n",
    "| Domain | Application | Example Models |\n",
    "|--------|-------------|----------------|\n",
    "| **Language** | Text generation, translation, chatbots | GPT-4, Claude, LLaMA |\n",
    "| **Vision** | Image classification, object detection | Vision Transformer (ViT), DALL-E |\n",
    "| **Speech** | Speech recognition, synthesis | Whisper, Wav2Vec 2.0 |\n",
    "| **Biology** | Protein structure prediction | AlphaFold 2 |\n",
    "| **Chemistry** | Molecule generation | ChemBERTa |\n",
    "| **Code** | Code completion, generation | Copilot, CodeGen |\n",
    "| **Multimodal** | Image+text understanding | GPT-4V, Flamingo |\n",
    "\n",
    "**The Numbers Don't Lie:**\n",
    "\n",
    "**Research Impact:**\n",
    "- üìÑ \"Attention Is All You Need\" has **80,000+ citations** (one of the most cited AI papers ever)\n",
    "- üìà Over **70% of top AI papers** now use Transformers\n",
    "- üèÜ Transformers power **every major AI breakthrough** since 2017\n",
    "\n",
    "**Real-World Impact:**\n",
    "- üí∞ **$100B+ market** for LLM applications\n",
    "- üë• **Billions of users** interact with Transformer-based AI daily\n",
    "- üöÄ **10,000+ startups** building on Transformer foundation models\n",
    "- üè¢ Every tech giant (Google, Meta, Microsoft, OpenAI) bets on Transformers\n",
    "\n",
    "**Why You Need to Understand Transformers:**\n",
    "\n",
    "1. **Career relevance**: Transformers are the foundation of modern AI jobs\n",
    "2. **Problem-solving**: Enable solutions that were impossible before\n",
    "3. **Innovation**: Understanding Transformers lets you build the next breakthrough\n",
    "4. **Universal architecture**: One architecture that works across domains\n",
    "\n",
    "**What Makes Transformers Special:**\n",
    "\n",
    "üéØ **Attention mechanism**: The secret sauce we'll learn today\n",
    "üéØ **Positional encoding**: How to handle sequence order without recurrence\n",
    "üéØ **Layer normalization**: Stable training for deep networks\n",
    "üéØ **Residual connections**: Enable training of 100+ layer models\n",
    "üéØ **Feed-forward networks**: Add non-linearity and capacity\n",
    "\n",
    "**The Bottom Line:**\n",
    "\n",
    "Transformers didn't just improve AI - they **transformed it** (pun intended! üòÑ). They are the reason why:\n",
    "- ChatGPT can have human-like conversations\n",
    "- DALL-E can generate photorealistic images from text\n",
    "- GitHub Copilot can write code with you\n",
    "- AlphaFold can predict protein structures\n",
    "- Modern AI can do things that seemed like science fiction just 5 years ago\n",
    "\n",
    "**Today's goal:** Understand the **attention mechanism** - the core innovation that made all of this possible!\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9f3ab",
   "metadata": {},
   "source": [
    "### **1. What is Attention? The Human Perspective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06843b3f",
   "metadata": {},
   "source": [
    "Before diving into the mathematics, let's understand attention through something you do naturally every day: **paying attention**.\n",
    "\n",
    "**Example 1: Reading This Sentence**\n",
    "\n",
    "When you read the sentence: *\"The cat, which was fluffy and orange, sat on the mat\"*, your brain doesn't process each word in isolation. When you reach the word \"sat\", you automatically:\n",
    "- ‚úÖ Remember that \"cat\" is the subject (even though it's far away)\n",
    "- ‚úÖ Ignore the descriptive details (\"fluffy and orange\")\n",
    "- ‚úÖ Connect \"sat\" with \"cat\" for subject-verb agreement\n",
    "- ‚úÖ Understand \"mat\" is the location\n",
    "\n",
    "You **selectively attend** to relevant words while reading!\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*7hbg6pGtFgYRakP_GZVF4A.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "**Example 2: Listening in a Crowded Room**\n",
    "\n",
    "Imagine you're at a party with multiple conversations happening. You can:\n",
    "- üéØ Focus on your friend's voice (high attention weight)\n",
    "- üîá Tune out background noise (low attention weight)\n",
    "- üëÇ Shift attention when you hear your name elsewhere\n",
    "\n",
    "This is the **Cocktail Party Effect** ‚Äì your brain dynamically adjusts attention weights!\n",
    "\n",
    "**Example 3: Looking at a Photograph**\n",
    "\n",
    "When shown a photo and asked \"Where is the dog?\", your eyes:\n",
    "- üëÄ Scan the entire image quickly\n",
    "- üéØ Focus intensely on regions with dog-like features\n",
    "- ‚ö° Process all regions in parallel (not sequentially!)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://i.pinimg.com/736x/fc/e6/a6/fce6a68b9dfcb1de76e1b477294ad0f2.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**The Key Insight**\n",
    "\n",
    "In all these examples, you:\n",
    "1. Have access to **all information simultaneously** (parallel processing)\n",
    "2. Assign different **importance weights** to different pieces of information\n",
    "3. Combine information based on **relevance to your current goal**\n",
    "\n",
    "This is exactly what **attention mechanisms** do for neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965dff00",
   "metadata": {},
   "source": [
    "### **2. The Core Principle: Weighted Information Aggregation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6eceb",
   "metadata": {},
   "source": [
    "**From Human Intuition to Mathematical Formulation**\n",
    "\n",
    "Let's formalize what we just observed. Attention mechanisms compute a weighted sum of values, where the weights represent **how much attention** to pay to each element.\n",
    "\n",
    "**Simple Example: Computing Average Grade**\n",
    "\n",
    "Imagine you have three test scores: [85, 90, 95]\n",
    "\n",
    "**Uniform Attention** (equal weights):\n",
    "$$\\text{Average} = \\frac{1}{3}(85) + \\frac{1}{3}(90) + \\frac{1}{3}(95) = 90$$\n",
    "\n",
    "**Weighted Attention** (finals count more):\n",
    "$$\\text{Weighted} = 0.2(85) + 0.3(90) + 0.5(95) = 91.5$$\n",
    "\n",
    "The weights [0.2, 0.3, 0.5] represent **how much attention** to pay to each score!\n",
    "\n",
    "**Generalizing to Sequences**\n",
    "\n",
    "For a sequence of words $[w_1, w_2, ..., w_n]$, attention computes:\n",
    "\n",
    "$$\\text{Output}_i = \\sum_{j=1}^{n} \\alpha_{ij} \\cdot \\text{value}_j$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_{ij}$ = attention weight from word $i$ to word $j$ (\"How much should word $i$ attend to word $j$?\")\n",
    "- $\\text{value}_j$ = the information content of word $j$\n",
    "- $\\sum_{j=1}^{n} \\alpha_{ij} = 1$ (weights sum to 1, like probabilities)\n",
    "\n",
    "**The Magic: Dynamic Weights**\n",
    "\n",
    "Unlike RNNs where information flows sequentially, attention weights are:\n",
    "- ‚ö° **Computed dynamically** based on content (not position)\n",
    "- üîó **Connect any two words directly** (no sequential bottleneck)\n",
    "- üìä **Different for each word** (context-dependent)\n",
    "- üéØ **Learned during training** (optimized for the task)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*1K4x4M_i-JiJAVSqBWIvBQ.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41f9c2",
   "metadata": {},
   "source": [
    "### **3. A Concrete Example: Understanding \"The Animal Didn't Cross the Street Because It Was Too Tired\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b86cc",
   "metadata": {},
   "source": [
    "Let's see attention in action with a real sentence!\n",
    "\n",
    "**The Challenge:**\n",
    "\n",
    "What does \"it\" refer to in this sentence?\n",
    "\n",
    "*\"The animal didn't cross the street because it was too tired.\"*\n",
    "\n",
    "Possible interpretations:\n",
    "1. \"it\" = the animal (makes sense! ‚úÖ)\n",
    "2. \"it\" = the street (doesn't make sense ‚ùå)\n",
    "\n",
    "**How Attention Solves This:**\n",
    "\n",
    "When processing the word \"it\", an attention mechanism:\n",
    "\n",
    "| Query Token | Word | Attention Weight | Reasoning |\n",
    "|---|---|---|---|\n",
    "| it | The | 0.02 | Not relevant to \"it\" |\n",
    "| it | **animal** | **0.65** | üéØ **High attention! Likely referent** |\n",
    "| it | didn't | 0.01 | Grammar word, not relevant |\n",
    "| it | cross | 0.03 | Verb, not a noun |\n",
    "| it | the | 0.01 | Not relevant |\n",
    "| it | street | 0.15 | Possible but unlikely |\n",
    "| it | because | 0.01 | Connector word |\n",
    "| it | it | 0.05 | Self-reference |\n",
    "| it | was | 0.01 | Grammar word |\n",
    "| it | too | 0.01 | Modifier |\n",
    "| it | tired | 0.05 | Adjective, gives context |\n",
    "\n",
    "**Attention Weight Visualization:**\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://cdn.prod.website-files.com/65c4ab17d1f4702114123723/662b32066c6cfbca695875fd_image-png-Aug-02-2023-02-02-22-8493-PM.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "The model learns that:\n",
    "- Pronouns should attend heavily to nouns\n",
    "- Semantic compatibility matters (animals get tired, streets don't)\n",
    "- Recent nouns get more weight (recency bias)\n",
    "\n",
    "**The Attention Output:**\n",
    "\n",
    "$$\\text{it}_{\\text{representation}} = 0.65 \\cdot \\text{animal} + 0.15 \\cdot \\text{street} + \\text{(small contributions from others)}$$\n",
    "\n",
    "The representation of \"it\" is now **strongly influenced by \"animal\"** ‚Äì the model has successfully resolved the reference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ab920",
   "metadata": {},
   "source": [
    "### **4. Why This Is Revolutionary: Parallel Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c540d33",
   "metadata": {},
   "source": [
    "**The Sequential Bottleneck (RNNs)**\n",
    "\n",
    "Remember from Day 6 how RNNs process sequences:\n",
    "\n",
    "```\n",
    "Step 1: Process \"The\"     ‚Üí h‚ÇÅ\n",
    "Step 2: Process \"animal\"   ‚Üí h‚ÇÇ (depends on h‚ÇÅ) ‚è≥\n",
    "Step 3: Process \"didn't\"   ‚Üí h‚ÇÉ (depends on h‚ÇÇ) ‚è≥\n",
    "Step 4: Process \"cross\"    ‚Üí h‚ÇÑ (depends on h‚ÇÉ) ‚è≥\n",
    "...and so on sequentially...\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- ‚ùå Must wait for previous steps (no parallelization)\n",
    "- ‚ùå Information from \"The\" gets diluted by step 10\n",
    "- ‚ùå Slow training (can't process words in parallel)\n",
    "- ‚ùå Long-range dependencies are hard to learn\n",
    "\n",
    "**The Attention Revolution**\n",
    "\n",
    "Attention processes ALL words simultaneously:\n",
    "\n",
    "```\n",
    "Step 1: Process ALL words in parallel ‚ö°\n",
    "        ‚Üì\n",
    "    [The, animal, didn't, cross, the, street, because, it, was, too, tired]\n",
    "        ‚Üì\n",
    "Step 2: Compute attention between ALL pairs ‚ö°\n",
    "        ‚Üì\n",
    "    Every word directly attends to every other word!\n",
    "        ‚Üì\n",
    "Step 3: Weighted aggregation ‚ö°\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **Massive parallelization** (GPU utilization ~90% vs ~30% for RNNs)\n",
    "- ‚úÖ **Direct connections** between any two words (no information decay)\n",
    "- ‚úÖ **Constant path length** (word 1 ‚Üí word 50 is just 1 hop, not 49!)\n",
    "- ‚úÖ **Faster training** (10-100x speedup on modern hardware)\n",
    "\n",
    "**Visual Comparison:**\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-08.jpg\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "**The Numbers:**\n",
    "\n",
    "For a sequence of length $n$:\n",
    "\n",
    "| Metric | RNN | Attention |\n",
    "|--------|-----|----------|\n",
    "| Path length (word 1 ‚Üí word n) | $O(n)$ | $O(1)$ üéØ |\n",
    "| Operations per layer | $O(n)$ | $O(n^2)$ |\n",
    "| Parallelizable? | ‚ùå No | ‚úÖ Yes |\n",
    "| GPU efficiency | Low (~30%) | High (~90%) |\n",
    "| Training time (100K steps) | ~10 hours | ~1-2 hours ‚ö° |\n",
    "\n",
    "**Note:** While attention has $O(n^2)$ complexity, the parallelization benefits far outweigh this for sequences up to ~2000 tokens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb1072",
   "metadata": {},
   "source": [
    "### **5. The Intuition Builder: A Simple Analogy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b11c6b",
   "metadata": {},
   "source": [
    "Let's solidify your understanding with a library analogy:\n",
    "\n",
    "**üèõÔ∏è The Library Analogy**\n",
    "\n",
    "Imagine you're researching \"climate change impacts\" in a library:\n",
    "\n",
    "**Your Query:** \"How does climate change affect polar bears?\"\n",
    "\n",
    "**The Library Catalog (Keys):**\n",
    "- Book 1: \"Climate Change Overview\" üîë\n",
    "- Book 2: \"Polar Bear Biology\" üîë\n",
    "- Book 3: \"Arctic Ecosystems\" üîë\n",
    "- Book 4: \"17th Century Poetry\" üîë\n",
    "- Book 5: \"Ocean Acidification\" üîë\n",
    "\n",
    "**What You Do:**\n",
    "\n",
    "1. **Compare Query with Keys** (Matching step)\n",
    "   - Your query ‚Üî \"Climate Change Overview\": High relevance! ‚úÖ\n",
    "   - Your query ‚Üî \"Polar Bear Biology\": High relevance! ‚úÖ\n",
    "   - Your query ‚Üî \"Arctic Ecosystems\": Medium relevance ‚úì\n",
    "   - Your query ‚Üî \"17th Century Poetry\": No relevance ‚ùå\n",
    "   - Your query ‚Üî \"Ocean Acidification\": Low relevance\n",
    "\n",
    "2. **Assign Attention Weights** (based on relevance)\n",
    "   - Book 1: 0.35 (35% attention)\n",
    "   - Book 2: 0.40 (40% attention) üéØ\n",
    "   - Book 3: 0.20 (20% attention)\n",
    "   - Book 4: 0.00 (0% attention)\n",
    "   - Book 5: 0.05 (5% attention)\n",
    "\n",
    "3. **Read Content (Values) Proportionally**\n",
    "   - Spend 40% of your time on \"Polar Bear Biology\"\n",
    "   - Spend 35% on \"Climate Change Overview\"\n",
    "   - Spend 20% on \"Arctic Ecosystems\"\n",
    "   - Skip \"17th Century Poetry\" entirely\n",
    "   - Briefly skim \"Ocean Acidification\"\n",
    "\n",
    "4. **Synthesize Information** (Weighted aggregation)\n",
    "   - Your final understanding = \n",
    "     - 0.40 √ó (Polar Bear content) +\n",
    "     - 0.35 √ó (Climate content) +\n",
    "     - 0.20 √ó (Arctic content) +\n",
    "     - 0.05 √ó (Ocean content)\n",
    "\n",
    "**Mapping to Attention:**\n",
    "\n",
    "| Library Concept | Attention Mechanism |\n",
    "|----------------|--------------------|\n",
    "| Your research question | **Query (Q)** |\n",
    "| Book titles in catalog | **Keys (K)** |\n",
    "| Book contents | **Values (V)** |\n",
    "| Relevance matching | **Q¬∑K (dot product)** |\n",
    "| Time allocation | **Attention weights (Œ±)** |\n",
    "| Final understanding | **Output (weighted sum of V)** |\n",
    "\n",
    "**The Formula Revealed:**\n",
    "\n",
    "$$\\text{Understanding} = \\sum_{i} \\alpha_i \\cdot \\text{Book}_i$$\n",
    "\n",
    "$$\\text{where } \\alpha_i = \\text{softmax}(\\frac{\\text{Query} \\cdot \\text{Key}_i}{\\sqrt{d}})$$\n",
    "\n",
    "This is **exactly** how attention mechanisms work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac0877",
   "metadata": {},
   "source": [
    "### **6. Scaled Dot-Product Attention: The Mathematical Heart**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ec407",
   "metadata": {},
   "source": [
    "Now let's dive into the actual mechanism! The **scaled dot-product attention** is the fundamental building block of the Transformer architecture.\n",
    "\n",
    "**The Complete Formula:**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "This looks intimidating, but let's break it down step-by-step!\n",
    "\n",
    "**Step-by-Step Breakdown:**\n",
    "\n",
    "**Step 1: Compute Similarity Scores (Q¬∑K^T)**\n",
    "\n",
    "Think of this as \"how relevant is each key to each query?\"\n",
    "\n",
    "$$\\text{Scores} = QK^T$$\n",
    "\n",
    "- **Q** (Query): \"What am I looking for?\" - Shape: $(n, d_k)$\n",
    "- **K** (Key): \"What do I offer?\" - Shape: $(n, d_k)$\n",
    "- **Result**: Similarity matrix - Shape: $(n, n)$\n",
    "\n",
    "**Intuition:** Dot product measures similarity (like cosine similarity). High dot product = high relevance!\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"it\"  ‚Üí  [0.2, 0.8, 0.3]\n",
    "Key: \"animal\"  ‚Üí  [0.3, 0.9, 0.2]\n",
    "Similarity = 0.2√ó0.3 + 0.8√ó0.9 + 0.3√ó0.2 = 0.84 (High! ‚úÖ)\n",
    "\n",
    "Query: \"it\"  ‚Üí  [0.2, 0.8, 0.3]\n",
    "Key: \"street\"  ‚Üí  [0.7, 0.1, 0.5]\n",
    "Similarity = 0.2√ó0.7 + 0.8√ó0.1 + 0.3√ó0.5 = 0.37 (Low ‚ùå)\n",
    "```\n",
    "\n",
    "**Step 2: Scale by ‚àöd_k**\n",
    "\n",
    "$$\\text{Scaled Scores} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "**Why scaling?** This is CRUCIAL! Let's see why:\n",
    "\n",
    "**Problem Without Scaling:**\n",
    "\n",
    "As the dimension $d_k$ increases, dot products grow larger in magnitude:\n",
    "\n",
    "| Dimension | Example Dot Product | Softmax Behavior |\n",
    "|-----------|-------------------|------------------|\n",
    "| $d_k = 2$ | 0.84 | Soft distribution ‚úÖ |\n",
    "| $d_k = 64$ | 15.2 | Starts peaking üòê |\n",
    "| $d_k = 512$ | 45.8 | **Extremely peaked** üö® |\n",
    "\n",
    "When dot products are too large, softmax pushes almost all probability to one element:\n",
    "\n",
    "```python\n",
    "# Without scaling (d_k = 512)\n",
    "scores = [45.8, 12.3, 8.1]\n",
    "softmax(scores) = [0.9999, 0.0001, 0.0000]  # Almost one-hot! üö®\n",
    "\n",
    "# With scaling (divide by ‚àö512 ‚âà 22.6)\n",
    "scaled = [2.03, 0.54, 0.36]\n",
    "softmax(scaled) = [0.65, 0.20, 0.15]  # Nice distribution! ‚úÖ\n",
    "```\n",
    "\n",
    "**The Math Behind It:**\n",
    "\n",
    "For a $d_k$-dimensional random vector with unit variance, the dot product has variance $d_k$. Dividing by $\\sqrt{d_k}$ normalizes the variance back to 1:\n",
    "\n",
    "$$\\text{Var}(QK^T) = d_k \\implies \\text{Var}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) = 1$$\n",
    "\n",
    "This keeps gradients healthy and prevents **vanishing gradients** during training!\n",
    "\n",
    "**Step 3: Apply Softmax**\n",
    "\n",
    "$$\\text{Attention Weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "Softmax converts scores to probabilities (all positive, sum to 1):\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp(s_{ij})}{\\sum_{k=1}^{n} \\exp(s_{ik})}$$\n",
    "\n",
    "**Properties:**\n",
    "- ‚úÖ All weights between 0 and 1\n",
    "- ‚úÖ Sum of weights = 1 (like probabilities)\n",
    "- ‚úÖ Differentiable (enables backpropagation)\n",
    "- ‚úÖ \"Soft\" selection (vs hard argmax)\n",
    "\n",
    "**Step 4: Weighted Sum of Values**\n",
    "\n",
    "$$\\text{Output} = \\text{Attention Weights} \\times V$$\n",
    "\n",
    "Finally, we aggregate the values using our computed attention weights:\n",
    "\n",
    "$$\\text{Output}_i = \\sum_{j=1}^{n} \\alpha_{ij} \\cdot V_j$$\n",
    "\n",
    "- **V** (Value): \"What information do I provide?\" - Shape: $(n, d_v)$\n",
    "- **Output**: Contextualized representation - Shape: $(n, d_v)$\n",
    "\n",
    "**The Complete Picture:**\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Fba830026-6d8f-4e77-b288-f75dd3a51457%2Fimage.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0cfcd",
   "metadata": {},
   "source": [
    "Let's implement scaled dot-product attention in PyTorch to truly understand what's happening!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e260c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention_pytorch(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention (PyTorch version with batching)\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch_size, seq_len, d_k)\n",
    "        K: Key tensor (batch_size, seq_len, d_k)\n",
    "        V: Value tensor (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask (batch_size, seq_len, seq_len) or (seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len, d_v)\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Get dimension for scaling\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1 & 2: Compute scaled scores\n",
    "    # Q: (batch, seq_len, d_k)\n",
    "    # K.transpose: (batch, d_k, seq_len)\n",
    "    # scores: (batch, seq_len, seq_len)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 4: Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test with batched input\n",
    "print(\"=\"*60)\n",
    "print(\"PYTORCH IMPLEMENTATION TEST\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "# Create random Q, K, V for a batch\n",
    "torch.manual_seed(42)\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"Q: {Q.shape} (batch_size, seq_len, d_k)\")\n",
    "print(f\"K: {K.shape}\")\n",
    "print(f\"V: {V.shape}\")\n",
    "print()\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights = scaled_dot_product_attention_pytorch(Q, K, V)\n",
    "\n",
    "print(f\"Output shapes:\")\n",
    "print(f\"Output: {output.shape} (batch_size, seq_len, d_v)\")\n",
    "print(f\"Attention weights: {attention_weights.shape} (batch_size, seq_len, seq_len)\")\n",
    "print()\n",
    "\n",
    "print(f\"Attention weights for first sample:\")\n",
    "print(attention_weights[0].detach().numpy())\n",
    "print()\n",
    "print(f\"Row sums (should be ~1.0): {attention_weights[0].sum(dim=1).detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452d159",
   "metadata": {},
   "source": [
    "**Visualizing Batch Attention:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for both samples in the batch\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx in range(2):\n",
    "    sns.heatmap(attention_weights[idx].detach().numpy(),\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                cmap='YlOrRd',\n",
    "                ax=axes[idx],\n",
    "                cbar_kws={'label': 'Attention'})\n",
    "    axes[idx].set_title(f'Batch Sample {idx+1}\\nAttention Weights')\n",
    "    axes[idx].set_xlabel('Key Position')\n",
    "    axes[idx].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcaf003",
   "metadata": {},
   "source": [
    "### **7. Multi-Head Attention: Multiple Perspectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56113e88",
   "metadata": {},
   "source": [
    "**The \"Why Multiple Perspectives?\" Analogy**\n",
    "\n",
    "Imagine you're analyzing a movie review:\n",
    "\n",
    "*\"The cinematography was breathtaking, but the plot felt rushed and the acting was mediocre.\"*\n",
    "\n",
    "**Different Experts Analyzing the Same Text:**\n",
    "\n",
    "| Expert | Focus | What They Notice |\n",
    "|--------|-------|-----------------|\n",
    "| **Syntax Expert** | Grammar structure | Subject-verb relationships, conjunctions |\n",
    "| **Sentiment Expert** | Emotional tone | \"breathtaking\" (positive), \"rushed\" (negative) |\n",
    "| **Entity Expert** | Key concepts | \"cinematography\", \"plot\", \"acting\" |\n",
    "| **Dependency Expert** | Long-range links | \"but\" connects contrasting ideas |\n",
    "\n",
    "Each expert looks at the SAME text but focuses on DIFFERENT patterns!\n",
    "\n",
    "This is exactly what **Multi-Head Attention** does ‚Äì it runs multiple attention mechanisms in parallel, each learning to focus on different aspects of the input.\n",
    "\n",
    "**The Key Insight:**\n",
    "\n",
    "Instead of having one attention mechanism with large dimensions, we split it into multiple smaller \"heads\":\n",
    "\n",
    "- **Single-Head:** One 512-dimensional attention ‚ö†Ô∏è\n",
    "- **Multi-Head (8 heads):** Eight 64-dimensional attentions ‚úÖ\n",
    "\n",
    "Each head can learn a different attention pattern:\n",
    "- üéØ Head 1: Syntactic dependencies (subject-verb)\n",
    "- üéØ Head 2: Semantic relationships (similar words)\n",
    "- üéØ Head 3: Positional patterns (adjacent words)\n",
    "- üéØ Head 4: Long-range dependencies\n",
    "- ... and so on\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://velog.velcdn.com/images/jhyunee/post/c48e0195-6443-4156-bccd-844599d7c9d2/image.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "Where each head is:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "**Breaking Down the Process:**\n",
    "\n",
    "1. **Project**: Each head gets its own projection matrices $W^Q_i, W^K_i, W^V_i$\n",
    "2. **Attend**: Each head computes attention independently\n",
    "3. **Concatenate**: Combine all head outputs\n",
    "4. **Project Again**: Final linear transformation $W^O$\n",
    "\n",
    "**Dimensions Flow:**\n",
    "\n",
    "```\n",
    "Input: (batch, seq_len, d_model=512)\n",
    "   ‚Üì\n",
    "For each of h=8 heads:\n",
    "   ‚Üì Project to d_k = d_model/h = 64\n",
    "   Q, K, V: (batch, seq_len, 64)\n",
    "   ‚Üì Attention\n",
    "   head_i: (batch, seq_len, 64)\n",
    "   ‚Üì\n",
    "Concatenate all 8 heads:\n",
    "   ‚Üì\n",
    "(batch, seq_len, 8√ó64=512)\n",
    "   ‚Üì Final projection W^O\n",
    "(batch, seq_len, d_model=512)\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "\n",
    "- ‚úÖ **Representational diversity**: Different heads capture different patterns\n",
    "- ‚úÖ **Parallel computation**: All heads run simultaneously (GPU efficient!)\n",
    "- ‚úÖ **Same computational cost**: 8 heads √ó 64 dims ‚âà 1 head √ó 512 dims\n",
    "- ‚úÖ **Redundancy**: If one head fails to learn, others can compensate\n",
    "- ‚úÖ **Interpretability**: Can visualize what each head learned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82463d94",
   "metadata": {},
   "source": [
    "### **8 Implementing Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55768ab",
   "metadata": {},
   "source": [
    "Let's build a complete Multi-Head Attention module as an `nn.Module`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism\n",
    "    \n",
    "    Args:\n",
    "        d_model: Total dimension of the model (e.g., 512)\n",
    "        num_heads: Number of attention heads (e.g., 8)\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V (one for each)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # For visualization\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k)\n",
    "        \n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        # Reshape to (batch_size, seq_len, num_heads, d_k)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        # Transpose to (batch_size, num_heads, seq_len, d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Inverse of split_heads\n",
    "        \n",
    "        Args:\n",
    "            x: (batch_size, num_heads, seq_len, d_k)\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        # Transpose to (batch_size, seq_len, num_heads, d_k)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        # Reshape to (batch_size, seq_len, d_model)\n",
    "        return x.view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            Q: Query tensor (batch_size, seq_len, d_model)\n",
    "            K: Key tensor (batch_size, seq_len, d_model)\n",
    "            V: Value tensor (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask (batch_size, 1, seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        Q = self.W_q(Q)  # (batch_size, seq_len, d_model)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # 2. Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # 3. Scaled dot-product attention for all heads in parallel\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        # (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Store for visualization\n",
    "        self.attention_weights = attention_weights.detach()\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        # (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 5. Final linear projection\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test the MultiHeadAttention module\n",
    "print(\"=\"*60)\n",
    "print(\"MULTI-HEAD ATTENTION TEST\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Create module\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create random input\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass (self-attention: Q=K=V)\n",
    "output = mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "print()\n",
    "\n",
    "print(f\"Architecture details:\")\n",
    "print(f\"- Model dimension (d_model): {d_model}\")\n",
    "print(f\"- Number of heads: {num_heads}\")\n",
    "print(f\"- Dimension per head (d_k): {d_model // num_heads}\")\n",
    "print(f\"- Attention weights shape: {mha.attention_weights.shape}\")\n",
    "print(f\"  (batch_size, num_heads, seq_len, seq_len)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd688771",
   "metadata": {},
   "source": [
    "### **9 Visualizing Different Head Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac160b4b",
   "metadata": {},
   "source": [
    "Let's visualize what different attention heads learn to focus on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns from different heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get attention weights for first sample in batch\n",
    "attention = mha.attention_weights[0]  # (num_heads, seq_len, seq_len)\n",
    "\n",
    "for head in range(num_heads):\n",
    "    ax = axes[head]\n",
    "    sns.heatmap(attention[head].numpy(),\n",
    "                cmap='YlOrRd',\n",
    "                ax=ax,\n",
    "                cbar=True,\n",
    "                square=True)\n",
    "    ax.set_title(f'Head {head+1}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: 8 Different Attention Patterns', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Observation:\")\n",
    "print(\"Each head learns DIFFERENT attention patterns!\")\n",
    "print(\"- Some heads might focus on local patterns (diagonal)\")\n",
    "print(\"- Some heads might focus on specific positions\")\n",
    "print(\"- Some heads might have diffuse attention (uniform)\")\n",
    "print(\"\\nThis diversity allows the model to capture multiple relationships simultaneously!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a356239",
   "metadata": {},
   "source": [
    "### **10. Attention Masking Strategies: Controlling Information Flow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327aafc",
   "metadata": {},
   "source": [
    "Masking is CRITICAL for making attention mechanisms work correctly in real applications. Let's understand why and how!\n",
    "\n",
    "**Why Do We Need Masking?**\n",
    "\n",
    "Consider two fundamental problems:\n",
    "\n",
    "**Problem 1: Padding Tokens üö´**\n",
    "\n",
    "When batching sequences of different lengths, we pad shorter sequences:\n",
    "\n",
    "```\n",
    "Sentence 1: \"I love AI\" (3 tokens)\n",
    "Sentence 2: \"Deep learning is amazing\" (4 tokens)\n",
    "\n",
    "Padded batch:\n",
    "[\"I\", \"love\", \"AI\", <PAD>]\n",
    "[\"Deep\", \"learning\", \"is\", \"amazing\"]\n",
    "```\n",
    "\n",
    "**Issue:** We don't want attention to focus on meaningless `<PAD>` tokens!\n",
    "\n",
    "**Problem 2: Future Information Leakage üîÆ**\n",
    "\n",
    "When training language models to predict the next word:\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the\"\n",
    "Target: \"mat\"\n",
    "```\n",
    "\n",
    "During training, if position 3 (\"sat\") can attend to position 6 (\"mat\"), the model **cheats** by seeing future words it shouldn't know yet!\n",
    "\n",
    "**Solution:** Attention masking! üé≠\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Z2iYe3L52m4B7HNcLxfKuA.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ef728",
   "metadata": {},
   "source": [
    "### **11 Padding Mask: Ignoring Padded Tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24a38e",
   "metadata": {},
   "source": [
    "**The Padding Mask Strategy:**\n",
    "\n",
    "For padded positions, set attention scores to $-\\infty$ (or a very large negative number like $-10^9$) BEFORE applying softmax:\n",
    "\n",
    "$$\\text{scores}_{\\text{masked}} = \\begin{cases} \n",
    "\\text{score}_{ij} & \\text{if position } j \\text{ is not padded} \\\\\n",
    "-\\infty & \\text{if position } j \\text{ is padded}\n",
    "\\end{cases}$$\n",
    "\n",
    "After softmax: $\\text{softmax}(-\\infty) = 0$ ‚úÖ\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "```\n",
    "Original sequence: [\"I\", \"love\", \"AI\", <PAD>]\n",
    "                     1     1     1     0     ‚Üê Mask (1=real, 0=pad)\n",
    "\n",
    "Attention scores before masking:\n",
    "          I    love   AI    <PAD>\n",
    "    I   [0.5   0.3   0.2    0.4]\n",
    "  love  [0.1   0.6   0.1    0.2]\n",
    "   AI   [0.3   0.2   0.4    0.3]\n",
    "  <PAD> [0.2   0.3   0.2    0.5]\n",
    "\n",
    "After masking <PAD> column:\n",
    "          I    love   AI    <PAD>\n",
    "    I   [0.5   0.3   0.2    -‚àû]\n",
    "  love  [0.1   0.6   0.1    -‚àû]\n",
    "   AI   [0.3   0.2   0.4    -‚àû]\n",
    "  <PAD> [0.2   0.3   0.2    -‚àû]\n",
    "\n",
    "After softmax (normalized excluding <PAD>):\n",
    "          I    love   AI    <PAD>\n",
    "    I   [0.50  0.30  0.20   0.00] ‚úÖ\n",
    "  love  [0.12  0.75  0.12   0.00] ‚úÖ\n",
    "   AI   [0.33  0.22  0.44   0.00] ‚úÖ\n",
    "  <PAD> [0.33  0.41  0.27   0.00] ‚úÖ\n",
    "```\n",
    "\n",
    "**Key Point:** The padded positions receive ZERO attention weight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2beb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, pad_token=0):\n",
    "    \"\"\"\n",
    "    Create padding mask\n",
    "    \n",
    "    Args:\n",
    "        seq: Input sequence (batch_size, seq_len)\n",
    "        pad_token: Token id used for padding\n",
    "    \n",
    "    Returns:\n",
    "        mask: Padding mask (batch_size, 1, 1, seq_len)\n",
    "    \"\"\"\n",
    "    # Create mask: 1 for real tokens, 0 for padding\n",
    "    mask = (seq != pad_token).unsqueeze(1).unsqueeze(2)\n",
    "    return mask.float()\n",
    "\n",
    "# Example: Batch with padding\n",
    "print(\"=\"*60)\n",
    "print(\"PADDING MASK DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Create a batch where sequences have different lengths\n",
    "# 0 represents <PAD> token\n",
    "sequences = torch.tensor([\n",
    "    [5, 8, 3, 2, 0, 0],  # Length 4 (2 pads)\n",
    "    [7, 4, 9, 1, 6, 2],  # Length 6 (0 pads)\n",
    "    [3, 8, 0, 0, 0, 0],  # Length 2 (4 pads)\n",
    "])\n",
    "\n",
    "print(\"Input sequences (0 = <PAD>):\")\n",
    "print(sequences)\n",
    "print()\n",
    "\n",
    "# Create padding mask\n",
    "pad_mask = create_padding_mask(sequences, pad_token=0)\n",
    "print(f\"Padding mask shape: {pad_mask.shape}\")\n",
    "print(\"Padding mask (1=real, 0=pad):\")\n",
    "print(pad_mask.squeeze())\n",
    "print()\n",
    "\n",
    "# Create dummy Q, K, V for demonstration\n",
    "batch_size, seq_len = sequences.shape\n",
    "d_model = 8\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Compute attention WITHOUT mask\n",
    "output_no_mask, attn_no_mask = scaled_dot_product_attention_pytorch(Q, K, V, mask=None)\n",
    "\n",
    "# Compute attention WITH padding mask\n",
    "output_with_mask, attn_with_mask = scaled_dot_product_attention_pytorch(Q, K, V, mask=pad_mask)\n",
    "\n",
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without mask\n",
    "sns.heatmap(attn_no_mask[0].detach().numpy(),\n",
    "            annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Without Padding Mask\\n(Attends to <PAD> tokens! ‚ùå)')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "# With mask\n",
    "sns.heatmap(attn_with_mask[0].detach().numpy(),\n",
    "            annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('With Padding Mask\\n(Ignores <PAD> tokens! ‚úÖ)')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "# Mark padded positions\n",
    "for ax in axes:\n",
    "    ax.axvline(x=4, color='blue', linewidth=2, linestyle='--', label='Padding starts')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Notice: With padding mask, columns 5-6 (padded positions) have ZERO attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d3b11",
   "metadata": {},
   "source": [
    "### **12 Causal/Look-Ahead Mask: Preventing Future Information Leakage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99bd62",
   "metadata": {},
   "source": [
    "**The Look-Ahead Problem:**\n",
    "\n",
    "When training autoregressive models (like GPT), the model predicts the next token based ONLY on previous tokens:\n",
    "\n",
    "```\n",
    "Training sequence: \"The cat sat on the mat\"\n",
    "\n",
    "Predicting position 3 (\"sat\"):\n",
    "‚úÖ Can see: \"The\", \"cat\"\n",
    "‚ùå Cannot see: \"sat\", \"on\", \"the\", \"mat\" (these are in the future!)\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "Without masking, during training:\n",
    "- The model would see the answer before predicting it (cheating! üö´)\n",
    "- At test time, it won't have access to future tokens\n",
    "- This mismatch causes poor generalization\n",
    "\n",
    "**The Causal Mask:**\n",
    "\n",
    "A causal mask is a **lower triangular matrix** that only allows attention to previous positions:\n",
    "\n",
    "$$\\text{Mask} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Row 1 (position 1): Can only attend to position 1 (itself)\n",
    "- Row 2 (position 2): Can attend to positions 1-2\n",
    "- Row 3 (position 3): Can attend to positions 1-3\n",
    "- Row 4 (position 4): Can attend to positions 1-4\n",
    "\n",
    "**Visual Example:**\n",
    "\n",
    "```\n",
    "Sequence: [\"The\", \"cat\", \"sat\", \"on\"]\n",
    "\n",
    "Without causal mask (WRONG ‚ùå):\n",
    "        The  cat  sat  on\n",
    "  The   ‚úì    ‚úì    ‚úì    ‚úì   ‚Üê Can see everything!\n",
    "  cat   ‚úì    ‚úì    ‚úì    ‚úì\n",
    "  sat   ‚úì    ‚úì    ‚úì    ‚úì\n",
    "  on    ‚úì    ‚úì    ‚úì    ‚úì\n",
    "\n",
    "With causal mask (CORRECT ‚úÖ):\n",
    "        The  cat  sat  on\n",
    "  The   ‚úì    ‚úó    ‚úó    ‚úó   ‚Üê Only sees itself\n",
    "  cat   ‚úì    ‚úì    ‚úó    ‚úó   ‚Üê Sees The, cat\n",
    "  sat   ‚úì    ‚úì    ‚úì    ‚úó   ‚Üê Sees The, cat, sat\n",
    "  on    ‚úì    ‚úì    ‚úì    ‚úì   ‚Üê Sees all previous\n",
    "```\n",
    "\n",
    "**Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create causal (look-ahead) mask\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        mask: Lower triangular mask (1, 1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Create lower triangular matrix\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CAUSAL MASK DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "seq_len = 6\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(f\"Causal mask shape: {causal_mask.shape}\")\n",
    "print(\"\\nCausal mask (1=can attend, 0=cannot attend):\")\n",
    "print(causal_mask.squeeze().numpy().astype(int))\n",
    "print()\n",
    "\n",
    "# Visualize the causal mask\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(causal_mask.squeeze().numpy(),\n",
    "            annot=True,\n",
    "            fmt='.0f',\n",
    "            cmap='RdYlGn',\n",
    "            cbar_kws={'label': 'Attention Allowed'},\n",
    "            linewidths=0.5)\n",
    "plt.title('Causal Mask\\n(Lower triangular = only attend to past)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Key Position (attending to)')\n",
    "plt.ylabel('Query Position (attending from)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Position 0 can only attend to position 0\")\n",
    "print(\"- Position 1 can attend to positions 0-1\")\n",
    "print(\"- Position 2 can attend to positions 0-2\")\n",
    "print(\"- And so on...\")\n",
    "print(\"\\n‚úÖ This prevents the model from 'cheating' by seeing future tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db530492",
   "metadata": {},
   "source": [
    "**Comparing Attention With and Without Causal Mask:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c936cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy Q, K, V\n",
    "batch_size = 1\n",
    "d_model = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Without causal mask\n",
    "output_no_mask, attn_no_mask = scaled_dot_product_attention_pytorch(Q, K, V, mask=None)\n",
    "\n",
    "# With causal mask\n",
    "output_with_mask, attn_with_mask = scaled_dot_product_attention_pytorch(Q, K, V, mask=causal_mask)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without mask\n",
    "sns.heatmap(attn_no_mask[0].detach().numpy(),\n",
    "            annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            ax=axes[0], vmin=0, vmax=0.5)\n",
    "axes[0].set_title('Without Causal Mask\\n(Can see future! ‚ùå)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "# With causal mask\n",
    "sns.heatmap(attn_with_mask[0].detach().numpy(),\n",
    "            annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            ax=axes[1], vmin=0, vmax=0.5)\n",
    "axes[1].set_title('With Causal Mask\\n(Only sees past! ‚úÖ)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Observations:\")\n",
    "print(\"1. WITHOUT mask: Upper triangle has non-zero values (attending to future)\")\n",
    "print(\"2. WITH mask: Upper triangle is all zeros (cannot attend to future)\")\n",
    "print(\"3. The attention is properly 'causal' - only looking backward in time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35415f",
   "metadata": {},
   "source": [
    "### **13 Combined Masking: Padding + Causal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b81f5",
   "metadata": {},
   "source": [
    "In real-world scenarios (like training GPT), we often need BOTH types of masking:\n",
    "\n",
    "1. **Padding mask**: Ignore padding tokens\n",
    "2. **Causal mask**: Prevent looking ahead\n",
    "\n",
    "**Combined Mask Strategy:**\n",
    "\n",
    "We combine them using logical AND (both must be 1 for attention to be allowed):\n",
    "\n",
    "$$\\text{Combined Mask} = \\text{Padding Mask} \\land \\text{Causal Mask}$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Sequence: [\"The\", \"cat\", \"sat\", <PAD>]\n",
    "            1      1      1      0     ‚Üê Padding mask\n",
    "\n",
    "Causal mask:\n",
    "  1  0  0  0\n",
    "  1  1  0  0\n",
    "  1  1  1  0\n",
    "  1  1  1  1\n",
    "\n",
    "Combined mask (element-wise AND):\n",
    "  1  0  0  0     ‚Üê Position 0: Only self, no <PAD>\n",
    "  1  1  0  0     ‚Üê Position 1: Up to position 1, no <PAD>\n",
    "  1  1  1  0     ‚Üê Position 2: Up to position 2, no <PAD>\n",
    "  0  0  0  0     ‚Üê Position 3: <PAD> row (masked out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_mask(seq, pad_token=0):\n",
    "    \"\"\"\n",
    "    Create combined padding + causal mask\n",
    "    \n",
    "    Args:\n",
    "        seq: Input sequence (batch_size, seq_len)\n",
    "        pad_token: Token id for padding\n",
    "    \n",
    "    Returns:\n",
    "        combined_mask: Combined mask (batch_size, 1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = seq.shape\n",
    "    \n",
    "    # Padding mask: (batch_size, 1, 1, seq_len)\n",
    "    pad_mask = create_padding_mask(seq, pad_token)\n",
    "    \n",
    "    # Causal mask: (1, 1, seq_len, seq_len)\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    \n",
    "    # Combine with logical AND\n",
    "    # Broadcasting will handle the dimension differences\n",
    "    combined_mask = pad_mask * causal_mask\n",
    "    \n",
    "    return combined_mask\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMBINED MASKING DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Create sequence with padding\n",
    "sequence = torch.tensor([\n",
    "    [5, 8, 3, 2, 0, 0]  # Last 2 are padding\n",
    "]).long()\n",
    "\n",
    "print(\"Sequence (0 = <PAD>):\")\n",
    "print(sequence)\n",
    "print()\n",
    "\n",
    "# Get individual masks\n",
    "pad_mask = create_padding_mask(sequence, pad_token=0)\n",
    "causal_mask = create_causal_mask(sequence.shape[1])\n",
    "combined_mask = create_combined_mask(sequence, pad_token=0)\n",
    "\n",
    "# Visualize all three masks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "masks = [\n",
    "    (pad_mask.squeeze(), \"Padding Mask\\n(Ignore <PAD> tokens)\"),\n",
    "    (causal_mask.squeeze(), \"Causal Mask\\n(No future peeking)\"),\n",
    "    (combined_mask.squeeze(), \"Combined Mask\\n(Both constraints)\")\n",
    "]\n",
    "\n",
    "for idx, (mask, title) in enumerate(masks):\n",
    "    sns.heatmap(mask.numpy(),\n",
    "                annot=True,\n",
    "                fmt='.0f',\n",
    "                cmap='RdYlGn',\n",
    "                ax=axes[idx],\n",
    "                cbar_kws={'label': 'Allowed'},\n",
    "                vmin=0, vmax=1)\n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Key Position')\n",
    "    axes[idx].set_ylabel('Query Position')\n",
    "    \n",
    "    # Mark padding region\n",
    "    axes[idx].axvline(x=4, color='blue', linewidth=2, linestyle='--', alpha=0.5)\n",
    "    axes[idx].axhline(y=4, color='blue', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis:\")\n",
    "print(\"1. Padding mask: Last 2 columns are blocked (columns 4-5)\")\n",
    "print(\"2. Causal mask: Upper triangle is blocked\")\n",
    "print(\"3. Combined mask: BOTH constraints applied!\")\n",
    "print(\"   - Upper triangle is blocked (causal)\")\n",
    "print(\"   - Last 2 columns are blocked (padding)\")\n",
    "print(\"   - Last 2 rows are also blocked (queries from <PAD> positions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d35e50",
   "metadata": {},
   "source": [
    "### **14 Why Masking Is Crucial for Language Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a103a6a",
   "metadata": {},
   "source": [
    "Let's understand why masking is absolutely essential for training language models like GPT!\n",
    "\n",
    "**The Training-Inference Mismatch Problem:**\n",
    "\n",
    "**‚ùå Without Causal Masking (WRONG):**\n",
    "\n",
    "```\n",
    "Training:\n",
    "Input:  \"The cat sat on the\"\n",
    "Target: \"mat\"\n",
    "\n",
    "During training, position 3 (\"on\") can see:\n",
    "[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"] ‚Üê Sees the answer!\n",
    "\n",
    "Model learns: \"When I see 'on the mat', predict 'mat'\" (trivial!)\n",
    "```\n",
    "\n",
    "**At Inference:**\n",
    "```\n",
    "Input: \"The cat sat on the\"\n",
    "Model has: [\"The\", \"cat\", \"sat\", \"on\", \"the\"] ‚Üê No future context!\n",
    "Model fails: It never learned to predict without seeing the answer!\n",
    "```\n",
    "\n",
    "**‚úÖ With Causal Masking (CORRECT):**\n",
    "\n",
    "```\n",
    "Training:\n",
    "Input:  \"The cat sat on the\"\n",
    "Target: \"mat\"\n",
    "\n",
    "During training, position 3 (\"on\") can ONLY see:\n",
    "[\"The\", \"cat\", \"sat\", \"on\"] ‚Üê Matches inference condition!\n",
    "\n",
    "Model learns: \"When I see 'on' after 'The cat sat', predict the next token\"\n",
    "```\n",
    "\n",
    "**At Inference:**\n",
    "```\n",
    "Input: \"The cat sat on the\"\n",
    "Model has: [\"The\", \"cat\", \"sat\", \"on\", \"the\"] ‚Üê Same condition as training!\n",
    "Model succeeds: It learned to predict from past context only!\n",
    "```\n",
    "\n",
    "**Real-World Impact:**\n",
    "\n",
    "| Metric | Without Masking | With Masking |\n",
    "|--------|----------------|--------------|\n",
    "| Training loss | Very low (cheating) | Higher (realistic) |\n",
    "| Test loss | Very high (fails) | Lower (generalizes) |\n",
    "| Text quality | Incoherent | Coherent ‚úÖ |\n",
    "| Training time | Wasted | Productive ‚úÖ |\n",
    "\n",
    "**Why Padding Mask Matters:**\n",
    "\n",
    "1. **Computational efficiency**: Don't waste computation on meaningless tokens\n",
    "2. **Numerical stability**: Padding tokens have undefined gradients\n",
    "3. **Semantic correctness**: Model shouldn't learn that `<PAD>` is a meaningful word\n",
    "4. **Batch training**: Different length sequences in same batch need padding\n",
    "\n",
    "**Example of Padding Mask Impact:**\n",
    "\n",
    "```python\n",
    "# Without padding mask\n",
    "\"I love <PAD> <PAD>\" ‚Üí Model might learn \"<PAD>\" predicts \"<PAD>\"\n",
    "\n",
    "# With padding mask  \n",
    "\"I love\" ‚Üí Model correctly focuses on \"I\" and \"love\" only\n",
    "```\n",
    "\n",
    "**The Bottom Line:**\n",
    "\n",
    "üéØ **Causal masking** = Ensures train/test consistency (no cheating!)\n",
    "üéØ **Padding masking** = Ensures computational and semantic correctness\n",
    "üéØ **Combined masking** = Both benefits for production language models!\n",
    "\n",
    "Without proper masking:\n",
    "- ‚ùå Models fail to generalize\n",
    "- ‚ùå Waste computational resources\n",
    "- ‚ùå Learn incorrect patterns\n",
    "- ‚ùå Poor text generation quality\n",
    "\n",
    "With proper masking:\n",
    "- ‚úÖ Models generalize well\n",
    "- ‚úÖ Efficient training\n",
    "- ‚úÖ Learn correct patterns\n",
    "- ‚úÖ High-quality text generation\n",
    "\n",
    "**This is why every modern language model (GPT, LLaMA, etc.) uses causal masking!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81282b7a",
   "metadata": {},
   "source": [
    "**Visual Summary: Complete Attention Flow with Masking:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a23406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete demonstration: Multi-head attention with combined masking\n",
    "print(\"=\"*60)\n",
    "print(\"COMPLETE ATTENTION PIPELINE WITH MASKING\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Setup\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "# Create model\n",
    "mha_masked = MultiHeadAttention(d_model, num_heads, dropout=0.0)\n",
    "\n",
    "# Create sequences with padding\n",
    "sequences = torch.tensor([\n",
    "    [5, 8, 3, 2, 7, 9, 0, 0],  # 2 padding tokens\n",
    "    [4, 6, 1, 8, 2, 5, 3, 9],  # No padding\n",
    "])\n",
    "\n",
    "# Create embeddings (normally from embedding layer)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create combined mask\n",
    "combined_mask = create_combined_mask(sequences, pad_token=0)\n",
    "\n",
    "# Forward pass with masking\n",
    "output_masked = mha_masked(x, x, x, mask=combined_mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Combined mask shape: {combined_mask.shape}\")\n",
    "print(f\"Output shape: {output_masked.shape}\")\n",
    "print()\n",
    "\n",
    "# Visualize attention patterns for all heads (first sample)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "attention = mha_masked.attention_weights[0]  # First sample\n",
    "\n",
    "for head in range(num_heads):\n",
    "    ax = axes[head]\n",
    "    sns.heatmap(attention[head].numpy(),\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                cmap='YlOrRd',\n",
    "                ax=ax,\n",
    "                cbar=True,\n",
    "                vmin=0,\n",
    "                vmax=0.5)\n",
    "    ax.set_title(f'Head {head+1} (with Combined Mask)', fontweight='bold')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    \n",
    "    # Mark padding region\n",
    "    ax.axvline(x=6, color='blue', linewidth=2, linestyle='--', alpha=0.7, label='Padding')\n",
    "    ax.axhline(y=6, color='blue', linewidth=2, linestyle='--', alpha=0.7)\n",
    "    if head == 0:\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention with Combined Masking\\n(Causal + Padding)', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Notice in all heads:\")\n",
    "print(\"1. Upper triangle is zero (causal masking)\")\n",
    "print(\"2. Last 2 columns are zero (padding masking)\")\n",
    "print(\"3. Last 2 rows are zero (queries from padding positions)\")\n",
    "print(\"4. Each head still learns different patterns within valid region!\")\n",
    "print()\n",
    "print(\"üéØ This is production-ready attention for language modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683b1ad",
   "metadata": {},
   "source": [
    "### **15. Self-Attention vs Cross-Attention: Clarifying Key Terms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c68e5",
   "metadata": {},
   "source": [
    "You'll frequently encounter terms like **Self-Attention** and **Cross-Attention** in papers and documentation. Let's demystify these concepts once and for all!\n",
    "\n",
    "**The Key Distinction: Where Do Q, K, V Come From?**\n",
    "\n",
    "Remember that attention computes:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The **type of attention** depends on the **source of these matrices**!\n",
    "\n",
    "---\n",
    "\n",
    "### **Self-Attention: Looking Within Yourself**\n",
    "\n",
    "**Definition:** Self-attention is when Query (Q), Key (K), and Value (V) all come from the **same sequence**.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "$$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
    "\n",
    "Where $X$ is the **same input sequence** for all three!\n",
    "\n",
    "**The Intuition:**\n",
    "\n",
    "Imagine you're reading a sentence and trying to understand each word by looking at **other words in the same sentence**:\n",
    "\n",
    "*\"The animal didn't cross the street because it was too tired.\"*\n",
    "\n",
    "When processing \"it\":\n",
    "- **Query**: \"What does 'it' refer to?\" (from the word \"it\")\n",
    "- **Keys**: \"What nouns are available?\" (from ALL words in the sentence)\n",
    "- **Values**: \"What information do those nouns provide?\" (from ALL words)\n",
    "\n",
    "All three come from the **same sentence**!\n",
    "\n",
    "**Visual Example:**\n",
    "\n",
    "```\n",
    "Input sentence: [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "                    ‚Üì      ‚Üì      ‚Üì     ‚Üì     ‚Üì\n",
    "All words attend to each other within the same sequence\n",
    "\n",
    "\"cat\" looks at: [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "\"sat\" looks at: [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "\"mat\" looks at: [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "```\n",
    "\n",
    "**Where It's Used:**\n",
    "\n",
    "| Model | Where | Purpose |\n",
    "|-------|-------|---------|\n",
    "| **GPT** | Every layer | Each token attends to previous tokens in the sequence |\n",
    "| **BERT** | Every layer | Each token attends to all tokens (bidirectional) |\n",
    "| **Vision Transformer** | Every layer | Each image patch attends to all other patches |\n",
    "| **Encoder** | All layers | Process input by relating elements to each other |\n",
    "| **Decoder** | Masked self-attention layers | Generate output by attending to previous outputs |\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Self-attention: Q, K, V from the same source\n",
    "x = torch.randn(batch_size, seq_len, d_model)  # Single input sequence\n",
    "\n",
    "Q = x @ W_q  # Query from x\n",
    "K = x @ W_k  # Key from x\n",
    "V = x @ W_v  # Value from x\n",
    "\n",
    "# All three come from the same x!\n",
    "output = attention(Q, K, V)\n",
    "```\n",
    "\n",
    "**Real-World Analogy:**\n",
    "\n",
    "Self-attention is like **proofreading your own essay**:\n",
    "- You read your own words (Query)\n",
    "- You check against your own words (Keys)\n",
    "- You understand from your own words (Values)\n",
    "- Everything comes from **your essay alone**!\n",
    "\n",
    "---\n",
    "\n",
    "### **Cross-Attention: Looking at Something Else**\n",
    "\n",
    "**Definition:** Cross-attention is when Query (Q) comes from **one sequence**, but Key (K) and Value (V) come from a **different sequence**.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "$$Q = X_{\\text{target}}W^Q, \\quad K = X_{\\text{source}}W^K, \\quad V = X_{\\text{source}}W^V$$\n",
    "\n",
    "Where $X_{\\text{target}}$ and $X_{\\text{source}}$ are **different sequences**!\n",
    "\n",
    "**The Intuition:**\n",
    "\n",
    "Imagine you're **translating** a sentence:\n",
    "\n",
    "- **Source (English)**: \"The cat sat on the mat\"\n",
    "- **Target (French)**: \"Le chat s'est assis sur le tapis\"\n",
    "\n",
    "When generating the French word \"chat\" (cat):\n",
    "- **Query**: \"What am I generating?\" (from French output so far: \"Le\")\n",
    "- **Keys**: \"What's available in the input?\" (from English: \"The\", \"cat\", \"sat\", ...)\n",
    "- **Values**: \"What information does the input provide?\" (from English words)\n",
    "\n",
    "The Query comes from **French** (target), but Keys and Values come from **English** (source)!\n",
    "\n",
    "**Visual Example:**\n",
    "\n",
    "```\n",
    "Source sequence (English): [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "                               ‚Üì      ‚Üì      ‚Üì     ‚Üì     ‚Üì\n",
    "                            Keys & Values\n",
    "                                    ‚Üë\n",
    "                               Cross-Attend\n",
    "                                    ‚Üë\n",
    "Target sequence (French):  [\"Le\", \"chat\", \"?\", ...]\n",
    "                                          ‚Üë\n",
    "                                       Query\n",
    "```\n",
    "\n",
    "**Where It's Used:**\n",
    "\n",
    "| Model | Where | Purpose |\n",
    "|-------|-------|---------|\n",
    "| **Encoder-Decoder Translation** | Decoder cross-attention | Target language attends to source language |\n",
    "| **Image Captioning** | Decoder | Text attends to image features |\n",
    "| **Visual Question Answering** | Various layers | Text attends to image regions |\n",
    "| **Multimodal Models** | Fusion layers | One modality attends to another |\n",
    "| **Text-to-Image (DALL-E)** | Image decoder | Image tokens attend to text description |\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Cross-attention: Q from one source, K & V from another\n",
    "encoder_output = torch.randn(batch_size, src_len, d_model)  # Source sequence\n",
    "decoder_hidden = torch.randn(batch_size, tgt_len, d_model)  # Target sequence\n",
    "\n",
    "Q = decoder_hidden @ W_q      # Query from TARGET\n",
    "K = encoder_output @ W_k      # Key from SOURCE\n",
    "V = encoder_output @ W_v      # Value from SOURCE\n",
    "\n",
    "# Q from target, K & V from source!\n",
    "output = attention(Q, K, V)\n",
    "```\n",
    "\n",
    "**Real-World Analogy:**\n",
    "\n",
    "Cross-attention is like **translating from a textbook**:\n",
    "- You're writing in French (Query - what you're generating)\n",
    "- You're reading from an English book (Keys & Values - source information)\n",
    "- You look up English words to write French words\n",
    "- Two different sources!\n",
    "\n",
    "---\n",
    "\n",
    "### **Side-by-Side Comparison**\n",
    "\n",
    "| Aspect | Self-Attention | Cross-Attention |\n",
    "|--------|----------------|-----------------|\n",
    "| **Q source** | Same sequence | Target sequence |\n",
    "| **K source** | Same sequence | Source sequence |\n",
    "| **V source** | Same sequence | Source sequence |\n",
    "| **Purpose** | Relate elements within a sequence | Relate elements across sequences |\n",
    "| **Example** | Understanding word relationships in a sentence | Translating from English to French |\n",
    "| **Sequence lengths** | Q, K, V have same length | Q length ‚â† K/V length |\n",
    "| **Use case** | GPT, BERT, ViT (single input) | Translation, image captioning (two inputs) |\n",
    "| **Attention matrix shape** | $(seq\\_len, seq\\_len)$ | $(target\\_len, source\\_len)$ |\n",
    "\n",
    "---\n",
    "\n",
    "### **Complete Example: Translation Model**\n",
    "\n",
    "Let's see how both types work together in a **Transformer translation model**:\n",
    "\n",
    "```\n",
    "English: \"The cat sat\"  ‚Üí  French: \"Le chat s'est assis\"\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          ENCODER (English)              ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Input: [\"The\", \"cat\", \"sat\"]          ‚îÇ\n",
    "‚îÇ     ‚Üì                                   ‚îÇ\n",
    "‚îÇ  Self-Attention (within English)        ‚îÇ\n",
    "‚îÇ  - \"cat\" attends to \"The\", \"cat\", \"sat\"‚îÇ\n",
    "‚îÇ     ‚Üì                                   ‚îÇ\n",
    "‚îÇ  Encoder Output                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚Üì\n",
    "        Keys & Values (K, V)\n",
    "                  ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          DECODER (French)               ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Output so far: [\"Le\", \"chat\"]         ‚îÇ\n",
    "‚îÇ     ‚Üì                                   ‚îÇ\n",
    "‚îÇ  1. Self-Attention (within French)      ‚îÇ\n",
    "‚îÇ     - \"chat\" attends to \"Le\", \"chat\"   ‚îÇ\n",
    "‚îÇ     (Masked - can't see future!)       ‚îÇ\n",
    "‚îÇ     ‚Üì                                   ‚îÇ\n",
    "‚îÇ  2. Cross-Attention (French‚ÜíEnglish)    ‚îÇ\n",
    "‚îÇ     - Q: \"chat\" (from French)          ‚îÇ\n",
    "‚îÇ     - K,V: encoder output (English)    ‚îÇ\n",
    "‚îÇ     - \"chat\" attends to English words!  ‚îÇ\n",
    "‚îÇ     ‚Üì                                   ‚îÇ\n",
    "‚îÇ  3. Generate next word: \"s'est\"        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**The Flow:**\n",
    "1. **Encoder Self-Attention**: English words relate to each other\n",
    "2. **Decoder Self-Attention**: French words relate to each other (with causal mask)\n",
    "3. **Decoder Cross-Attention**: French words look up English words for translation\n",
    "4. Repeat for each French word!\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Implementation Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Demonstrate self-attention vs cross-attention\n",
    "print(\"=\"*60)\n",
    "print(\"SELF-ATTENTION vs CROSS-ATTENTION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Setup\n",
    "batch_size = 1\n",
    "d_model = 64\n",
    "\n",
    "# Self-Attention Example\n",
    "print(\"1. SELF-ATTENTION EXAMPLE\")\n",
    "print(\"-\" * 40)\n",
    "seq_len = 6\n",
    "x_self = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# All Q, K, V from the same source\n",
    "Q_self = x_self @ torch.randn(d_model, d_model)\n",
    "K_self = x_self @ torch.randn(d_model, d_model)\n",
    "V_self = x_self @ torch.randn(d_model, d_model)\n",
    "\n",
    "output_self, attn_self = scaled_dot_product_attention_pytorch(Q_self, K_self, V_self)\n",
    "\n",
    "print(f\"Input sequence length: {seq_len}\")\n",
    "print(f\"Q shape (from input): {Q_self.shape}\")\n",
    "print(f\"K shape (from input): {K_self.shape}\")\n",
    "print(f\"V shape (from input): {V_self.shape}\")\n",
    "print(f\"Attention matrix: {attn_self.shape} ‚Üí ({seq_len}√ó{seq_len})\")\n",
    "print(\"‚úÖ Square attention matrix (same sequence)\")\n",
    "print()\n",
    "\n",
    "# Cross-Attention Example\n",
    "print(\"2. CROSS-ATTENTION EXAMPLE\")\n",
    "print(\"-\" * 40)\n",
    "target_len = 4  # Target sequence (e.g., French)\n",
    "source_len = 6  # Source sequence (e.g., English)\n",
    "\n",
    "x_target = torch.randn(batch_size, target_len, d_model)  # Decoder hidden states\n",
    "x_source = torch.randn(batch_size, source_len, d_model)  # Encoder output\n",
    "\n",
    "# Q from target, K & V from source\n",
    "Q_cross = x_target @ torch.randn(d_model, d_model)\n",
    "K_cross = x_source @ torch.randn(d_model, d_model)\n",
    "V_cross = x_source @ torch.randn(d_model, d_model)\n",
    "\n",
    "output_cross, attn_cross = scaled_dot_product_attention_pytorch(Q_cross, K_cross, V_cross)\n",
    "\n",
    "print(f\"Target sequence length: {target_len}\")\n",
    "print(f\"Source sequence length: {source_len}\")\n",
    "print(f\"Q shape (from target): {Q_cross.shape}\")\n",
    "print(f\"K shape (from source): {K_cross.shape}\")\n",
    "print(f\"V shape (from source): {V_cross.shape}\")\n",
    "print(f\"Attention matrix: {attn_cross.shape} ‚Üí ({target_len}√ó{source_len})\")\n",
    "print(\"‚úÖ Rectangular attention matrix (different sequences)\")\n",
    "print()\n",
    "\n",
    "# Visualize both\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Self-Attention\n",
    "sns.heatmap(attn_self[0].detach().numpy(),\n",
    "            annot=True, fmt='.2f', cmap='Blues',\n",
    "            ax=axes[0], square=True,\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "axes[0].set_title('Self-Attention\\n(Q, K, V from same sequence)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Key Position (same sequence)')\n",
    "axes[0].set_ylabel('Query Position (same sequence)')\n",
    "axes[0].text(0.5, -0.15, f'Shape: {seq_len}√ó{seq_len} (Square)',\n",
    "             ha='center', transform=axes[0].transAxes,\n",
    "             fontsize=10, style='italic')\n",
    "\n",
    "# Cross-Attention\n",
    "sns.heatmap(attn_cross[0].detach().numpy(),\n",
    "            annot=True, fmt='.2f', cmap='Oranges',\n",
    "            ax=axes[1],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "axes[1].set_title('Cross-Attention\\n(Q from target, K&V from source)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Key Position (source sequence)')\n",
    "axes[1].set_ylabel('Query Position (target sequence)')\n",
    "axes[1].text(0.5, -0.15, f'Shape: {target_len}√ó{source_len} (Rectangular)',\n",
    "             ha='center', transform=axes[1].transAxes,\n",
    "             fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Observations:\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "print(\"Self-Attention:\")\n",
    "print(\"  ‚Ä¢ Square attention matrix (6√ó6)\")\n",
    "print(\"  ‚Ä¢ Each position attends to all positions in SAME sequence\")\n",
    "print(\"  ‚Ä¢ Used in: GPT, BERT, Vision Transformers\")\n",
    "print()\n",
    "print(\"Cross-Attention:\")\n",
    "print(\"  ‚Ä¢ Rectangular attention matrix (4√ó6)\")\n",
    "print(\"  ‚Ä¢ Target positions attend to source positions\")\n",
    "print(\"  ‚Ä¢ Used in: Translation, image captioning, multimodal models\")\n",
    "print(\"‚îÅ\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80685c7c",
   "metadata": {},
   "source": [
    "### **Common Misconceptions Clarified**\n",
    "\n",
    "**‚ùå Misconception 1:** \"Self-attention is only for encoders\"\n",
    "- **‚úÖ Truth:** Both encoders AND decoders use self-attention! Decoders just add causal masking.\n",
    "\n",
    "**‚ùå Misconception 2:** \"Cross-attention is just a different formula\"\n",
    "- **‚úÖ Truth:** Same formula! Only the source of Q, K, V changes.\n",
    "\n",
    "**‚ùå Misconception 3:** \"You need different code for self vs cross-attention\"\n",
    "- **‚úÖ Truth:** The same `attention()` function works for both! Just pass different inputs.\n",
    "\n",
    "**‚ùå Misconception 4:** \"Cross-attention is always bidirectional\"\n",
    "- **‚úÖ Truth:** Cross-attention direction matters! Target‚ÜíSource (typical) vs Source‚ÜíTarget (rare).\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick Reference: When to Use Which**\n",
    "\n",
    "**Use Self-Attention when:**\n",
    "- ‚úÖ Processing a single sequence (text, image, audio)\n",
    "- ‚úÖ Finding relationships within the same data\n",
    "- ‚úÖ Building encoders (BERT, Vision Transformer)\n",
    "- ‚úÖ Building autoregressive decoders (GPT) with causal mask\n",
    "- ‚úÖ All tokens should relate to each other\n",
    "\n",
    "**Use Cross-Attention when:**\n",
    "- ‚úÖ Connecting two different sequences (translation)\n",
    "- ‚úÖ Conditioning generation on context (image captioning)\n",
    "- ‚úÖ Multimodal fusion (text + image)\n",
    "- ‚úÖ Encoder-decoder architectures\n",
    "- ‚úÖ Target needs to look up information from source\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| Feature | Self-Attention | Cross-Attention |\n",
    "|---------|----------------|-----------------|\n",
    "| **Input sequences** | 1 | 2 |\n",
    "| **Q source** | Same as K, V | Different from K, V |\n",
    "| **Matrix shape** | Square $(n \\times n)$ | Rectangular $(m \\times n)$ |\n",
    "| **Typical use** | Understanding context | Connecting contexts |\n",
    "| **Models** | GPT, BERT, ViT | Translation, captioning |\n",
    "| **Implementation** | `attention(x, x, x)` | `attention(target, source, source)` |\n",
    "| **Masking types** | Causal (for decoders) | Usually none (or padding) |\n",
    "| **Complexity** | $O(n^2)$ | $O(m \\times n)$ |\n",
    "\n",
    "---\n",
    "\n",
    "### **The Big Picture: How They Work Together**\n",
    "\n",
    "Most powerful models use **BOTH** types of attention:\n",
    "\n",
    "```\n",
    "ENCODER (GPT-style):\n",
    "‚îú‚îÄ‚îÄ Self-Attention (causal)\n",
    "‚îî‚îÄ‚îÄ Output\n",
    "\n",
    "ENCODER-DECODER (Translation):\n",
    "‚îú‚îÄ‚îÄ ENCODER\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Self-Attention (bidirectional)\n",
    "‚îú‚îÄ‚îÄ DECODER\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Self-Attention (causal) ‚Üê Relates target words\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Cross-Attention ‚Üê Connects target to source\n",
    "‚îî‚îÄ‚îÄ Output\n",
    "\n",
    "MULTIMODAL (Image Captioning):\n",
    "‚îú‚îÄ‚îÄ IMAGE ENCODER\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Self-Attention (patches attend to patches)\n",
    "‚îú‚îÄ‚îÄ TEXT DECODER\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Self-Attention (words attend to previous words)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Cross-Attention (words attend to image patches)\n",
    "‚îî‚îÄ‚îÄ Generated Caption\n",
    "```\n",
    "\n",
    "**Now you understand the fundamental attention types that power all modern AI! üéì**\n",
    "\n",
    "The same mathematical formula, just different sources for Q, K, V ‚Äì that's the beauty of attention mechanisms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d8116a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Summary: What We've Learned**\n",
    "\n",
    "Congratulations! You now understand the core mechanisms that power modern language models! üéâ\n",
    "\n",
    "**Section 6: Scaled Dot-Product Attention**\n",
    "- ‚úÖ The fundamental attention formula: $\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "- ‚úÖ Why scaling by $\\sqrt{d_k}$ prevents gradient problems\n",
    "- ‚úÖ Implemented attention from scratch in NumPy\n",
    "- ‚úÖ Built production-ready PyTorch implementation with batching\n",
    "\n",
    "**Section 7: Multi-Head Attention**\n",
    "- ‚úÖ Why multiple attention heads capture diverse patterns\n",
    "- ‚úÖ How to split dimensions across heads\n",
    "- ‚úÖ Complete `MultiHeadAttention` module implementation\n",
    "- ‚úÖ Visualized different attention patterns learned by each head\n",
    "\n",
    "**Section 8: Attention Masking**\n",
    "- ‚úÖ **Padding mask**: Ignore meaningless padding tokens\n",
    "- ‚úÖ **Causal mask**: Prevent future information leakage\n",
    "- ‚úÖ **Combined masking**: Apply both constraints simultaneously\n",
    "- ‚úÖ Why masking is crucial for training language models\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "üéØ **Attention = Weighted information aggregation** based on relevance\n",
    "üéØ **Scaling prevents gradient problems** as dimensions increase\n",
    "üéØ **Multiple heads capture multiple relationships** in parallel\n",
    "üéØ **Masking ensures correct learning** by controlling information flow\n",
    "üéØ **These mechanisms power GPT, BERT, and all modern LLMs!**\n",
    "\n",
    "**What's Next?**\n",
    "\n",
    "Now that you understand attention mechanisms, you're ready to:\n",
    "- Build complete Transformer models\n",
    "- Understand how GPT generates text\n",
    "- Explore BERT and other architectures\n",
    "- Fine-tune large language models\n",
    "- Build your own NLP applications!\n",
    "\n",
    "The attention mechanism you learned today is literally used in models like:\n",
    "- **GPT-4** (ChatGPT)\n",
    "- **LLaMA** (Meta's LLM)\n",
    "- **Claude** (Anthropic)\n",
    "- **PaLM** (Google)\n",
    "- **And virtually every modern LLM!**\n",
    "\n",
    "You now understand the core innovation that revolutionized AI! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
