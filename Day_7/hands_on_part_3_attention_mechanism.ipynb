{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de444834",
   "metadata": {},
   "source": [
    "### **1. Transformers and Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af36deb",
   "metadata": {},
   "source": [
    "In 2017, researchers at Google published a groundbreaking paper titled **\"Attention Is All You Need\"**. They introduced the **Transformer architecture**, which fundamentally changed how we approach sequence processing tasks in AI.\n",
    "\n",
    "| Innovation | Benefit | Impact |\n",
    "|-----------|---------|--------|\n",
    "| **Parallel Processing** | Process all words simultaneously | Training 10-100x faster than RNNs |\n",
    "| **Attention Mechanism** | Direct connections between any words | Better understanding of context |\n",
    "| **Scalability** | Works better with more data/parameters | Powers models from millions to trillions of parameters |\n",
    "| **Transfer Learning** | Pre-train once, adapt to many tasks | Enables ChatGPT, GPT-4, BERT, and more |\n",
    "\n",
    "The **attention mechanism** is what makes Transformers special. It allows the model to focus on relevant parts of the input when processing each word - just like how you naturally focus on important words when reading!\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n",
    "  <div style=\"width: 40%; text-align: center;\">\n",
    "    <img src=\"https://substackcdn.com/image/fetch/$s_!jtT-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4d7dc94-6f18-4973-a501-de1d5b101c10_1903x856.png\" width=\"100%\"/>\n",
    "    <p><i>Attention Mechanism</i></p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"width: 30%; text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" width=\"100%\"/>\n",
    "    <p><i>The Transformer Architecture</i></p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "In this notebook, we'll understand **how attention works** - the fundamental mechanism behind all modern language models. By the end, you'll know exactly how ChatGPT, GPT-4, and other AI systems process and understand language!\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd83a2",
   "metadata": {},
   "source": [
    "### **2. The Intuition behind Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67803496",
   "metadata": {},
   "source": [
    "Let's solidify your understanding with a library analogy:\n",
    "\n",
    "**üèõÔ∏è The Library Analogy**\n",
    "\n",
    "Imagine you're researching \"climate change impacts\" in a library:\n",
    "\n",
    "**Your Query:** \"How does climate change affect polar bears?\"\n",
    "\n",
    "**The Library Catalog (Keys):**\n",
    "- Book 1: \"Climate Change Overview\" üîë\n",
    "- Book 2: \"Polar Bear Biology\" üîë\n",
    "- Book 3: \"Arctic Ecosystems\" üîë\n",
    "- Book 4: \"17th Century Poetry\" üîë\n",
    "- Book 5: \"Ocean Acidification\" üîë\n",
    "\n",
    "**What You Do:**\n",
    "\n",
    "1. **Compare Query with Keys** (Matching step)\n",
    "   - Your query ‚Üî \"Climate Change Overview\": High relevance! ‚úÖ\n",
    "   - Your query ‚Üî \"Polar Bear Biology\": High relevance! ‚úÖ\n",
    "   - Your query ‚Üî \"Arctic Ecosystems\": Medium relevance ‚úì\n",
    "   - Your query ‚Üî \"17th Century Poetry\": No relevance ‚ùå\n",
    "   - Your query ‚Üî \"Ocean Acidification\": Low relevance\n",
    "\n",
    "2. **Assign Attention Weights** (based on relevance)\n",
    "   - Book 1: 0.35 (35% attention)\n",
    "   - Book 2: 0.40 (40% attention) üéØ\n",
    "   - Book 3: 0.20 (20% attention)\n",
    "   - Book 4: 0.00 (0% attention)\n",
    "   - Book 5: 0.05 (5% attention)\n",
    "\n",
    "3. **Read Content (Values) Proportionally**\n",
    "   - Spend 40% of your time on \"Polar Bear Biology\"\n",
    "   - Spend 35% on \"Climate Change Overview\"\n",
    "   - Spend 20% on \"Arctic Ecosystems\"\n",
    "   - Skip \"17th Century Poetry\" entirely\n",
    "   - Briefly skim \"Ocean Acidification\"\n",
    "\n",
    "4. **Synthesize Information** (Weighted aggregation)\n",
    "   - Your final understanding = \n",
    "     - 0.40 √ó (Polar Bear content) +\n",
    "     - 0.35 √ó (Climate content) +\n",
    "     - 0.20 √ó (Arctic content) +\n",
    "     - 0.05 √ó (Ocean content)\n",
    "\n",
    "**Mapping to Attention:**\n",
    "\n",
    "| Library Concept | Attention Mechanism |\n",
    "|----------------|--------------------|\n",
    "| Your research question | **Query (Q)** |\n",
    "| Book titles in catalog | **Keys (K)** |\n",
    "| Book contents | **Values (V)** |\n",
    "| Relevance matching | **Q¬∑K (dot product)** |\n",
    "| Time allocation | **Attention weights (Œ±)** |\n",
    "| Final understanding | **Output (weighted sum of V)** |\n",
    "\n",
    "**The Formula Revealed:**\n",
    "\n",
    "$$\\text{Understanding} = \\sum_{i} \\alpha_i \\cdot \\text{Book}_i$$\n",
    "\n",
    "$$\\text{where } \\alpha_i = \\text{softmax}(\\frac{\\text{Query} \\cdot \\text{Key}_i}{\\sqrt{d}})$$\n",
    "\n",
    "This is **exactly** how attention mechanisms work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b174d",
   "metadata": {},
   "source": [
    "### **3. Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712909b7",
   "metadata": {},
   "source": [
    "**The Complete Formula:**\n",
    "\n",
    "| | |\n",
    "| :---: | :---: |\n",
    "| $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$ | <img src=\"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Fba830026-6d8f-4e77-b288-f75dd3a51457%2Fimage.png\" width=\"400\" alt=\"Attention Formula Diagram\"/> |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24f447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1f2418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,65 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a36140",
   "metadata": {},
   "source": [
    "**The Look-Ahead Problem:**\n",
    "\n",
    "When training autoregressive models (like GPT), the model predicts the next token based ONLY on previous tokens:\n",
    "\n",
    "```\n",
    "Training sequence: \"The cat sat on the mat\"\n",
    "\n",
    "Predicting position 3 (\"sat\"):\n",
    "‚úÖ Can see: \"The\", \"cat\"\n",
    "‚ùå Cannot see: \"sat\", \"on\", \"the\", \"mat\" (these are in the future!)\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "Without masking, during training:\n",
    "- The model would see the answer before predicting it (cheating! üö´)\n",
    "- At test time, it won't have access to future tokens\n",
    "- This mismatch causes poor generalization\n",
    "\n",
    "**The Causal Mask:**\n",
    "\n",
    "A causal mask is a **lower triangular matrix** that only allows attention to previous positions:\n",
    "\n",
    "$$\\text{Mask} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a272d4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mask\n",
    "\n",
    "mask = torch.tril(torch.ones(T,T))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e453dcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 1\n",
    "\n",
    "attn_scores = mask / mask.sum(1, keepdim = True) # normalize the rows\n",
    "out_1 = attn_scores @ x # (T,T) @ (B,T,C) ---> (B,T,T) @ (B,T,C) ---> (B,T,C)\n",
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0921fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2\n",
    "\n",
    "attn_scores = torch.zeros(T,T)\n",
    "attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "out_2 = attn_scores @ x\n",
    "out_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf971d",
   "metadata": {},
   "source": [
    "- Every single token at every position will now emit three vectors, a Query and a Key and a Value\n",
    "- Query means What am I looking for?\n",
    "- Key means What do I contain?\n",
    "- Value means What will I communicate?\n",
    "- Their dot product of Q and K will then basically give us attention scores meaning which token has a higher affinity to which other tokens.\n",
    "- Finally we will take the dot product of the attention scores with the values to get the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647bf0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "torch.Size([4, 8, 65])\n",
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "head_size = 65\n",
    "key = torch.nn.Linear(C, head_size, bias = False)\n",
    "query = torch.nn.Linear(C, head_size, bias = False)\n",
    "value = torch.nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "\n",
    "print(k.shape)\n",
    "print(q.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44405bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = q @ k.transpose(-2,-1) / head_size**0.5  # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "out = attn_scores @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e909ce77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [7.1821e-01, 2.8179e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [6.2292e-01, 2.6785e-01, 1.0923e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.7394e-02, 3.5937e-02, 8.8566e-02, 8.4810e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [9.2227e-01, 1.0381e-02, 4.4360e-02, 7.6919e-04, 2.2219e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [7.4951e-02, 1.1837e-01, 2.4863e-01, 3.7787e-01, 6.3060e-03, 1.7387e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.8730e-01, 5.8106e-02, 6.1382e-02, 3.6453e-03, 6.4791e-01, 1.8433e-02,\n",
       "         2.3230e-02, 0.0000e+00],\n",
       "        [4.1080e-01, 6.0570e-02, 2.1063e-02, 1.6063e-03, 1.6883e-01, 1.5380e-02,\n",
       "         4.0297e-03, 3.1772e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
