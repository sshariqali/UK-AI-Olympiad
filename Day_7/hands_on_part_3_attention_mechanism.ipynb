{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a3254",
   "metadata": {},
   "source": [
    "## Day 7 - Part 3: The Attention Mechanism\n",
    "\n",
    "---\n",
    "\n",
    "### üîó **Continuing from Parts 1 & 2**\n",
    "\n",
    "So far in Day 7, we've built a **Bigram Language Model**:\n",
    "\n",
    "| Part | What We Did | Key Limitation |\n",
    "|:----:|-------------|----------------|\n",
    "| Part 1 | Built basic Bigram model | Only looks at 1 previous character |\n",
    "| Part 2 | Added proper training loop | Still only looks at 1 previous character! |\n",
    "\n",
    "**The Problem:** Our Bigram model predicts based on ONLY the last character. It has no way to use broader context!\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the ___\"\n",
    "Bigram sees: \"e\" ‚Üí makes prediction\n",
    "We want:    \"The cat sat on the\" ‚Üí make prediction (use ALL context!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Agenda for this Notebook**\n",
    "\n",
    "| Section | Topic | Description |\n",
    "|:-------:|-------|-------------|\n",
    "| 1 | **Transformers & Attention** | Why attention is revolutionary |\n",
    "| 2 | **The Intuition** | Library analogy for Query-Key-Value |\n",
    "| 3 | **Implementation** | Build attention step-by-step |\n",
    "\n",
    "---\n",
    "\n",
    "### üéì **Learning Objectives**\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand WHY attention is needed\n",
    "- ‚úÖ Grasp the Query-Key-Value intuition\n",
    "- ‚úÖ Implement self-attention from scratch\n",
    "- ‚úÖ Understand causal masking (no peeking at future!)\n",
    "\n",
    "This is the **most important concept** in modern AI - let's master it! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de444834",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Transformers & Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af36deb",
   "metadata": {},
   "source": [
    "In 2017, researchers at Google published a groundbreaking paper titled **\"Attention Is All You Need\"**. They introduced the **Transformer architecture**, which fundamentally changed how we approach sequence processing tasks in AI.\n",
    "\n",
    "| Innovation | Benefit | Impact |\n",
    "|-----------|---------|--------|\n",
    "| **Parallel Processing** | Process all words simultaneously | Training 10-100x faster than RNNs |\n",
    "| **Attention Mechanism** | Direct connections between any words | Better understanding of context |\n",
    "| **Scalability** | Works better with more data/parameters | Powers models from millions to trillions of parameters |\n",
    "| **Transfer Learning** | Pre-train once, adapt to many tasks | Enables ChatGPT, GPT-4, BERT, and more |\n",
    "\n",
    "The **attention mechanism** is what makes Transformers special. It allows the model to focus on relevant parts of the input when processing each word - just like how you naturally focus on important words when reading!\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n",
    "  <div style=\"width: 40%; text-align: center;\">\n",
    "    <img src=\"https://substackcdn.com/image/fetch/$s_!jtT-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4d7dc94-6f18-4973-a501-de1d5b101c10_1903x856.png\" width=\"100%\"/>\n",
    "    <p><i>Attention Mechanism</i></p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"width: 30%; text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" width=\"100%\"/>\n",
    "    <p><i>The Transformer Architecture</i></p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "In this notebook, we'll understand **how attention works** - the fundamental mechanism behind all modern language models. By the end, you'll know exactly how ChatGPT, GPT-4, and other AI systems process and understand language!\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b174d",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: The Intuition Behind Attention\n",
    "\n",
    "Before diving into code, let's build strong intuition!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67803496",
   "metadata": {},
   "source": [
    "Let's solidify your understanding with a library analogy:\n",
    "\n",
    "**üèõÔ∏è The Library Analogy**\n",
    "\n",
    "Imagine you're researching \"climate change impacts\" in a library:\n",
    "\n",
    "**Your Query:** \"How does climate change affect polar bears?\"\n",
    "\n",
    "**The Library Catalog (Keys):**\n",
    "- Book 1: \"Climate Change Overview\" üîë\n",
    "- Book 2: \"Polar Bear Biology\" üîë\n",
    "- Book 3: \"Arctic Ecosystems\" üîë\n",
    "- Book 4: \"17th Century Poetry\" üîë\n",
    "- Book 5: \"Ocean Acidification\" üîë\n",
    "\n",
    "**What You Do:**\n",
    "\n",
    "1. **Compare Query with Keys** (Matching step)\n",
    "   - Your query ‚Üî \"Climate Change Overview\": High relevance! ‚úÖ\n",
    "   - Your query ‚Üî \"Polar Bear Biology\": High relevance! ‚úÖ\n",
    "   - Your query ‚Üî \"Arctic Ecosystems\": Medium relevance ‚úì\n",
    "   - Your query ‚Üî \"17th Century Poetry\": No relevance ‚ùå\n",
    "   - Your query ‚Üî \"Ocean Acidification\": Low relevance\n",
    "\n",
    "2. **Assign Attention Weights** (based on relevance)\n",
    "   - Book 1: 0.35 (35% attention)\n",
    "   - Book 2: 0.40 (40% attention) üéØ\n",
    "   - Book 3: 0.20 (20% attention)\n",
    "   - Book 4: 0.00 (0% attention)\n",
    "   - Book 5: 0.05 (5% attention)\n",
    "\n",
    "3. **Read Content (Values) Proportionally**\n",
    "   - Spend 40% of your time on \"Polar Bear Biology\"\n",
    "   - Spend 35% on \"Climate Change Overview\"\n",
    "   - Spend 20% on \"Arctic Ecosystems\"\n",
    "   - Skip \"17th Century Poetry\" entirely\n",
    "   - Briefly skim \"Ocean Acidification\"\n",
    "\n",
    "4. **Synthesize Information** (Weighted aggregation)\n",
    "   - Your final understanding = \n",
    "     - 0.40 √ó (Polar Bear content) +\n",
    "     - 0.35 √ó (Climate content) +\n",
    "     - 0.20 √ó (Arctic content) +\n",
    "     - 0.05 √ó (Ocean content)\n",
    "\n",
    "**Mapping to Attention:**\n",
    "\n",
    "| Library Concept | Attention Mechanism |\n",
    "|----------------|--------------------|\n",
    "| Your research question | **Query (Q)** |\n",
    "| Book titles in catalog | **Keys (K)** |\n",
    "| Book contents | **Values (V)** |\n",
    "| Relevance matching | **Q¬∑K (dot product)** |\n",
    "| Time allocation | **Attention weights (Œ±)** |\n",
    "| Final understanding | **Output (weighted sum of V)** |\n",
    "\n",
    "**The Formula Revealed:**\n",
    "\n",
    "$$\\text{Understanding} = \\sum_{i} \\alpha_i \\cdot \\text{Book}_i$$\n",
    "\n",
    "$$\\text{where } \\alpha_i = \\text{softmax}(\\frac{\\text{Query} \\cdot \\text{Key}_i}{\\sqrt{d}})$$\n",
    "\n",
    "This is **exactly** how attention mechanisms work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712909b7",
   "metadata": {},
   "source": [
    "---\n",
    "## üíª Section 3: Implementation\n",
    "\n",
    "Now let's implement attention step-by-step! We'll build up to the complete formula:\n",
    "\n",
    "| | |\n",
    "| :---: | :---: |\n",
    "| $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$ | <img src=\"https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Fba830026-6d8f-4e77-b288-f75dd3a51457%2Fimage.png\" width=\"400\" alt=\"Attention Formula Diagram\"/> |\n",
    "\n",
    "Let's start with some random input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24f447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60c962",
   "metadata": {},
   "source": [
    "### üìä Creating Sample Input\n",
    "\n",
    "We'll create random input data to work with:\n",
    "- **B = 4**: Batch size (4 sequences)\n",
    "- **T = 8**: Time/sequence length (8 tokens per sequence)\n",
    "- **C = 65**: Channels/embedding dimension (same as our vocab size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1f2418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,65 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a36140",
   "metadata": {},
   "source": [
    "---\n",
    "### üîí The Look-Ahead Problem (Why We Need Masking)\n",
    "\n",
    "**Critical Concept for Language Models!**\n",
    "\n",
    "When training autoregressive models (like GPT), the model predicts the next token based ONLY on previous tokens:\n",
    "\n",
    "```\n",
    "Training sequence: \"The cat sat on the mat\"\n",
    "\n",
    "Predicting position 3 (\"sat\"):\n",
    "‚úÖ Can see: \"The\", \"cat\"\n",
    "‚ùå Cannot see: \"sat\", \"on\", \"the\", \"mat\" (these are in the future!)\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "- Without masking, during training the model would see the answer before predicting it (cheating! üö´)\n",
    "- At test time, it won't have access to future tokens\n",
    "- This mismatch causes poor generalization\n",
    "\n",
    "**The Solution - Causal Mask:**\n",
    "\n",
    "A **lower triangular matrix** that only allows attention to previous positions:\n",
    "\n",
    "$$\\text{Mask} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Position 1 can only see position 1. Position 2 can see positions 1-2. And so on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02bb95",
   "metadata": {},
   "source": [
    "Let's create this mask using PyTorch's `tril` (lower triangular) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a272d4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mask\n",
    "\n",
    "mask = torch.tril(torch.ones(T,T))\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482b62c",
   "metadata": {},
   "source": [
    "---\n",
    "### üîÑ Version 1: Simple Averaging (Uniform Attention)\n",
    "\n",
    "**Idea:** Each position averages ALL previous positions equally.\n",
    "\n",
    "**How it works:**\n",
    "1. Take the mask (1s and 0s)\n",
    "2. Normalize each row so it sums to 1 (divide by row sum)\n",
    "3. Multiply with input to get weighted average\n",
    "\n",
    "**Example for position 3:**\n",
    "```\n",
    "mask row 3: [1, 1, 1, 0, 0, 0, 0, 0]\n",
    "normalized: [0.33, 0.33, 0.33, 0, 0, 0, 0, 0]  (sums to 1!)\n",
    "```\n",
    "\n",
    "This means position 3's output is: (1/3 √ó token_1) + (1/3 √ó token_2) + (1/3 √ó token_3)\n",
    "\n",
    "**Limitation:** Every previous token gets EQUAL weight. But shouldn't some tokens be more important than others? ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e453dcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 1\n",
    "\n",
    "attn_scores = mask / mask.sum(1, keepdim = True) # normalize the rows\n",
    "out_1 = attn_scores @ x # (T,T) @ (B,T,C) ---> (B,T,T) @ (B,T,C) ---> (B,T,C)\n",
    "out_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae7bfe",
   "metadata": {},
   "source": [
    "---\n",
    "### üéØ Version 2: Using Softmax (The Real Way!)\n",
    "\n",
    "**Idea:** Use `softmax` for normalization, but first set future positions to `-inf`.\n",
    "\n",
    "**Why `-inf`?**\n",
    "- `softmax(-inf) = 0` (mathematically!)\n",
    "- So future positions contribute ZERO to the output\n",
    "- All other positions share the remaining weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0921fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2\n",
    "\n",
    "attn_scores = torch.zeros(T,T)\n",
    "attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "out_2 = attn_scores @ x\n",
    "out_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccbf659",
   "metadata": {},
   "source": [
    "**Why Version 2 is Better:**\n",
    "- Version 1 only works with uniform weights\n",
    "- Version 2 can have ANY starting weights (we'll learn these!)\n",
    "- The `-inf` masking trick is standard in all Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf971d",
   "metadata": {},
   "source": [
    "---\n",
    "### üîë Adding Query, Key, Value: The Complete Attention!\n",
    "\n",
    "Both versions above use **uniform weights** - every token gets equal attention. But we want **learned, content-dependent weights**!\n",
    "\n",
    "**The Query-Key-Value Framework:**\n",
    "\n",
    "Every token produces THREE vectors:\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What do I contain?\"\n",
    "- **Value (V)**: \"What information will I share?\"\n",
    "\n",
    "**How Attention Works:**\n",
    "1. Each token's Query \"asks a question\"\n",
    "2. All Keys \"answer\" how relevant they are (via dot product)\n",
    "3. High Q¬∑K score = high attention weight\n",
    "4. Values are combined using these weights\n",
    "\n",
    "**The Magic Formula:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The $\\sqrt{d_k}$ scaling prevents dot products from getting too large (which would make softmax too \"peaky\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647bf0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "torch.Size([4, 8, 65])\n",
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "head_size = 65\n",
    "key = torch.nn.Linear(C, head_size, bias = False)\n",
    "query = torch.nn.Linear(C, head_size, bias = False)\n",
    "value = torch.nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "\n",
    "print(k.shape)\n",
    "print(q.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a82b5",
   "metadata": {},
   "source": [
    "Let's create the Q, K, V projection layers. These are **learned** linear transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44405bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = q @ k.transpose(-2,-1) / head_size**0.5  # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "attn_scores = torch.softmax(attn_scores, dim=-1)\n",
    "out = attn_scores @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831a142",
   "metadata": {},
   "source": [
    "Now let's compute **self-attention** step by step:\n",
    "\n",
    "1. **Compute attention scores**: $QK^T$ (how much each token attends to others)\n",
    "2. **Scale**: Divide by $\\sqrt{d_k}$ to stabilize gradients\n",
    "3. **Mask**: Set future positions to `-inf`\n",
    "4. **Softmax**: Normalize to get attention weights (sum to 1)\n",
    "5. **Apply to Values**: Weighted combination of V vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909ce77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [7.1821e-01, 2.8179e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [6.2292e-01, 2.6785e-01, 1.0923e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [2.7394e-02, 3.5937e-02, 8.8566e-02, 8.4810e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [9.2227e-01, 1.0381e-02, 4.4360e-02, 7.6919e-04, 2.2219e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [7.4951e-02, 1.1837e-01, 2.4863e-01, 3.7787e-01, 6.3060e-03, 1.7387e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [1.8730e-01, 5.8106e-02, 6.1382e-02, 3.6453e-03, 6.4791e-01, 1.8433e-02,\n",
       "         2.3230e-02, 0.0000e+00],\n",
       "        [4.1080e-01, 6.0570e-02, 2.1063e-02, 1.6063e-03, 1.6883e-01, 1.5380e-02,\n",
       "         4.0297e-03, 3.1772e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the attention weights for the first sequence\n",
    "print(\"Attention weights for first sequence:\")\n",
    "print(attn_scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddaf0d1",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Summary: What We Learned\n",
    "\n",
    "### üéØ Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Attention** | Mechanism allowing tokens to \"communicate\" and share information |\n",
    "| **Query (Q)** | \"What am I looking for?\" - the question each token asks |\n",
    "| **Key (K)** | \"What do I contain?\" - the relevance signal each token provides |\n",
    "| **Value (V)** | \"What information do I share?\" - the content passed forward |\n",
    "| **Causal Mask** | Prevents looking at future tokens (critical for language models!) |\n",
    "| **Softmax** | Normalizes attention weights to sum to 1 |\n",
    "| **Scaling (‚àöd)** | Prevents dot products from getting too large |\n",
    "\n",
    "### üîÑ The Attention Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### üìä Version Comparison\n",
    "\n",
    "| Version | Description | Weights | Use Case |\n",
    "|:-------:|-------------|---------|----------|\n",
    "| **V1** | Simple averaging | Uniform (1/n) | Understanding the concept |\n",
    "| **V2** | Softmax with masking | Uniform (but flexible) | Foundation for real attention |\n",
    "| **Full** | Q-K-V attention | Learned, content-dependent | Real Transformers! |\n",
    "\n",
    "### ‚û°Ô∏è What's Next?\n",
    "\n",
    "In Day 8, we'll learn:\n",
    "- üß† **Multi-Head Attention**: Run multiple attention \"heads\" in parallel\n",
    "- üèóÔ∏è **Transformer Blocks**: Combine attention with feed-forward layers\n",
    "- üìà **Scaling Up**: Build a real GPT-style language model!\n",
    "\n",
    "**Congratulations!** You now understand the core mechanism behind ChatGPT, GPT-4, Claude, and all modern language models! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a5d6d5",
   "metadata": {},
   "source": [
    "### üîç Visualizing Attention Weights\n",
    "\n",
    "Let's look at the attention weights. Notice:\n",
    "- Each row sums to 1 (softmax!)\n",
    "- Upper triangle is 0 (causal mask!)\n",
    "- Values vary based on content (learned Q, K!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
