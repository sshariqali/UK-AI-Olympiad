{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1befbb03",
   "metadata": {},
   "source": [
    "# ğŸ”´ Task 3 â€” English â†’ French Translator (Encoder-Decoder Transformer)\n",
    "\n",
    "---\n",
    "\n",
    "### **Building on Day 8: From Decoder-Only GPT â†’ Full Encoder-Decoder Transformer**\n",
    "\n",
    "In Day 8 (`hands_on_part_7_scaling.ipynb`) we built a **decoder-only** GPT that generates Shakespeare. Now we extend it into an **encoder-decoder** Transformer for **machine translation**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Changes from Day 8?**\n",
    "\n",
    "| Component | Day 8 (GPT) | Task 3 (Translator) | Status |\n",
    "|-----------|-------------|---------------------|--------|\n",
    "| `Head` | Self-attention only (Q,K,V from same input) | Accepts **optional external K,V** for cross-attention | ğŸ”„ MODIFIED |\n",
    "| `MultiHeadAttention` | Forwards single input `x` | Forwards **separate `x` and optional `context`** | ğŸ”„ MODIFIED |\n",
    "| `FeedForward` | Expand â†’ ReLU â†’ Contract â†’ Dropout | **Identical** | âœ… UNCHANGED |\n",
    "| `Block` (Decoder) | Self-Attention â†’ FFN (2 sublayers) | Self-Attention â†’ **Cross-Attention** â†’ FFN (**3 sublayers**) | ğŸ”„ MODIFIED |\n",
    "| `EncoderBlock` | â€” | Self-Attention â†’ FFN (**no causal mask**) | ğŸ†• NEW |\n",
    "| `Encoder` | â€” | Embedding + Positional Enc + N Ã— EncoderBlock | ğŸ†• NEW |\n",
    "| `Decoder` | Was the whole `Transformer` class | Embedding + Positional Enc + N Ã— DecoderBlock | ğŸ”„ MODIFIED |\n",
    "| `EncoderDecoderTransformer` | â€” | Encoder + Decoder + Linear head | ğŸ†• NEW |\n",
    "| Data pipeline | Character-level, single text | **Word-level**, parallel ENâ†”FR pairs, `<PAD>` `<SOS>` `<EOS>` | ğŸ†• NEW |\n",
    "| Generation | Autoregressive sampling | **Greedy decoding** with `<SOS>` â†’ `<EOS>` | ğŸ†• NEW |\n",
    "\n",
    "> **Legend:** âœ… UNCHANGED &nbsp;|&nbsp; ğŸ”„ MODIFIED &nbsp;|&nbsp; ğŸ†• NEW\n",
    "\n",
    "---\n",
    "\n",
    "### **Architecture Overview**\n",
    "\n",
    "```\n",
    "English Input                        French Output\n",
    "\"I love you\"                         \"<SOS> Je t'aime <EOS>\"\n",
    "     â†“                                        â†“\n",
    "[Token Embedding]                    [Token Embedding]\n",
    "     â†“                                        â†“\n",
    "[Positional Encoding]                [Positional Encoding]\n",
    "     â†“                                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   ENCODER    â”‚                    â”‚      DECODER         â”‚\n",
    "â”‚              â”‚                    â”‚                      â”‚\n",
    "â”‚ Self-Attn    â”‚                    â”‚ Masked Self-Attn     â”‚\n",
    "â”‚ + Add & Norm â”‚    enc_output      â”‚ + Add & Norm         â”‚\n",
    "â”‚              â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚                      â”‚\n",
    "â”‚ FFN          â”‚                    â”‚ Cross-Attn (Q=dec,   â”‚\n",
    "â”‚ + Add & Norm â”‚                    â”‚   K,V=enc_output)    â”‚\n",
    "â”‚              â”‚                    â”‚ + Add & Norm         â”‚\n",
    "â”‚ Ã— N layers   â”‚                    â”‚                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚ FFN                  â”‚\n",
    "                                    â”‚ + Add & Norm         â”‚\n",
    "                                    â”‚                      â”‚\n",
    "                                    â”‚ Ã— N layers           â”‚\n",
    "                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                             â†“\n",
    "                                    [Linear â†’ Vocab Logits]\n",
    "                                             â†“\n",
    "                                    \"Je t'aime\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af03f578",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3245ad",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Hyperparameters\n",
    "\n",
    "ğŸ”„ **MODIFIED from Day 8** â€” Adjusted for a word-level translation task on CPU.\n",
    "\n",
    "| Parameter | Day 8 (GPT) | Task 3 (Translator) | Why? |\n",
    "|-----------|-------------|---------------------|------|\n",
    "| `batch_size` | 32 | **64** | Shorter sequences, can fit more |\n",
    "| `block_size` | 256 | **20** | Max sentence length (words, not chars) |\n",
    "| `n_embed` | 384 | **128** | Smaller model for CPU training |\n",
    "| `n_head` | 6 | **4** | Fewer heads for smaller d_model |\n",
    "| `n_layer` | 6 | **3** | Fewer layers for tractable training |\n",
    "| `dropout` | 0.2 | **0.1** | Less dropout for smaller model |\n",
    "| `max_iters` | 5000 | **8000** | More iterations needed for translation |\n",
    "| `learning_rate` | 3e-4 | **3e-4** | Same |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Hyperparameters  (ğŸ”„ MODIFIED from Day 8)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "batch_size  = 64\n",
    "block_size  = 20       # max sentence length in tokens (words)\n",
    "max_iters   = 8000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters  = 100\n",
    "n_embed     = 128      # embedding dimension\n",
    "n_head      = 4        # number of attention heads\n",
    "n_layer     = 3        # number of encoder/decoder blocks\n",
    "dropout     = 0.1\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f924f28",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: ğŸ†• Data Pipeline (NEW â€” Word-Level ENâ†”FR Pairs)\n",
    "\n",
    "This is entirely new compared to Day 8's character-level Shakespeare pipeline.\n",
    "\n",
    "We will:\n",
    "1. Download Englishâ†’French sentence pairs from Tatoeba / ManyThings.org\n",
    "2. Clean and filter to short pairs (â‰¤ `block_size` words)\n",
    "3. Build word-level vocabularies with special tokens: `<PAD>`, `<SOS>`, `<EOS>`\n",
    "4. Convert sentences to padded integer tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ed844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Download the dataset\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "data_url  = \"https://www.manythings.org/anki/fra-eng.zip\"\n",
    "zip_path  = \"fra-eng.zip\"\n",
    "txt_file  = \"fra.txt\"\n",
    "\n",
    "if not os.path.exists(txt_file):\n",
    "    print(\"Downloading English-French dataset...\")\n",
    "    urllib.request.urlretrieve(data_url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(\".\")\n",
    "    print(\"âœ… Download complete!\")\n",
    "else:\n",
    "    print(\"âœ… Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc01cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Load and clean sentence pairs\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"Lowercase, strip accents (optional), and clean punctuation.\"\"\"\n",
    "    s = s.lower().strip()\n",
    "    # Separate punctuation with spaces so they become their own tokens\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # Remove anything that isn't a letter, common punctuation, or space\n",
    "    s = re.sub(r\"[^a-zA-ZÃ©Ã¨ÃªÃ«Ã Ã¢Ã¤Ã¹Ã»Ã¼Ã´Ã¶Ã®Ã¯Ã§Å“Ã¦ .!?'-]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read file â€” format: English\\tFrench\\tAttribution\n",
    "pairs = []\n",
    "with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) >= 2:\n",
    "            en = normalize_string(parts[0])\n",
    "            fr = normalize_string(parts[1])\n",
    "            pairs.append((en, fr))\n",
    "\n",
    "print(f\"Total pairs loaded: {len(pairs):,}\")\n",
    "print(f\"\\nExample pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  EN: {pairs[i][0]}\")\n",
    "    print(f\"  FR: {pairs[i][1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Filter to short sentences (tractable on CPU)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MAX_WORDS = block_size - 2  # leave room for <SOS> and <EOS>\n",
    "NUM_PAIRS = 20000            # use first N pairs (already sorted shortest-first)\n",
    "\n",
    "filtered_pairs = []\n",
    "for en, fr in pairs[:50000]:  # search within first 50k\n",
    "    en_words = en.split()\n",
    "    fr_words = fr.split()\n",
    "    if len(en_words) <= MAX_WORDS and len(fr_words) <= MAX_WORDS:\n",
    "        filtered_pairs.append((en, fr))\n",
    "    if len(filtered_pairs) >= NUM_PAIRS:\n",
    "        break\n",
    "\n",
    "# Shuffle for randomness\n",
    "random.seed(42)\n",
    "random.shuffle(filtered_pairs)\n",
    "\n",
    "print(f\"Filtered pairs: {len(filtered_pairs):,}\")\n",
    "print(f\"Max words per sentence: {MAX_WORDS}\")\n",
    "print(f\"\\nSample filtered pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  EN: {filtered_pairs[i][0]}\")\n",
    "    print(f\"  FR: {filtered_pairs[i][1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4a953",
   "metadata": {},
   "source": [
    "### ğŸ†• Building Word-Level Vocabularies\n",
    "\n",
    "In Day 8 we had a **character-level** tokenizer (`stoi` / `itos`) built from all unique characters. Here we build a **word-level** tokenizer with special tokens.\n",
    "\n",
    "| Special Token | Purpose |\n",
    "|:---:|---|\n",
    "| `<PAD>` (0) | Padding â€” fills shorter sentences to `block_size` |\n",
    "| `<SOS>` (1) | Start-Of-Sequence â€” tells the decoder to start generating |\n",
    "| `<EOS>` (2) | End-Of-Sequence â€” signals the sentence is complete |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203cc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Word-level tokenizer with special tokens\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Compare with Day 8:\n",
    "#   Day 8:  stoi = { ch: i for i, ch in enumerate(chars) }  â† character-level\n",
    "#   Task 3: stoi = { word: i for i, word in enumerate(vocab) }  â† word-level + special tokens\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"Build word vocabulary from list of sentences.\"\"\"\n",
    "    word_counts = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Special tokens first, then sorted vocabulary\n",
    "    special_tokens = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "    vocab_words = special_tokens + sorted(word_counts.keys())\n",
    "    \n",
    "    stoi = {word: i for i, word in enumerate(vocab_words)}\n",
    "    itos = {i: word for word, i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "# Build separate vocabularies for English and French\n",
    "en_sentences = [p[0] for p in filtered_pairs]\n",
    "fr_sentences = [p[1] for p in filtered_pairs]\n",
    "\n",
    "en_stoi, en_itos = build_vocab(en_sentences)\n",
    "fr_stoi, fr_itos = build_vocab(fr_sentences)\n",
    "\n",
    "# Vocab sizes\n",
    "src_vocab_size = len(en_stoi)\n",
    "tgt_vocab_size = len(fr_stoi)\n",
    "\n",
    "PAD_IDX = en_stoi[PAD_TOKEN]  # 0\n",
    "SOS_IDX = en_stoi[SOS_TOKEN]  # 1\n",
    "EOS_IDX = en_stoi[EOS_TOKEN]  # 2\n",
    "\n",
    "print(f\"English vocabulary size: {src_vocab_size:,}\")\n",
    "print(f\"French vocabulary size:  {tgt_vocab_size:,}\")\n",
    "print(f\"\\nSpecial token indices: PAD={PAD_IDX}, SOS={SOS_IDX}, EOS={EOS_IDX}\")\n",
    "print(f\"\\nSample English tokens: {list(en_stoi.items())[:8]}\")\n",
    "print(f\"Sample French tokens:  {list(fr_stoi.items())[:8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Encode sentences to padded tensors\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def encode_sentence(sentence, stoi, max_len):\n",
    "    \"\"\"Encode a sentence to a list of token indices with <SOS>, <EOS>, and <PAD>.\"\"\"\n",
    "    words = sentence.split()\n",
    "    tokens = [stoi[SOS_TOKEN]] + [stoi.get(w, stoi[PAD_TOKEN]) for w in words] + [stoi[EOS_TOKEN]]\n",
    "    # Pad to max_len\n",
    "    tokens = tokens + [stoi[PAD_TOKEN]] * (max_len - len(tokens))\n",
    "    return tokens[:max_len]  # truncate if needed\n",
    "\n",
    "def decode_sentence(token_ids, itos):\n",
    "    \"\"\"Decode token ids back to a sentence string.\"\"\"\n",
    "    words = []\n",
    "    for idx in token_ids:\n",
    "        word = itos[idx]\n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "        if word not in (PAD_TOKEN, SOS_TOKEN):\n",
    "            words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Encode all pairs\n",
    "src_data = []  # English (encoder input)\n",
    "tgt_data = []  # French  (decoder input/target)\n",
    "\n",
    "for en, fr in filtered_pairs:\n",
    "    src_data.append(encode_sentence(en, en_stoi, block_size))\n",
    "    tgt_data.append(encode_sentence(fr, fr_stoi, block_size))\n",
    "\n",
    "src_data = torch.tensor(src_data, dtype=torch.long)\n",
    "tgt_data = torch.tensor(tgt_data, dtype=torch.long)\n",
    "\n",
    "print(f\"Source (EN) tensor shape: {src_data.shape}\")\n",
    "print(f\"Target (FR) tensor shape: {tgt_data.shape}\")\n",
    "\n",
    "# Verify encoding/decoding\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  EN original: {filtered_pairs[0][0]}\")\n",
    "print(f\"  EN encoded:  {src_data[0].tolist()}\")\n",
    "print(f\"  EN decoded:  {decode_sentence(src_data[0].tolist(), en_itos)}\")\n",
    "print(f\"  FR original: {filtered_pairs[0][1]}\")\n",
    "print(f\"  FR encoded:  {tgt_data[0].tolist()}\")\n",
    "print(f\"  FR decoded:  {decode_sentence(tgt_data[0].tolist(), fr_itos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43020c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Train / Validation split\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Day 8 used 90/10 split on characters. We do the same on sentence pairs.\n",
    "\n",
    "n = int(len(src_data) * 0.9)\n",
    "train_src, val_src = src_data[:n], src_data[n:]\n",
    "train_tgt, val_tgt = tgt_data[:n], tgt_data[n:]\n",
    "\n",
    "print(f\"Training pairs:   {len(train_src):,}\")\n",
    "print(f\"Validation pairs: {len(val_src):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a777df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ”„ Batch loading (MODIFIED from Day 8)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Day 8 sampled random windows from one long text.\n",
    "# Here we sample random (src, tgt) sentence pairs.\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"Sample a batch of (source, target) pairs.\"\"\"\n",
    "    if split == 'train':\n",
    "        src, tgt = train_src, train_tgt\n",
    "    else:\n",
    "        src, tgt = val_src, val_tgt\n",
    "    \n",
    "    ix = torch.randint(len(src), (batch_size,))\n",
    "    src_batch = src[ix].to(device)  # (B, T)\n",
    "    tgt_batch = tgt[ix].to(device)  # (B, T)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87853331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ”„ Loss estimation (MODIFIED from Day 8)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Same structure as Day 8, but now we pass src and tgt separately,\n",
    "# and ignore loss on <PAD> tokens.\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            src_batch, tgt_batch = get_batch(split)\n",
    "            # Decoder input: all tokens except last\n",
    "            # Decoder target: all tokens except first (<SOS>)\n",
    "            dec_input = tgt_batch[:, :-1]\n",
    "            dec_target = tgt_batch[:, 1:]\n",
    "            logits = model(src_batch, dec_input)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = dec_target.reshape(B * T)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=PAD_IDX)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4b3bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Model Components\n",
    "\n",
    "We now build the model piece by piece, clearly marking what's changed from Day 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea193f35",
   "metadata": {},
   "source": [
    "### 4.1 `Head` â€” Single Attention Head\n",
    "\n",
    "ğŸ”„ **MODIFIED from Day 8**\n",
    "\n",
    "**Day 8:** Q, K, V all come from the **same** input `x`.\n",
    "**Task 3:** Q comes from `x`, but K and V can come from a **different** source (encoder output) â€” this is what makes **cross-attention** possible!\n",
    "\n",
    "```python\n",
    "# Day 8 (self-attention only):\n",
    "def forward(self, x):\n",
    "    k = self.key(x)        # K from x\n",
    "    q = self.query(x)      # Q from x\n",
    "    v = self.value(x)      # V from x\n",
    "\n",
    "# Task 3 (self-attention OR cross-attention):\n",
    "def forward(self, x, context=None):\n",
    "    source = context if context is not None else x\n",
    "    k = self.key(source)   # K from context (encoder output) or x\n",
    "    q = self.query(x)      # Q always from x (decoder state)\n",
    "    v = self.value(source)  # V from context (encoder output) or x\n",
    "```\n",
    "\n",
    "> **This is the KEY change** that enables the encoder-decoder architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b515587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Head  (ğŸ”„ MODIFIED â€” now supports cross-attention)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of attention.\n",
    "    \n",
    "    ğŸ”„ CHANGE from Day 8:\n",
    "       - Added `context` parameter to forward()\n",
    "       - When context is None  â†’ self-attention  (same as Day 8)\n",
    "       - When context is given â†’ cross-attention  (Q from x, K/V from context)\n",
    "       - Causal mask only applied when use_causal_mask=True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, use_causal_mask=True):          # ğŸ”„ NEW parameter\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.use_causal_mask = use_causal_mask                    # ğŸ”„ NEW\n",
    "        if use_causal_mask:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context=None):                           # ğŸ”„ NEW parameter\n",
    "        B, T, C = x.shape\n",
    "        source = context if context is not None else x            # ğŸ”„ NEW line\n",
    "        \n",
    "        q = self.query(x)                                         # Q always from x\n",
    "        k = self.key(source)                                      # ğŸ”„ K from source\n",
    "        v = self.value(source)                                    # ğŸ”„ V from source\n",
    "        \n",
    "        # Attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        \n",
    "        # Causal mask only for decoder self-attention                ğŸ”„ CONDITIONAL\n",
    "        if self.use_causal_mask:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706e3a5",
   "metadata": {},
   "source": [
    "### 4.2 `MultiHeadAttention`\n",
    "\n",
    "ğŸ”„ **MODIFIED from Day 8** â€” Passes `context` through to each head.\n",
    "\n",
    "```python\n",
    "# Day 8:\n",
    "def forward(self, x):\n",
    "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "# Task 3:\n",
    "def forward(self, x, context=None):         # ğŸ”„ Added context parameter\n",
    "    out = torch.cat([h(x, context) for h in self.heads], dim=-1)  # ğŸ”„ Pass context\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9595c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MultiHeadAttention  (ğŸ”„ MODIFIED â€” passes context)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of attention in parallel.\n",
    "    \n",
    "    ğŸ”„ CHANGE from Day 8: Passes optional `context` to each Head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, use_causal_mask=True):  # ğŸ”„ NEW param\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, use_causal_mask=use_causal_mask)        # ğŸ”„ pass mask flag\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context=None):                              # ğŸ”„ NEW parameter\n",
    "        out = torch.cat([h(x, context) for h in self.heads], dim=-1) # ğŸ”„ pass context\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79652b3a",
   "metadata": {},
   "source": [
    "### 4.3 `FeedForward`\n",
    "\n",
    "âœ… **UNCHANGED from Day 8** â€” Exactly the same expand â†’ ReLU â†’ contract â†’ dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# FeedForward  (âœ… UNCHANGED from Day 8)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network â€” identical to Day 8.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * 4, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ecada0",
   "metadata": {},
   "source": [
    "### 4.4 `EncoderBlock` ğŸ†• NEW\n",
    "\n",
    "The Encoder block is simpler than the Decoder block â€” it has **no causal mask** and **no cross-attention**:\n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚     Self-Attention       â”‚  â† Bidirectional (no mask!)\n",
    "    â”‚     (see ALL tokens)     â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚     Add & LayerNorm      â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚     Feed-Forward         â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚     Add & LayerNorm      â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Compare with **Day 8's `Block`** which always used a causal mask. The Encoder block is the same structure but allows each token to attend to **all** other tokens (including future ones), since we want to fully understand the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# EncoderBlock  (ğŸ†• NEW â€” no causal mask)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Encoder block: Self-Attention (bidirectional) + FFN.\n",
    "    \n",
    "    ğŸ†• This is like Day 8's Block, but with use_causal_mask=False\n",
    "    so every token can attend to every other token â€” no masking!\n",
    "    \n",
    "    Day 8 Block had:    self.sa = MultiHeadAttention(n_head, head_size)  â† causal mask ON\n",
    "    EncoderBlock has:   self.sa = MultiHeadAttention(n_head, head_size, use_causal_mask=False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa   = MultiHeadAttention(n_head, head_size, use_causal_mask=False)  # ğŸ†• No mask\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1  = nn.LayerNorm(n_embed)\n",
    "        self.ln2  = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))        # Self-attention + residual\n",
    "        x = x + self.ffwd(self.ln2(x))      # FFN + residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519a039",
   "metadata": {},
   "source": [
    "### 4.5 `DecoderBlock` ğŸ”„ MODIFIED\n",
    "\n",
    "The Decoder block has **3 sublayers** instead of Day 8's 2:\n",
    "\n",
    "```\n",
    "Day 8 Block (2 sublayers):           Task 3 DecoderBlock (3 sublayers):\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Masked Self-Attention  â”‚          â”‚ Masked Self-Attention  â”‚  â† Same as Day 8\n",
    "â”‚ + Add & Norm           â”‚          â”‚ + Add & Norm           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Feed-Forward           â”‚          â”‚ Cross-Attention        â”‚  â† ğŸ†• NEW sublayer!\n",
    "â”‚ + Add & Norm           â”‚          â”‚ (Q=decoder, K,V=encoderâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  output)               â”‚\n",
    "                                    â”‚ + Add & Norm           â”‚\n",
    "                                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "                                    â”‚ Feed-Forward           â”‚  â† Same as Day 8\n",
    "                                    â”‚ + Add & Norm           â”‚\n",
    "                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "> **The cross-attention sublayer is the bridge between encoder and decoder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DecoderBlock  (ğŸ”„ MODIFIED â€” adds cross-attention sublayer)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Decoder block: Masked Self-Attention + Cross-Attention + FFN.\n",
    "    \n",
    "    ğŸ”„ CHANGES from Day 8's Block:\n",
    "       1. Self-attention still has causal mask (same as Day 8)\n",
    "       2. ğŸ†• Added cross-attention sublayer (Q from decoder, K/V from encoder)\n",
    "       3. ğŸ†• Added extra LayerNorm (ln2) for cross-attention\n",
    "       4. forward() now takes `enc_output` parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        \n",
    "        # Sublayer 1: Masked self-attention (same as Day 8)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, use_causal_mask=True)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        \n",
    "        # Sublayer 2: ğŸ†• Cross-attention (no causal mask â€” attend to full encoder output)\n",
    "        self.cross_attn = MultiHeadAttention(n_head, head_size, use_causal_mask=False)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        \n",
    "        # Sublayer 3: Feed-forward (same as Day 8)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln3 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x, enc_output):                            # ğŸ”„ NEW parameter\n",
    "        # Sublayer 1: Masked self-attention + residual (same as Day 8)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # Sublayer 2: ğŸ†• Cross-attention + residual\n",
    "        # Q comes from decoder (x), K and V come from encoder output\n",
    "        x = x + self.cross_attn(self.ln2(x), context=enc_output) # ğŸ†• This line is NEW!\n",
    "        \n",
    "        # Sublayer 3: FFN + residual (same as Day 8)\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe17be",
   "metadata": {},
   "source": [
    "### 4.6 ğŸ†• Sinusoidal Positional Encoding\n",
    "\n",
    "Day 8 used **learned** positional embeddings (`nn.Embedding(block_size, n_embed)`).\n",
    "\n",
    "For the encoder-decoder, we use **sinusoidal** positional encoding (from the original \"Attention Is All You Need\" paper), as covered in `encoder.ipynb`:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right), \\quad PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "> Either approach works â€” we use sinusoidal here for variety and to match the original Transformer paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PositionalEncoding  (ğŸ†• NEW â€” sinusoidal, not learned)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Day 8 used: self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "# Task 3 uses: sinusoidal encoding (no learnable parameters)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding from 'Attention Is All You Need'.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)   # odd dimensions\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e935c02",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: ğŸ†• Complete Encoder-Decoder Transformer\n",
    "\n",
    "In Day 8, the entire model was a single `Transformer` class (decoder-only).\n",
    "\n",
    "Now we split it into **three classes**: `Encoder`, `Decoder`, and `EncoderDecoderTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a17c8",
   "metadata": {},
   "source": [
    "### 5.1 `Encoder` ğŸ†• NEW\n",
    "\n",
    "The Encoder reads the full English input and produces rich contextual representations.\n",
    "\n",
    "```python\n",
    "# Day 8 had NO encoder â€” it was all one Transformer class.\n",
    "# Now the encoder is a separate component.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Encoder  (ğŸ†• NEW)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder: Embedding + Positional Encoding + N Ã— EncoderBlock.\n",
    "    \n",
    "    Takes source tokens (English) and produces contextual representations\n",
    "    that the decoder will attend to via cross-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed, padding_idx=PAD_IDX)\n",
    "        self.pos_encoding = PositionalEncoding(n_embed, max_len=block_size, dropout_rate=dropout)\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (B, T_src) â€” English token indices\n",
    "        x = self.token_embedding(src)   # (B, T_src, n_embed)\n",
    "        x = self.pos_encoding(x)        # (B, T_src, n_embed)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return x  # (B, T_src, n_embed) â€” encoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea45a3",
   "metadata": {},
   "source": [
    "### 5.2 `Decoder` ğŸ”„ MODIFIED\n",
    "\n",
    "Compare with Day 8's `Transformer` class:\n",
    "\n",
    "```python\n",
    "# Day 8 Transformer (decoder-only):\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, y=None):\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)                    # â† no encoder output needed\n",
    "        ...\n",
    "\n",
    "# Task 3 Decoder:\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed, padding_idx=PAD_IDX)\n",
    "        self.pos_encoding = PositionalEncoding(n_embed, ...)\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(...) for _ in range(n_layer)])  # ğŸ”„ DecoderBlock\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, tgt, enc_output):       # ğŸ”„ takes enc_output!\n",
    "        ...\n",
    "        for block in self.blocks:\n",
    "            x = block(x, enc_output)           # ğŸ”„ pass enc_output to each block\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Decoder  (ğŸ”„ MODIFIED from Day 8's Transformer class)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Transformer Decoder: Embedding + Positional Encoding + N Ã— DecoderBlock.\n",
    "    \n",
    "    ğŸ”„ CHANGES from Day 8:\n",
    "       1. Uses DecoderBlock (3 sublayers) instead of Block (2 sublayers)\n",
    "       2. forward() takes enc_output in addition to target tokens\n",
    "       3. Passes enc_output to each DecoderBlock for cross-attention\n",
    "       4. No lm_head here â€” that goes in the full model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed, padding_idx=PAD_IDX)\n",
    "        self.pos_encoding = PositionalEncoding(n_embed, max_len=block_size, dropout_rate=dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(n_embed, n_head) for _ in range(n_layer)    # ğŸ”„ DecoderBlock\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, tgt, enc_output):                              # ğŸ”„ NEW parameter\n",
    "        # tgt: (B, T_tgt) â€” French token indices\n",
    "        x = self.token_embedding(tgt)    # (B, T_tgt, n_embed)\n",
    "        x = self.pos_encoding(x)         # (B, T_tgt, n_embed)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, enc_output)      # ğŸ”„ pass encoder output for cross-attention\n",
    "        x = self.ln_f(x)\n",
    "        return x  # (B, T_tgt, n_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c35210",
   "metadata": {},
   "source": [
    "### 5.3 `EncoderDecoderTransformer` ğŸ†• NEW\n",
    "\n",
    "This is the complete model that ties everything together.\n",
    "\n",
    "```\n",
    "English tokens â”€â–º Encoder â”€â–º enc_output â”€â”\n",
    "                                          â”‚ (cross-attention)\n",
    "French tokens  â”€â–º Decoder â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "              Linear Head â”€â–º French vocab logits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# EncoderDecoderTransformer  (ğŸ†• NEW)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    \"\"\"Complete Encoder-Decoder Transformer for sequence-to-sequence translation.\n",
    "    \n",
    "    ğŸ†• This class has no equivalent in Day 8 â€” it combines the Encoder and Decoder\n",
    "    and adds the final linear projection to target vocabulary logits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size)\n",
    "        self.decoder = Decoder(tgt_vocab_size)\n",
    "        self.lm_head = nn.Linear(n_embed, tgt_vocab_size)   # project to French vocab\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (B, T_src) English token indices\n",
    "            tgt: (B, T_tgt) French token indices (decoder input)\n",
    "        Returns:\n",
    "            logits: (B, T_tgt, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        enc_output = self.encoder(src)            # (B, T_src, n_embed)\n",
    "        dec_output = self.decoder(tgt, enc_output) # (B, T_tgt, n_embed)\n",
    "        logits = self.lm_head(dec_output)          # (B, T_tgt, tgt_vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def translate(self, src, max_len=None):\n",
    "        \"\"\"\n",
    "        ğŸ†• Greedy decoding: translate a source sentence to target.\n",
    "        \n",
    "        Compare with Day 8's generate() which started from empty context.\n",
    "        Here we start from <SOS> and condition on the encoder output.\n",
    "        \n",
    "        Day 8:\n",
    "            idx = torch.zeros((1,1))  # start from nothing\n",
    "            logits, _ = self(idx)     # single forward pass\n",
    "            \n",
    "        Task 3:\n",
    "            enc_output = self.encoder(src)      # encode English first\n",
    "            tgt = [<SOS>]                        # start with <SOS>\n",
    "            logits = self(src_dummy, tgt)        # use encoder output\n",
    "        \"\"\"\n",
    "        if max_len is None:\n",
    "            max_len = block_size\n",
    "        \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            enc_output = self.encoder(src)   # (1, T_src, n_embed)\n",
    "            \n",
    "            # Start with <SOS> token\n",
    "            tgt_tokens = torch.full((1, 1), SOS_IDX, dtype=torch.long, device=device)\n",
    "            \n",
    "            for _ in range(max_len - 1):\n",
    "                dec_output = self.decoder(tgt_tokens, enc_output)\n",
    "                logits = self.lm_head(dec_output)\n",
    "                \n",
    "                # Greedy: pick the most likely next token\n",
    "                next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # (1, 1)\n",
    "                tgt_tokens = torch.cat([tgt_tokens, next_token], dim=1)\n",
    "                \n",
    "                # Stop if <EOS> is predicted\n",
    "                if next_token.item() == EOS_IDX:\n",
    "                    break\n",
    "        \n",
    "        return tgt_tokens[0].tolist()\n",
    "\n",
    "print(\"âœ… EncoderDecoderTransformer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c945ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Instantiate the model\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = EncoderDecoderTransformer(src_vocab_size, tgt_vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"  Encoder: {sum(p.numel() for p in model.encoder.parameters()):,}\")\n",
    "print(f\"  Decoder: {sum(p.numel() for p in model.decoder.parameters()):,}\")\n",
    "print(f\"  LM Head: {sum(p.numel() for p in model.lm_head.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098940e",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Training\n",
    "\n",
    "ğŸ”„ **MODIFIED from Day 8** â€” The training loop structure is the same, but:\n",
    "- We pass `(src, tgt_input)` instead of `(x, y)` from a single sequence\n",
    "- The decoder input is `tgt[:, :-1]` (everything except last token)\n",
    "- The target is `tgt[:, 1:]` (everything except `<SOS>`)\n",
    "- We use `ignore_index=PAD_IDX` in cross-entropy to skip padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce09b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Training Loop  (ğŸ”„ MODIFIED from Day 8)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(max_iters):\n",
    "\n",
    "    # Evaluate periodically (same as Day 8)\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        train_losses.append(losses['train'].item())\n",
    "        val_losses.append(losses['val'].item())\n",
    "        print(f\"step {i:5d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch  (ğŸ”„ MODIFIED â€” now (src, tgt) pairs)\n",
    "    src_batch, tgt_batch = get_batch('train')\n",
    "    \n",
    "    # ğŸ”„ NEW: Split target into decoder input and target labels\n",
    "    dec_input  = tgt_batch[:, :-1]   # <SOS> w1 w2 ... wN       (teacher forcing)\n",
    "    dec_target = tgt_batch[:, 1:]    # w1 w2 ... wN <EOS>/<PAD>\n",
    "\n",
    "    # Forward pass  (ğŸ”„ MODIFIED â€” pass src and dec_input)\n",
    "    logits = model(src_batch, dec_input)\n",
    "\n",
    "    # Compute loss  (ğŸ”„ MODIFIED â€” ignore PAD tokens)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B * T, C)\n",
    "    targets = dec_target.reshape(B * T)\n",
    "    loss = F.cross_entropy(logits, targets, ignore_index=PAD_IDX)\n",
    "\n",
    "    # Backprop (same as Day 8)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Final evaluation\n",
    "losses = estimate_loss()\n",
    "print(f\"\\nFinal: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Plot training curves\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.figure(figsize=(10, 5))\n",
    "steps = [i * eval_interval for i in range(len(train_losses))]\n",
    "plt.plot(steps, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(steps, val_losses, label='Val Loss', marker='s')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11accdfc",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: ğŸ†• Translation â€” The Moment of Truth!\n",
    "\n",
    "In Day 8, we generated Shakespeare from scratch. Here we **translate** English sentences into French using greedy decoding.\n",
    "\n",
    "**Day 8 generation:**\n",
    "```python\n",
    "context = torch.zeros((1, 1), dtype=torch.long)  # start from empty\n",
    "out = model.generate(context, max_new_tokens=1000)\n",
    "print(decode(out[0].tolist()))\n",
    "```\n",
    "\n",
    "**Task 3 translation:**\n",
    "```python\n",
    "src = encode_english(\"I love you\")   # encode the English input\n",
    "out = model.translate(src)            # greedy decode to French\n",
    "print(decode_french(out))             # \"je t'aime\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48366dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Translate sentences!\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def translate(english_sentence):\n",
    "    \"\"\"Translate an English sentence to French.\"\"\"\n",
    "    # Encode the English sentence\n",
    "    en_clean = normalize_string(english_sentence)\n",
    "    tokens = encode_sentence(en_clean, en_stoi, block_size)\n",
    "    src = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Translate using greedy decoding\n",
    "    output_ids = model.translate(src, max_len=block_size)\n",
    "    \n",
    "    # Decode the French output\n",
    "    french = decode_sentence(output_ids, fr_itos)\n",
    "    return french\n",
    "\n",
    "# Test on training examples\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSLATIONS FROM TRAINING DATA\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(10):\n",
    "    en, fr_expected = filtered_pairs[i]\n",
    "    fr_predicted = translate(en)\n",
    "    print(f\"\\n  EN:       {en}\")\n",
    "    print(f\"  Expected: {fr_expected}\")\n",
    "    print(f\"  Got:      {fr_predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ddd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Test on UNSEEN sentences from validation set\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSLATIONS FROM VALIDATION DATA (UNSEEN)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "val_start = int(len(filtered_pairs) * 0.9)  # validation split\n",
    "for i in range(val_start, min(val_start + 10, len(filtered_pairs))):\n",
    "    en, fr_expected = filtered_pairs[i]\n",
    "    fr_predicted = translate(en)\n",
    "    print(f\"\\n  EN:       {en}\")\n",
    "    print(f\"  Expected: {fr_expected}\")\n",
    "    print(f\"  Got:      {fr_predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68411265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Try your own sentences!\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "test_sentences = [\n",
    "    \"I am happy.\",\n",
    "    \"He is a student.\",\n",
    "    \"She is beautiful.\",\n",
    "    \"We are here.\",\n",
    "    \"It is cold.\",\n",
    "    \"I love you.\",\n",
    "    \"Thank you.\",\n",
    "    \"Good morning.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CUSTOM TRANSLATIONS\")\n",
    "print(\"=\" * 60)\n",
    "for en in test_sentences:\n",
    "    fr = translate(en)\n",
    "    print(f\"\\n  EN: {en}\")\n",
    "    print(f\"  FR: {fr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399524e2",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: ğŸ†• Visualise Attention Weights\n",
    "\n",
    "Let's peek inside the model to see what the cross-attention is learning â€” which English words does the decoder focus on when generating each French word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872142d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Attention weight visualization\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def get_cross_attention_weights(english_sentence):\n",
    "    \"\"\"Extract cross-attention weights from the last decoder layer.\"\"\"\n",
    "    model.eval()\n",
    "    en_clean = normalize_string(english_sentence)\n",
    "    tokens = encode_sentence(en_clean, en_stoi, block_size)\n",
    "    src = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        enc_output = model.encoder(src)\n",
    "    \n",
    "    # Decode step by step, collecting attention\n",
    "    tgt_tokens = torch.full((1, 1), SOS_IDX, dtype=torch.long, device=device)\n",
    "    fr_words = []\n",
    "    \n",
    "    # We'll hook into the last decoder block's cross-attention\n",
    "    attention_maps = []\n",
    "    \n",
    "    last_block = model.decoder.blocks[-1]\n",
    "    \n",
    "    # Register a hook on the cross-attention\n",
    "    hook_weights = {}\n",
    "    def hook_fn(module, input, output):\n",
    "        # input[0] is x, but we want the attention weights\n",
    "        # We'll compute them manually here\n",
    "        pass\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(block_size - 1):\n",
    "            dec_output = model.decoder(tgt_tokens, enc_output)\n",
    "            logits = model.lm_head(dec_output)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            \n",
    "            word = fr_itos.get(next_token.item(), \"<UNK>\")\n",
    "            if word == EOS_TOKEN:\n",
    "                break\n",
    "            if word != PAD_TOKEN and word != SOS_TOKEN:\n",
    "                fr_words.append(word)\n",
    "            \n",
    "            tgt_tokens = torch.cat([tgt_tokens, next_token], dim=1)\n",
    "    \n",
    "    # Now do one final pass to get cross-attention weights\n",
    "    # We manually compute attention in the last block's cross-attention heads\n",
    "    with torch.no_grad():\n",
    "        # Get decoder state just before cross-attention in last block\n",
    "        x = model.decoder.token_embedding(tgt_tokens)\n",
    "        x = model.decoder.pos_encoding(x)\n",
    "        \n",
    "        for block in model.decoder.blocks[:-1]:\n",
    "            x = block(x, enc_output)\n",
    "        \n",
    "        # Last block: self-attention first\n",
    "        x_after_sa = x + last_block.sa(last_block.ln1(x))\n",
    "        \n",
    "        # Cross-attention: compute weights manually\n",
    "        x_normed = last_block.ln2(x_after_sa)\n",
    "        \n",
    "        # Collect weights from each head\n",
    "        head_weights = []\n",
    "        for head in last_block.cross_attn.heads:\n",
    "            q = head.query(x_normed)\n",
    "            k = head.key(enc_output)\n",
    "            C = q.shape[-1]\n",
    "            wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "            wei = F.softmax(wei, dim=-1)\n",
    "            head_weights.append(wei[0].cpu().numpy())  # (T_dec, T_src)\n",
    "    \n",
    "    en_words = en_clean.split()\n",
    "    return head_weights, en_words, fr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ†• Plot cross-attention heatmap\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def plot_attention(english_sentence, head_idx=0):\n",
    "    \"\"\"Plot cross-attention weights for a translation.\"\"\"\n",
    "    head_weights, en_words, fr_words = get_cross_attention_weights(english_sentence)\n",
    "    \n",
    "    if not fr_words:\n",
    "        print(\"No output generated.\")\n",
    "        return\n",
    "    \n",
    "    # Average across heads\n",
    "    avg_weights = np.mean(head_weights, axis=0)\n",
    "    \n",
    "    # Trim to actual words (skip <SOS> in decoder, skip padding in encoder)\n",
    "    n_fr = len(fr_words) + 1  # +1 for <SOS> offset\n",
    "    n_en = len(en_words) + 2  # +2 for <SOS> and <EOS>\n",
    "    \n",
    "    # Plot average attention\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Average attention\n",
    "    attn_slice = avg_weights[1:len(fr_words)+1, 1:len(en_words)+1]  # skip special tokens\n",
    "    if attn_slice.shape[0] > 0 and attn_slice.shape[1] > 0:\n",
    "        im = axes[0].imshow(attn_slice, cmap='viridis', aspect='auto')\n",
    "        axes[0].set_xticks(range(len(en_words)))\n",
    "        axes[0].set_xticklabels(en_words, rotation=45, ha='right')\n",
    "        axes[0].set_yticks(range(len(fr_words)))\n",
    "        axes[0].set_yticklabels(fr_words)\n",
    "        axes[0].set_xlabel('English (Source)')\n",
    "        axes[0].set_ylabel('French (Target)')\n",
    "        axes[0].set_title(f'Average Cross-Attention')\n",
    "        plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # Plot 2: Specific head\n",
    "    head_attn = head_weights[head_idx][1:len(fr_words)+1, 1:len(en_words)+1]\n",
    "    if head_attn.shape[0] > 0 and head_attn.shape[1] > 0:\n",
    "        im2 = axes[1].imshow(head_attn, cmap='viridis', aspect='auto')\n",
    "        axes[1].set_xticks(range(len(en_words)))\n",
    "        axes[1].set_xticklabels(en_words, rotation=45, ha='right')\n",
    "        axes[1].set_yticks(range(len(fr_words)))\n",
    "        axes[1].set_yticklabels(fr_words)\n",
    "        axes[1].set_xlabel('English (Source)')\n",
    "        axes[1].set_ylabel('French (Target)')\n",
    "        axes[1].set_title(f'Head {head_idx} Cross-Attention')\n",
    "        plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.suptitle(f'\"{english_sentence}\" â†’ \"{\" \".join(fr_words)}\"', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention for a few sentences\n",
    "plot_attention(\"I love you.\")\n",
    "plot_attention(\"He is a student.\")\n",
    "plot_attention(\"It is cold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010725a",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Save / Load Model Weights\n",
    "\n",
    "Same pattern as Day 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a21641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Save model weights (same pattern as Day 8)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "torch.save(model.state_dict(), 'translator_weights.pth')\n",
    "print(\"âœ… Model weights saved to translator_weights.pth\")\n",
    "\n",
    "# To load later:\n",
    "# model.load_state_dict(torch.load('translator_weights.pth', map_location=device))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e23808",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Day 8 GPT vs Task 3 Encoder-Decoder\n",
    "\n",
    "| Aspect | Day 8 GPT (Decoder-Only) | Task 3 Translator (Encoder-Decoder) |\n",
    "|--------|--------------------------|--------------------------------------|\n",
    "| **Task** | Generate Shakespeare text | Translate English â†’ French |\n",
    "| **Architecture** | Decoder only | Encoder + Decoder with cross-attention |\n",
    "| **Tokenization** | Character-level (65 chars) | Word-level (~5K-10K words + special tokens) |\n",
    "| **Attention** | Causal self-attention only | Self-attention + **cross-attention** |\n",
    "| **Block sublayers** | 2 (self-attn + FFN) | Encoder: 2, Decoder: **3** (+ cross-attn) |\n",
    "| **Positional encoding** | Learned embeddings | Sinusoidal |\n",
    "| **Input** | Single text stream | Parallel (source, target) pairs |\n",
    "| **Generation** | Autoregressive sampling | Greedy decoding with `<SOS>`â†’`<EOS>` |\n",
    "| **Loss** | CrossEntropy on all tokens | CrossEntropy **ignoring** `<PAD>` tokens |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "The **only architectural addition** to go from a decoder-only GPT to a full encoder-decoder Transformer is:\n",
    "\n",
    "1. **An Encoder** (same blocks as decoder, minus the causal mask)\n",
    "2. **Cross-attention** in each decoder block (Q from decoder, K/V from encoder output)\n",
    "\n",
    "Everything else â€” the attention mechanism, feed-forward networks, layer norms, residual connections â€” is **exactly the same** as what you built in Day 8! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
