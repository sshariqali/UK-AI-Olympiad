{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1af7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e2406",
   "metadata": {},
   "source": [
    "## Day 9 ‚Äî Task 1: Bigram Name Generator üü¢\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Goal**\n",
    "\n",
    "Build a **character-level bigram language model** that generates novel human names ‚Äî following the same architecture from Day 7 (Parts 1 & 2) but applied to a **completely new domain**: baby names!\n",
    "\n",
    "Instead of generating Shakespeare-like text, our model will learn the statistical patterns of real names and invent plausible new ones.\n",
    "\n",
    "---\n",
    "\n",
    "### üìã **Agenda**\n",
    "\n",
    "| Section | Topic | Description |\n",
    "|:-------:|-------|-------------|\n",
    "| 1 | **Imports & Hyperparameters** | Load PyTorch and configure training settings |\n",
    "| 2 | **Loading the Names Dataset** | Download 32K baby names from Karpathy's repo |\n",
    "| 3 | **Exploring the Data** | Understand name structure, characters, and vocabulary |\n",
    "| 4 | **Building a Tokenizer** | Create `stoi` / `itos` mappings with a special start/end token |\n",
    "| 5 | **Data Preparation** | Build bigram (input, target) pairs and batch them |\n",
    "| 6 | **Building the Bigram Model** | `nn.Embedding`-based bigram language model |\n",
    "| 7 | **Training** | Train with `CrossEntropyLoss` and track train/val loss |\n",
    "| 8 | **Generating Names** | Sample plausible new names from the trained model |\n",
    "| 9 | **Analysis & Visualisation** | Visualise the learned bigram probabilities |\n",
    "\n",
    "---\n",
    "\n",
    "### üéì **Learning Objectives**\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Apply the bigram language model architecture to a new domain\n",
    "- ‚úÖ Build a character-level tokenizer with a special boundary token (`.`)\n",
    "- ‚úÖ Prepare name data as bigram input-target pairs\n",
    "- ‚úÖ Train the model and monitor train/validation loss\n",
    "- ‚úÖ Generate novel human names by sampling from the model\n",
    "- ‚úÖ Visualise the learned character transition probabilities\n",
    "\n",
    "Let's generate some names! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721a38d",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Imports & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2753ce9",
   "metadata": {},
   "source": [
    "We import PyTorch and **matplotlib** for visualisation. We also define all hyperparameters at the top ‚Äî a best practice we picked up in Day 7 Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Hyperparameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "batch_size = 64        # number of bigram pairs processed in parallel\n",
    "max_iters = 10000     # total training steps\n",
    "eval_interval = 1000     # how often to print train/val loss\n",
    "learning_rate = 1e-2     # step size for AdamW optimiser\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200       # batches to average when estimating loss\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e21b4a",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Loading the Names Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cc604",
   "metadata": {},
   "source": [
    "We'll use **Andrej Karpathy's `names.txt`** ‚Äî a list of 32,033 baby names sourced from the US Social Security Administration (2018). Each line contains one lowercase name.\n",
    "\n",
    "The file is hosted on GitHub, so we download it directly with `urllib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
    "filename = \"names.txt\"\n",
    "\n",
    "# Download the file only if it doesn't already exist\n",
    "if not os.path.exists(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "else:\n",
    "    print(f\"{filename} already exists ‚Äî skipping download\")\n",
    "\n",
    "# Read all names into a list\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "print(f\"Total names: {len(names)}\")\n",
    "print(f\"First 10 names: {names[:10]}\")\n",
    "print(f\"Last 10 names:  {names[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f3203",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f2e53",
   "metadata": {},
   "source": [
    "Before building a model, let's understand our data:\n",
    "- How long are the names?\n",
    "- What characters appear?\n",
    "- What does the distribution of name lengths look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "lengths = [len(name) for name in names]\n",
    "print(f\"Shortest name: {min(lengths)} characters  (e.g. {[n for n in names if len(n) == min(lengths)][:5]})\")\n",
    "print(f\"Longest name:  {max(lengths)} characters  (e.g. {[n for n in names if len(n) == max(lengths)][:3]})\")\n",
    "print(f\"Average length: {sum(lengths)/len(lengths):.1f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of name lengths\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(lengths, bins=range(1, max(lengths)+2), edgecolor='black', alpha=0.7, color='#1E88E5')\n",
    "plt.xlabel('Name Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Name Lengths')\n",
    "plt.xticks(range(1, max(lengths)+1))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca05c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What characters appear in the dataset?\n",
    "all_chars = sorted(list(set(''.join(names))))\n",
    "print(f\"Unique characters ({len(all_chars)}): {''.join(all_chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494ff69",
   "metadata": {},
   "source": [
    "**Observation:** The dataset contains only lowercase letters (a-z). No digits, spaces, or special characters ‚Äî perfect for a simple character-level model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adbf98d",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Building a Tokenizer (`stoi` / `itos`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454e99e",
   "metadata": {},
   "source": [
    "### üî§ Character-Level Tokenizer\n",
    "\n",
    "Just like in Day 7, we need to convert characters to integers and back.\n",
    "\n",
    "**But there's a twist!** In the Shakespeare dataset, we modelled a continuous stream of text. For names, each name is a **separate sequence** with a clear beginning and end.\n",
    "\n",
    "We introduce a **special boundary token `.`** (dot) to mark the start and end of each name:\n",
    "\n",
    "```\n",
    "\"emma\"  ‚Üí  .  e  m  m  a  .\n",
    "```\n",
    "\n",
    "This way:\n",
    "- The model learns which characters are likely to **start** a name (after `.`)\n",
    "- The model learns which characters are likely to **end** a name (before `.`)\n",
    "- When generating, we start with `.` and stop when the model produces another `.`\n",
    "\n",
    "The `.` token gets index **0** in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary: '.' (boundary) + all 26 lowercase letters\n",
    "chars = ['.'] + all_chars\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary ({vocab_size} tokens): {''.join(chars)}\")\n",
    "\n",
    "# Create the mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}   # string-to-index\n",
    "itos = {i: ch for i, ch in enumerate(chars)}   # index-to-string\n",
    "\n",
    "print(f\"\\nstoi: {stoi}\")\n",
    "print(f\"\\nitos: {itos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer\n",
    "test_name = \"emma\"\n",
    "encoded = [stoi[c] for c in test_name]\n",
    "decoded = ''.join([itos[i] for i in encoded])\n",
    "print(f\"'{test_name}' ‚Üí encoded: {encoded} ‚Üí decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3c700",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Data Preparation ‚Äî Building Bigram Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7ac6a",
   "metadata": {},
   "source": [
    "### What Are Bigrams?\n",
    "\n",
    "A **bigram** is a pair of consecutive characters. For the name `\"emma\"` (with boundary tokens), the bigrams are:\n",
    "\n",
    "| Input (current char) | Target (next char) |\n",
    "|:-------------------:|:-----------------:|\n",
    "| `.` | `e` |\n",
    "| `e` | `m` |\n",
    "| `m` | `m` |\n",
    "| `m` | `a` |\n",
    "| `a` | `.` |\n",
    "\n",
    "The model's job: **given the current character, predict the next character**.\n",
    "\n",
    "We build these pairs for ALL names in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6570e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all bigram (input, target) pairs from the dataset\n",
    "xs = []  # inputs\n",
    "ys = []  # targets\n",
    "\n",
    "for name in names:\n",
    "    # Add boundary tokens: .emma.\n",
    "    chars_in_name = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2 in zip(chars_in_name, chars_in_name[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) \n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(f\"Total bigram pairs: {len(xs)}\")\n",
    "print(f\"\\nFirst 10 pairs:\")\n",
    "for i in range(10):\n",
    "    print(f\"  {itos[xs[i].item()]} ‚Üí {itos[ys[i].item()]}  (indices: {xs[i].item()} ‚Üí {ys[i].item()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b3f09",
   "metadata": {},
   "source": [
    "### Train / Validation Split\n",
    "\n",
    "We shuffle and split the data 90/10, just like in Day 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "torch.manual_seed(42)\n",
    "perm = torch.randperm(len(xs))\n",
    "xs = xs[perm]\n",
    "ys = ys[perm]\n",
    "\n",
    "# 90% train, 10% validation\n",
    "n = int(0.9 * len(xs))\n",
    "x_train, y_train = xs[:n], ys[:n]\n",
    "x_val,   y_val   = xs[n:], ys[n:]\n",
    "\n",
    "print(f\"Training pairs:   {len(x_train)}\")\n",
    "print(f\"Validation pairs: {len(x_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879f66c",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "We sample random mini-batches from the training or validation set. Each sample is a single bigram pair (one input character, one target character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ec843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"Sample a random mini-batch of bigram pairs.\"\"\"\n",
    "    if split == 'train':\n",
    "        x_data, y_data = x_train, y_train\n",
    "    else:\n",
    "        x_data, y_data = x_val, y_val\n",
    "    ix = torch.randint(len(x_data), (batch_size,))\n",
    "    x = x_data[ix].to(device)\n",
    "    y = y_data[ix].to(device)\n",
    "    return x, y\n",
    "\n",
    "# Quick test\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"Batch input shape:  {xb.shape}\")\n",
    "print(f\"Batch target shape: {yb.shape}\")\n",
    "print(f\"\\nSample pairs from batch:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {itos[xb[i].item()]} ‚Üí {itos[yb[i].item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97092267",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Loss Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841040bf",
   "metadata": {},
   "source": [
    "As we learned in Day 7 Part 2, a single batch loss is noisy. We average over many batches for a stable estimate.\n",
    "\n",
    "`@torch.no_grad()` disables gradient computation ‚Äî we're only evaluating, not training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd60be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Average loss over several batches for both train and val splits.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a26bc",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Building the Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528befbf",
   "metadata": {},
   "source": [
    "### Architecture Recap\n",
    "\n",
    "This is the **same Bigram model** from Day 7, adapted for name generation:\n",
    "\n",
    "- A single `nn.Embedding(vocab_size, vocab_size)` layer\n",
    "- Each row `i` of the embedding table stores the **logits** (prediction scores) for \"what character comes after character `i`?\"\n",
    "- No hidden layers, no attention ‚Äî just a direct lookup table\n",
    "\n",
    "**Key difference from Day 7:**  \n",
    "- Input is a **single character** (not a sequence), so the forward pass is simpler\n",
    "- We add a `generate_name()` method that starts with `.` and stops when it produces another `.`\n",
    "\n",
    "```\n",
    "Embedding Table (27 √ó 27):\n",
    "\n",
    "         a    b    c  ...  z    .\n",
    "  .   [ 0.1  0.3  0.2 ... 0.0  0.0 ]   ‚Üê \"After '.', what comes next?\"\n",
    "  a   [ 0.0  0.2  0.1 ... 0.1  0.3 ]   ‚Üê \"After 'a', what comes next?\"\n",
    "  b   [ 0.2  0.0  0.1 ... 0.0  0.1 ]\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramNameModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        \"\"\"\n",
    "        x: (B,) tensor of character indices\n",
    "        y: (B,) tensor of target character indices (optional)\n",
    "        Returns: logits (B, vocab_size), loss (scalar or None)\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(x)   # (B, C) where C = vocab_size\n",
    "\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate_name(self, max_len=20):\n",
    "        \"\"\"\n",
    "        Generate a single name by sampling character by character.\n",
    "        Starts with '.' (index 0), stops when '.' is produced again.\n",
    "        \"\"\"\n",
    "        idx = torch.tensor([stoi['.']], device=device)  # start with '.'\n",
    "        name_chars = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits, _ = self(idx)\n",
    "            probs = F.softmax(logits, dim=-1)           # (1, C)\n",
    "            idx = torch.multinomial(probs, num_samples=1).squeeze()  # scalar\n",
    "            ch = itos[idx.item()]\n",
    "            if ch == '.':\n",
    "                break  # end of name\n",
    "            name_chars.append(ch)\n",
    "            idx = idx.unsqueeze(0)  # reshape for next iteration\n",
    "\n",
    "        return ''.join(name_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move to device\n",
    "model = BigramNameModel(vocab_size).to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {num_params} parameters\")\n",
    "print(f\"(That's a {vocab_size}√ó{vocab_size} = {vocab_size*vocab_size} embedding table)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c13df",
   "metadata": {},
   "source": [
    "### Test Before Training\n",
    "\n",
    "Let's see what the untrained model generates ‚Äî it should be complete gibberish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657e9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Names from UNTRAINED model:\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(10):\n",
    "    print(f\"  {model.generate_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5388377",
   "metadata": {},
   "source": [
    "As expected ‚Äî random sequences of characters! Let's fix that by training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993b7ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb3321",
   "metadata": {},
   "source": [
    "Our training loop follows the same structure from Day 7 Part 2:\n",
    "\n",
    "1. **Sample** a mini-batch of bigram pairs\n",
    "2. **Forward pass**: compute logits and cross-entropy loss\n",
    "3. **Backward pass**: compute gradients\n",
    "4. **Update**: adjust the embedding weights using AdamW\n",
    "5. **Evaluate**: every `eval_interval` steps, print stable train/val loss\n",
    "\n",
    "We also record the loss history so we can plot training curves afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0800777",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Track loss history for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "steps_recorded = []\n",
    "\n",
    "for step in range(max_iters):\n",
    "    \n",
    "    # Periodic evaluation\n",
    "    if step % eval_interval == 0 or step == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_losses.append(losses['train'].item())\n",
    "        val_losses.append(losses['val'].item())\n",
    "        steps_recorded.append(step)\n",
    "        print(f\"Step {step:5d}: train loss = {losses['train']:.4f}, val loss = {losses['val']:.4f}\")\n",
    "    \n",
    "    # Sample a batch\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "print(\"\\nTraining complete! ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd28e37",
   "metadata": {},
   "source": [
    "### Training Curves\n",
    "\n",
    "Let's visualise how the loss decreased during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f07f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps_recorded, train_losses, label='Train Loss', marker='o', color='#1E88E5')\n",
    "plt.plot(steps_recorded, val_losses, label='Val Loss', marker='s', color='#E53935')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Training & Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final val loss:   {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42213a6a",
   "metadata": {},
   "source": [
    "**How good is this loss?**\n",
    "\n",
    "The theoretical **minimum loss** for a perfect bigram model depends on the true bigram statistics in the data. With 27 tokens, a uniform random model would have loss $= -\\ln(1/27) \\approx 3.30$. Any loss well below that means the model has learned something meaningful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd0420",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Generating Names üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa6299",
   "metadata": {},
   "source": [
    "The moment of truth! Let's generate some names from our trained model.\n",
    "\n",
    "**How Generation Works:**\n",
    "1. Start with the boundary token `.` (index 0)\n",
    "2. The model predicts probabilities for the next character\n",
    "3. We **sample** from this distribution (not just pick the highest ‚Äî this adds variety!)\n",
    "4. Repeat until the model produces another `.` (end of name) or we hit `max_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"  Generated Names (Trained Model)\")\n",
    "print(\"=\" * 40)\n",
    "for i in range(20):\n",
    "    name = model.generate_name(max_len=20)\n",
    "    print(f\"  {i+1:2d}. {name}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd385a",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- The names look somewhat plausible ‚Äî they follow common character patterns in English names\n",
    "- Some might even sound like real names!\n",
    "- However, the model only looks at the **previous character** ‚Äî so it can't capture longer patterns like \"the\" or \"tion\"\n",
    "- A more powerful model (e.g., an MLP or Transformer) would produce better names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e4a1e",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Visualising the Learned Bigram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127e9bb",
   "metadata": {},
   "source": [
    "Let's peek inside the model and see what it learned! The embedding table is essentially a **27 √ó 27 matrix** where entry `(i, j)` tells us:\n",
    "\n",
    "> \"After character `i`, how likely is character `j` to come next?\"\n",
    "\n",
    "We convert the raw logits to probabilities using softmax and display them as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea82366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the learned embedding table and convert to probabilities\n",
    "with torch.no_grad():\n",
    "    W = model.token_embedding_table.weight.cpu()  # (27, 27) raw logits\n",
    "    P = torch.softmax(W, dim=1)                   # (27, 27) probabilities\n",
    "\n",
    "# Create the heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "im = ax.imshow(P.numpy(), cmap='Blues')\n",
    "\n",
    "# Label axes with characters\n",
    "labels = [itos[i] for i in range(vocab_size)]\n",
    "ax.set_xticks(range(vocab_size))\n",
    "ax.set_xticklabels(labels, fontsize=10)\n",
    "ax.set_yticks(range(vocab_size))\n",
    "ax.set_yticklabels(labels, fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Next Character (predicted)', fontsize=12)\n",
    "ax.set_ylabel('Current Character (input)', fontsize=12)\n",
    "ax.set_title('Learned Bigram Probabilities', fontsize=14)\n",
    "\n",
    "# Annotate each cell with the probability value\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        val = P[i, j].item()\n",
    "        if val > 0.02:  # only annotate non-trivial probabilities\n",
    "            color = 'white' if val > 0.15 else 'black'\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=6, color=color)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Probability', shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54764a8",
   "metadata": {},
   "source": [
    "### Reading the Heatmap\n",
    "\n",
    "- **Row `.`** (the boundary token): shows which characters most commonly **start** a name. You should see peaks at common starting letters like `a`, `j`, `m`, `s`.\n",
    "- **Column `.`**: shows which characters most commonly **end** a name. You should see peaks at common ending letters like `a`, `n`, `e`, `y`.\n",
    "- **Diagonal patterns**: letters that commonly double up (e.g., `l` ‚Üí `l`, `n` ‚Üí `n`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 most likely starting characters\n",
    "start_probs = P[stoi['.'], :]\n",
    "top_starts = torch.topk(start_probs, 5)\n",
    "print(\"Top 5 starting characters:\")\n",
    "for prob, idx in zip(top_starts.values, top_starts.indices):\n",
    "    print(f\"  '{itos[idx.item()]}' ‚Äî {prob.item():.1%}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Top 5 most likely ending characters (characters most likely followed by '.')\n",
    "end_probs = P[:, stoi['.']]\n",
    "top_ends = torch.topk(end_probs, 5)\n",
    "print(\"Top 5 ending characters:\")\n",
    "for prob, idx in zip(top_ends.values, top_ends.indices):\n",
    "    if itos[idx.item()] != '.':\n",
    "        print(f\"  '{itos[idx.item()]}' ‚Äî {prob.item():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdb9c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Comparing with Counting-Based Bigram\n",
    "\n",
    "As a sanity check, let's build a bigram model the \"classical\" way ‚Äî just counting character pairs ‚Äî and compare it to our neural network approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a574f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count bigrams from the dataset\n",
    "N = torch.zeros((vocab_size, vocab_size), dtype=torch.int32)\n",
    "\n",
    "for name in names:\n",
    "    chars_in_name = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2 in zip(chars_in_name, chars_in_name[1:]):\n",
    "        N[stoi[ch1], stoi[ch2]] += 1\n",
    "\n",
    "# Convert counts to probabilities (add smoothing to avoid log(0))\n",
    "P_count = (N + 1).float()\n",
    "P_count = P_count / P_count.sum(dim=1, keepdim=True)\n",
    "\n",
    "# Compute negative log-likelihood on the same data\n",
    "nll = 0.0\n",
    "count = 0\n",
    "for name in names:\n",
    "    chars_in_name = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2 in zip(chars_in_name, chars_in_name[1:]):\n",
    "        prob = P_count[stoi[ch1], stoi[ch2]]\n",
    "        nll += -torch.log(prob).item()\n",
    "        count += 1\n",
    "\n",
    "print(f\"Counting-based bigram average NLL: {nll / count:.4f}\")\n",
    "print(f\"Neural bigram final val loss:      {val_losses[-1]:.4f}\")\n",
    "print(f\"\\nBoth should be close ‚Äî they're modelling the same statistics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79843b1a",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Reflection üìù\n",
    "\n",
    "### What We Built\n",
    "- ‚úÖ Downloaded and explored the `names.txt` dataset (32K baby names)\n",
    "- ‚úÖ Built a character-level tokenizer with `stoi` / `itos` and a boundary token `.`\n",
    "- ‚úÖ Constructed bigram (input ‚Üí target) pairs from all names\n",
    "- ‚úÖ Implemented a `BigramNameModel` using `nn.Embedding`\n",
    "- ‚úÖ Trained with `CrossEntropyLoss` + `AdamW` optimiser\n",
    "- ‚úÖ Generated plausible new names by sampling from the trained model\n",
    "- ‚úÖ Visualised the learned bigram probabilities as a heatmap\n",
    "- ‚úÖ Verified that our neural approach matches classical counting\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | What We Learned |\n",
    "|---------|----------------|\n",
    "| **Boundary token** | Using `.` to mark name start/end lets the model learn which characters begin and end names |\n",
    "| **Embedding as lookup** | In a bigram model, the embedding table IS the entire model ‚Äî each row stores next-char logits |\n",
    "| **Cross-entropy loss** | Measures how well predicted probabilities match the true next character |\n",
    "| **Neural ‚âà Counting** | For bigrams, the neural network converges to the same solution as simple counting |\n",
    "\n",
    "### Limitations\n",
    "The bigram model only looks at **one character** at a time. It can't capture:\n",
    "- Common letter combinations like \"th\", \"ch\", \"tion\"\n",
    "- Name-level patterns like \"names starting with 'Chr' often end with 'is' or 'ina'\"\n",
    "\n",
    "**To go further:** an MLP or Transformer-based model could use longer context and generate much more realistic names!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
