{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceac56d",
   "metadata": {},
   "source": [
    "## Day 9 ‚Äî Task 2: Hyperparameter Exploration & Analysis of the GPT Model üü°\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Goal**\n",
    "\n",
    "Starting from the pre-trained GPT model built in Day 8 (`hands_on_part_7_scaling.ipynb` + `model_weights.pth`), you will:\n",
    "1. **Load and evaluate** the pre-trained baseline model\n",
    "2. **Experiment** with architectural changes ‚Äî vary `n_head`, `n_layer`, `n_embed`, `dropout`, and `block_size`\n",
    "3. **Retrain** smaller variants from scratch\n",
    "4. **Compare** validation losses across configurations\n",
    "5. **Visualise** attention weight heatmaps to understand what the model focuses on\n",
    "6. **Analyse** how each hyperparameter affects generation quality\n",
    "\n",
    "---\n",
    "\n",
    "### üìã **Agenda**\n",
    "\n",
    "| Section | Topic | Description |\n",
    "|:-------:|-------|-------------|\n",
    "| 1 | **Setup & Data Pipeline** | Load PyTorch, dataset, tokenizer, batching |\n",
    "| 2 | **Model Architecture** | Full Transformer definition (from Day 8) |\n",
    "| 3 | **Load Pre-trained Baseline** | Load `model_weights.pth` and evaluate |\n",
    "| 4 | **Attention Weight Visualisation** | Extract and plot attention heatmaps from the baseline |\n",
    "| 5 | **Hyperparameter Experiments** | Train smaller variants with different configs |\n",
    "| 6 | **Results Comparison** | Bar charts and tables comparing all configurations |\n",
    "| 7 | **Generation Quality Comparison** | Side-by-side text samples from each model |\n",
    "| 8 | **Written Analysis** | Discuss findings and draw conclusions |\n",
    "\n",
    "---\n",
    "\n",
    "### üéì **Skills Tested**\n",
    "\n",
    "- ‚úÖ Transformer architecture understanding (Days 7‚Äì8)\n",
    "- ‚úÖ Attention mechanism visualisation\n",
    "- ‚úÖ Matplotlib / plotting skills\n",
    "- ‚úÖ Critical analysis of model behaviour\n",
    "- ‚úÖ Experimental methodology ‚Äî changing one variable at a time\n",
    "\n",
    "Let's explore what makes a Transformer tick! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e89f84",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeaf9aa",
   "metadata": {},
   "source": [
    "We reuse the exact same data pipeline from `hands_on_part_7_scaling.ipynb`:\n",
    "- Load Tiny Shakespeare\n",
    "- Character-level tokenizer (`stoi` / `itos`)\n",
    "- 90/10 train/val split\n",
    "- `get_batch()` and `estimate_loss()` functions\n",
    "\n",
    "These stay the same across ALL experiments ‚Äî only the **model architecture** changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf9bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4a1e4",
   "metadata": {},
   "source": [
    "### Load and Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tiny shakespeare dataset\n",
    "with open(\"tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Tokenizer\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Encode and split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Training tokens: {len(train_data):,}\")\n",
    "print(f\"Validation tokens: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56831c1e",
   "metadata": {},
   "source": [
    "### Batch Loader & Loss Estimation\n",
    "\n",
    "These functions adapt to whatever `batch_size` and `block_size` are currently set. We define them to accept these as parameters so they work across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, block_size, batch_size=32):\n",
    "    \"\"\"Sample a random mini-batch.\"\"\"\n",
    "    d = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
    "    x = torch.stack([d[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([d[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, block_size, batch_size=32, eval_iters=200):\n",
    "    \"\"\"Average loss over many batches for stable measurement.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, block_size, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf1bcd",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Model Architecture (from Day 8)\n",
    "\n",
    "We define the full Transformer model ‚Äî `Head`, `MultiHeadAttention`, `FeedForward`, `Block`, and `Transformer` ‚Äî exactly as built in Day 8 Part 7.\n",
    "\n",
    "The key difference: we **parameterise everything** via a config dictionary so we can easily swap hyperparameters for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, return_weights=False):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        if return_weights:\n",
    "            raw_weights = wei.clone()  # save before dropout\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v\n",
    "        if return_weights:\n",
    "            return out, raw_weights\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(n_embed, head_size, block_size, dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, return_weights=False):\n",
    "        if return_weights:\n",
    "            results = [h(x, return_weights=True) for h in self.heads]\n",
    "            outs = [r[0] for r in results]\n",
    "            weights = [r[1] for r in results]\n",
    "            out = torch.cat(outs, dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out, weights\n",
    "        else:\n",
    "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Simple feed-forward network with ReLU.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * 4, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: LayerNorm ‚Üí MultiHead Attention ‚Üí LayerNorm ‚Üí FFN, with residual connections.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa   = MultiHeadAttention(n_embed, n_head, head_size, block_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embed, dropout)\n",
    "        self.ln1  = nn.LayerNorm(n_embed)\n",
    "        self.ln2  = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x, return_weights=False):\n",
    "        if return_weights:\n",
    "            sa_out, weights = self.sa(self.ln1(x), return_weights=True)\n",
    "            x = x + sa_out\n",
    "            x = x + self.ffwd(self.ln2(x))\n",
    "            return x, weights\n",
    "        else:\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "            x = x + self.ffwd(self.ln2(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"GPT-style decoder-only Transformer.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table    = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embed, n_head, block_size, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f    = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, y=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            y = y.view(B * T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    def get_attention_weights(self, idx, layer_idx=0):\n",
    "        \"\"\"\n",
    "        Forward pass that also returns the attention weights\n",
    "        from a specific transformer block (layer).\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        weights = None\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if i == layer_idx:\n",
    "                x, weights = block(x, return_weights=True)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        return weights  # list of (B, T, T) tensors, one per head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06015b28",
   "metadata": {},
   "source": [
    "### Helper: Count Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count total trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9567e38",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Load the Pre-trained Baseline Model\n",
    "\n",
    "We load the model from `model_weights.pth` using the **original Day 8 hyperparameters**:\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| `n_embed` | 384 |\n",
    "| `n_head` | 6 |\n",
    "| `n_layer` | 6 |\n",
    "| `block_size` | 256 |\n",
    "| `dropout` | 0.2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Baseline configuration (from Day 8 Part 7) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "baseline_config = {\n",
    "    'n_embed':    384,\n",
    "    'n_head':     6,\n",
    "    'n_layer':    6,\n",
    "    'block_size': 256,\n",
    "    'dropout':    0.2,\n",
    "}\n",
    "\n",
    "# Create model with baseline config\n",
    "baseline_model = Transformer(\n",
    "    vocab_size = vocab_size,\n",
    "    **baseline_config\n",
    ").to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "baseline_model.load_state_dict(\n",
    "    torch.load('model_weights.pth', map_location=device)\n",
    ")\n",
    "baseline_model.eval()\n",
    "\n",
    "print(f\"‚úÖ Baseline model loaded successfully!\")\n",
    "print(f\"   Parameters: {count_parameters(baseline_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2dbff",
   "metadata": {},
   "source": [
    "### Evaluate the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d358cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_losses = estimate_loss(baseline_model, block_size=256, batch_size=32)\n",
    "print(f\"Baseline ‚Äî Train loss: {baseline_losses['train']:.4f}, Val loss: {baseline_losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbfee51",
   "metadata": {},
   "source": [
    "### Generate Sample Text from Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated = baseline_model.generate(context, max_new_tokens=500)\n",
    "print(\"=\" * 60)\n",
    "print(\"  BASELINE MODEL ‚Äî Generated Text\")\n",
    "print(\"=\" * 60)\n",
    "print(decode(generated[0].tolist()))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aae057",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Attention Weight Visualisation üîç\n",
    "\n",
    "One of the most powerful ways to understand a Transformer is to **look at what it's paying attention to**.\n",
    "\n",
    "Each attention head in each layer produces a $(T \\times T)$ weight matrix, where entry $(i, j)$ tells us:\n",
    "\n",
    "> \"When producing the output for position $i$, how much does the model attend to position $j$?\"\n",
    "\n",
    "We'll extract these weights from the **pre-trained baseline** and plot them as heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f2d7b",
   "metadata": {},
   "source": [
    "### Prepare a Short Input Sequence\n",
    "\n",
    "We pick a short passage from Shakespeare for clear visualisation. Attention maps on long sequences are hard to read!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a short, recognisable passage\n",
    "sample_text = \"First Citizen:\\nBefore we proceed\"\n",
    "sample_tokens = encode(sample_text)\n",
    "sample_tensor = torch.tensor([sample_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "T = len(sample_tokens)\n",
    "print(f\"Input text: '{sample_text}'\")\n",
    "print(f\"Tokens ({T}): {sample_tokens}\")\n",
    "print(f\"Characters: {[itos[t] for t in sample_tokens]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5046b7e",
   "metadata": {},
   "source": [
    "### Extract and Plot Attention Weights\n",
    "\n",
    "We'll visualise attention heads from **Layer 0** (first layer) and **Layer 5** (last layer) of the baseline model to see how attention patterns differ between early and late layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heads(model, input_tensor, layer_idx, n_heads_to_show=6):\n",
    "    \"\"\"\n",
    "    Extract attention weights from a specific layer and plot each head as a heatmap.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        weights = model.get_attention_weights(input_tensor, layer_idx=layer_idx)\n",
    "\n",
    "    T = input_tensor.shape[1]\n",
    "    token_labels = [itos[input_tensor[0, i].item()] for i in range(T)]\n",
    "    # Make whitespace characters visible\n",
    "    display_labels = []\n",
    "    for ch in token_labels:\n",
    "        if ch == '\\n':\n",
    "            display_labels.append('\\\\n')\n",
    "        elif ch == ' ':\n",
    "            display_labels.append('‚ê£')\n",
    "        else:\n",
    "            display_labels.append(ch)\n",
    "\n",
    "    n_heads = min(len(weights), n_heads_to_show)\n",
    "    fig, axes = plt.subplots(1, n_heads, figsize=(4 * n_heads, 4))\n",
    "    if n_heads == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for h in range(n_heads):\n",
    "        ax = axes[h]\n",
    "        w = weights[h][0].cpu().numpy()  # (T, T), first batch element\n",
    "        im = ax.imshow(w, cmap='viridis', vmin=0, vmax=1)\n",
    "        ax.set_title(f'Head {h}', fontsize=11)\n",
    "        ax.set_xticks(range(T))\n",
    "        ax.set_xticklabels(display_labels, fontsize=7, rotation=90)\n",
    "        ax.set_yticks(range(T))\n",
    "        ax.set_yticklabels(display_labels, fontsize=7)\n",
    "        if h == 0:\n",
    "            ax.set_ylabel('Query position (output)', fontsize=9)\n",
    "        ax.set_xlabel('Key position (input)', fontsize=9)\n",
    "\n",
    "    fig.suptitle(f'Attention Weights ‚Äî Layer {layer_idx}', fontsize=14, y=1.02)\n",
    "    fig.colorbar(im, ax=axes, shrink=0.6, label='Attention Weight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf39f3",
   "metadata": {},
   "source": [
    "### Layer 0 ‚Äî First Transformer Block\n",
    "\n",
    "Early layers tend to learn **local patterns** ‚Äî attending to nearby characters, the previous character, or specific character types (vowels, consonants, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e82949",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heads(baseline_model, sample_tensor, layer_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b8093",
   "metadata": {},
   "source": [
    "### Layer 5 ‚Äî Last Transformer Block\n",
    "\n",
    "Later layers tend to learn more **abstract, long-range patterns** ‚Äî attending to semantically related positions or structural patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heads(baseline_model, sample_tensor, layer_idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86f5ae",
   "metadata": {},
   "source": [
    "### üîç What to Look For in the Heatmaps\n",
    "\n",
    "| Pattern | What It Means |\n",
    "|---------|---------------|\n",
    "| **Strong diagonal** | Head attends to the immediately previous token (like a bigram) |\n",
    "| **Vertical stripes** | Head attends to a specific position regardless of query position |\n",
    "| **Uniform rows** | Head distributes attention evenly (less specialised) |\n",
    "| **Block patterns** | Head groups tokens (e.g., within a word, within a line) |\n",
    "| **Triangular bottom-left** | Natural causal pattern ‚Äî later tokens attend to more context |\n",
    "\n",
    "Different heads learn **different roles** ‚Äî this is why multi-head attention is powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088931a5",
   "metadata": {},
   "source": [
    "### Average Attention Across All Heads in Each Layer\n",
    "\n",
    "Let's also see the **average** attention pattern per layer ‚Äî this smooths out individual head quirks and shows the layer's overall behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = baseline_config['n_layer']\n",
    "fig, axes = plt.subplots(1, n_layers, figsize=(4 * n_layers, 4))\n",
    "\n",
    "T = sample_tensor.shape[1]\n",
    "display_labels = []\n",
    "for i in range(T):\n",
    "    ch = itos[sample_tensor[0, i].item()]\n",
    "    if ch == '\\n':\n",
    "        display_labels.append('\\\\n')\n",
    "    elif ch == ' ':\n",
    "        display_labels.append('‚ê£')\n",
    "    else:\n",
    "        display_labels.append(ch)\n",
    "\n",
    "for layer_idx in range(n_layers):\n",
    "    with torch.no_grad():\n",
    "        weights = baseline_model.get_attention_weights(sample_tensor, layer_idx=layer_idx)\n",
    "    \n",
    "    # Average across all heads\n",
    "    avg_w = torch.stack([w[0] for w in weights]).mean(dim=0).cpu().numpy()\n",
    "\n",
    "    ax = axes[layer_idx]\n",
    "    im = ax.imshow(avg_w, cmap='viridis', vmin=0, vmax=0.5)\n",
    "    ax.set_title(f'Layer {layer_idx}', fontsize=11)\n",
    "    ax.set_xticks(range(T))\n",
    "    ax.set_xticklabels(display_labels, fontsize=6, rotation=90)\n",
    "    ax.set_yticks(range(T))\n",
    "    ax.set_yticklabels(display_labels, fontsize=6)\n",
    "\n",
    "fig.suptitle('Average Attention Weights per Layer (Baseline Model)', fontsize=14, y=1.02)\n",
    "fig.colorbar(im, ax=axes, shrink=0.6, label='Avg Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbc54b",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Hyperparameter Experiments üß™\n",
    "\n",
    "Now for the core experiment! We'll train **smaller variants** of the Transformer, changing **one hyperparameter at a time** compared to a reduced baseline.\n",
    "\n",
    "### Experimental Design\n",
    "\n",
    "**Why not use the full baseline config?**  \n",
    "The baseline (384 embed, 6 heads, 6 layers) takes 15-30 min to train on GPU. Instead, we use a **reduced baseline** that trains in 2-5 minutes, then vary one parameter at a time.\n",
    "\n",
    "| Experiment | What Changes | Reduced Baseline |\n",
    "|:----------:|:------------|:----------------|\n",
    "| **Baseline (small)** | nothing | `n_embed=64, n_head=4, n_layer=4, block_size=128, dropout=0.1` |\n",
    "| **Exp A** | `n_head` | 1 head vs 4 heads vs 8 heads |\n",
    "| **Exp B** | `n_layer` | 1 layer vs 4 layers vs 8 layers |\n",
    "| **Exp C** | `n_embed` | 32 vs 64 vs 128 |\n",
    "| **Exp D** | `dropout` | 0.0 vs 0.1 vs 0.3 |\n",
    "| **Exp E** | `block_size` | 32 vs 128 vs 256 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0245f",
   "metadata": {},
   "source": [
    "### Training Helper Function\n",
    "\n",
    "We define a reusable function that creates a model, trains it, records the loss curve, and returns everything we need for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_variant(config, label, max_iters=3000, eval_interval=500, learning_rate=3e-4, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train a Transformer variant from scratch and return results.\n",
    "    \n",
    "    Args:\n",
    "        config: dict with n_embed, n_head, n_layer, block_size, dropout\n",
    "        label: string name for this experiment\n",
    "        max_iters: training steps\n",
    "        eval_interval: how often to evaluate\n",
    "        learning_rate: optimiser LR\n",
    "        batch_size: mini-batch size\n",
    "    \n",
    "    Returns:\n",
    "        dict with model, losses, generated text, timing info\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Training: {label}\")\n",
    "    print(f\"  Config: {config}\")\n",
    "    \n",
    "    torch.manual_seed(1337)\n",
    "    \n",
    "    model = Transformer(vocab_size=vocab_size, **config).to(device)\n",
    "    n_params = count_parameters(model)\n",
    "    print(f\"  Parameters: {n_params:,}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    steps_list = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(max_iters):\n",
    "        if step % eval_interval == 0 or step == max_iters - 1:\n",
    "            losses = estimate_loss(model, config['block_size'], batch_size)\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            steps_list.append(step)\n",
    "            print(f\"  Step {step:5d}: train={losses['train']:.4f}, val={losses['val']:.4f}\")\n",
    "        \n",
    "        xb, yb = get_batch('train', config['block_size'], batch_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  Training time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Generate sample text\n",
    "    model.eval()\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    generated = model.generate(context, max_new_tokens=300)\n",
    "    sample_text = decode(generated[0].tolist())\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'label': label,\n",
    "        'config': config,\n",
    "        'n_params': n_params,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'steps': steps_list,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'elapsed': elapsed,\n",
    "        'sample_text': sample_text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff6a9b",
   "metadata": {},
   "source": [
    "### Define All Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ec409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Reduced baseline config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "small_baseline = {\n",
    "    'n_embed':    64,\n",
    "    'n_head':     4,\n",
    "    'n_layer':    4,\n",
    "    'block_size': 128,\n",
    "    'dropout':    0.1,\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ Experiment configs (change ONE parameter at a time) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "experiments = {\n",
    "    # Baseline\n",
    "    'Baseline (small)': small_baseline,\n",
    "\n",
    "    # Experiment A: vary n_head\n",
    "    'n_head=1':  {**small_baseline, 'n_head': 1},\n",
    "    'n_head=8':  {**small_baseline, 'n_head': 8},\n",
    "\n",
    "    # Experiment B: vary n_layer\n",
    "    'n_layer=1': {**small_baseline, 'n_layer': 1},\n",
    "    'n_layer=8': {**small_baseline, 'n_layer': 8},\n",
    "\n",
    "    # Experiment C: vary n_embed\n",
    "    'n_embed=32':  {**small_baseline, 'n_embed': 32, 'n_head': 4},  # 32/4=8 head_size\n",
    "    'n_embed=128': {**small_baseline, 'n_embed': 128, 'n_head': 4}, # 128/4=32 head_size\n",
    "\n",
    "    # Experiment D: vary dropout\n",
    "    'dropout=0.0': {**small_baseline, 'dropout': 0.0},\n",
    "    'dropout=0.3': {**small_baseline, 'dropout': 0.3},\n",
    "\n",
    "    # Experiment E: vary block_size\n",
    "    'block_size=32':  {**small_baseline, 'block_size': 32},\n",
    "    'block_size=256': {**small_baseline, 'block_size': 256},\n",
    "}\n",
    "\n",
    "print(f\"Total experiments to run: {len(experiments)}\")\n",
    "for name, cfg in experiments.items():\n",
    "    print(f\"  ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a9cf5",
   "metadata": {},
   "source": [
    "### Run All Experiments\n",
    "\n",
    "‚ö†Ô∏è **This cell may take a while** depending on your hardware. Each experiment trains for 3,000 steps, which takes ~1-3 minutes on GPU or ~3-8 minutes on CPU per experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, config in experiments.items():\n",
    "    results[name] = train_variant(config, label=name, max_iters=3000, eval_interval=500)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  ALL EXPERIMENTS COMPLETE! ‚úÖ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee286f",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Results Comparison üìä\n",
    "\n",
    "Now let's visualise and compare the results across all experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d447d32",
   "metadata": {},
   "source": [
    "### 6.1 ‚Äî Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary table\n",
    "print(f\"{'Experiment':<20} {'Params':>10} {'Train Loss':>12} {'Val Loss':>10} {'Time (s)':>10}\")\n",
    "print(\"-\" * 65)\n",
    "for name, r in results.items():\n",
    "    print(f\"{name:<20} {r['n_params']:>10,} {r['final_train_loss']:>12.4f} {r['final_val_loss']:>10.4f} {r['elapsed']:>10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64005e2",
   "metadata": {},
   "source": [
    "### 6.2 ‚Äî Validation Loss Bar Chart (All Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(results.keys())\n",
    "val_losses_all = [results[n]['final_val_loss'] for n in names]\n",
    "train_losses_all = [results[n]['final_train_loss'] for n in names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x_pos = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, train_losses_all, width, label='Train Loss', color='#1E88E5', alpha=0.8)\n",
    "bars2 = ax.bar(x_pos + width/2, val_losses_all, width, label='Val Loss', color='#E53935', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Experiment', fontsize=12)\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Train & Validation Loss Across All Experiments', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbf8eb",
   "metadata": {},
   "source": [
    "### 6.3 ‚Äî Training Curves: Effect of Number of Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99954f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "head_exps = ['n_head=1', 'Baseline (small)', 'n_head=8']\n",
    "colors = ['#FF7043', '#1E88E5', '#66BB6A']\n",
    "\n",
    "for name, color in zip(head_exps, colors):\n",
    "    r = results[name]\n",
    "    ax1.plot(r['steps'], r['train_losses'], marker='o', label=f\"{name} (train)\", color=color, linestyle='-')\n",
    "    ax2.plot(r['steps'], r['val_losses'], marker='s', label=f\"{name} (val)\", color=color, linestyle='-')\n",
    "\n",
    "ax1.set_title('Training Loss ‚Äî Varying n_head', fontsize=12)\n",
    "ax1.set_xlabel('Step'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.grid(alpha=0.3)\n",
    "ax2.set_title('Validation Loss ‚Äî Varying n_head', fontsize=12)\n",
    "ax2.set_xlabel('Step'); ax2.set_ylabel('Loss'); ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3071cc4",
   "metadata": {},
   "source": [
    "### 6.4 ‚Äî Training Curves: Effect of Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b21e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "layer_exps = ['n_layer=1', 'Baseline (small)', 'n_layer=8']\n",
    "\n",
    "for name, color in zip(layer_exps, colors):\n",
    "    r = results[name]\n",
    "    ax1.plot(r['steps'], r['train_losses'], marker='o', label=f\"{name} (train)\", color=color, linestyle='-')\n",
    "    ax2.plot(r['steps'], r['val_losses'], marker='s', label=f\"{name} (val)\", color=color, linestyle='-')\n",
    "\n",
    "ax1.set_title('Training Loss ‚Äî Varying n_layer', fontsize=12)\n",
    "ax1.set_xlabel('Step'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.grid(alpha=0.3)\n",
    "ax2.set_title('Validation Loss ‚Äî Varying n_layer', fontsize=12)\n",
    "ax2.set_xlabel('Step'); ax2.set_ylabel('Loss'); ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72a4f2",
   "metadata": {},
   "source": [
    "### 6.5 ‚Äî Training Curves: Effect of Embedding Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256bd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "embed_exps = ['n_embed=32', 'Baseline (small)', 'n_embed=128']\n",
    "\n",
    "for name, color in zip(embed_exps, colors):\n",
    "    r = results[name]\n",
    "    ax1.plot(r['steps'], r['train_losses'], marker='o', label=f\"{name} (train)\", color=color, linestyle='-')\n",
    "    ax2.plot(r['steps'], r['val_losses'], marker='s', label=f\"{name} (val)\", color=color, linestyle='-')\n",
    "\n",
    "ax1.set_title('Training Loss ‚Äî Varying n_embed', fontsize=12)\n",
    "ax1.set_xlabel('Step'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.grid(alpha=0.3)\n",
    "ax2.set_title('Validation Loss ‚Äî Varying n_embed', fontsize=12)\n",
    "ax2.set_xlabel('Step'); ax2.set_ylabel('Loss'); ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075e3b8",
   "metadata": {},
   "source": [
    "### 6.6 ‚Äî Training Curves: Effect of Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "dropout_exps = ['dropout=0.0', 'Baseline (small)', 'dropout=0.3']\n",
    "\n",
    "for name, color in zip(dropout_exps, colors):\n",
    "    r = results[name]\n",
    "    ax1.plot(r['steps'], r['train_losses'], marker='o', label=f\"{name} (train)\", color=color, linestyle='-')\n",
    "    ax2.plot(r['steps'], r['val_losses'], marker='s', label=f\"{name} (val)\", color=color, linestyle='-')\n",
    "\n",
    "ax1.set_title('Training Loss ‚Äî Varying dropout', fontsize=12)\n",
    "ax1.set_xlabel('Step'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.grid(alpha=0.3)\n",
    "ax2.set_title('Validation Loss ‚Äî Varying dropout', fontsize=12)\n",
    "ax2.set_xlabel('Step'); ax2.set_ylabel('Loss'); ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b05014",
   "metadata": {},
   "source": [
    "### 6.7 ‚Äî Training Curves: Effect of Block Size (Context Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "block_exps = ['block_size=32', 'Baseline (small)', 'block_size=256']\n",
    "\n",
    "for name, color in zip(block_exps, colors):\n",
    "    r = results[name]\n",
    "    ax1.plot(r['steps'], r['train_losses'], marker='o', label=f\"{name} (train)\", color=color, linestyle='-')\n",
    "    ax2.plot(r['steps'], r['val_losses'], marker='s', label=f\"{name} (val)\", color=color, linestyle='-')\n",
    "\n",
    "ax1.set_title('Training Loss ‚Äî Varying block_size', fontsize=12)\n",
    "ax1.set_xlabel('Step'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.grid(alpha=0.3)\n",
    "ax2.set_title('Validation Loss ‚Äî Varying block_size', fontsize=12)\n",
    "ax2.set_xlabel('Step'); ax2.set_ylabel('Loss'); ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd19ae",
   "metadata": {},
   "source": [
    "### 6.8 ‚Äî Parameter Count vs. Validation Loss\n",
    "\n",
    "Does throwing more parameters at the problem always help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for name, r in results.items():\n",
    "    ax.scatter(r['n_params'], r['final_val_loss'], s=100, zorder=5)\n",
    "    ax.annotate(name, (r['n_params'], r['final_val_loss']),\n",
    "                textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Number of Parameters', fontsize=12)\n",
    "ax.set_ylabel('Final Validation Loss', fontsize=12)\n",
    "ax.set_title('Parameters vs. Validation Loss', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6636e07",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Generation Quality Comparison ‚úçÔ∏è\n",
    "\n",
    "Numbers tell one story, but **reading the generated text** tells another. Let's compare outputs side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, r in results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"  Val Loss: {r['final_val_loss']:.4f} | Params: {r['n_params']:,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    # Show first 300 chars of generated text\n",
    "    print(r['sample_text'][:300])\n",
    "    print(f\"{'‚îÄ'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49abd9",
   "metadata": {},
   "source": [
    "### Attention Heads: Small Baseline vs. Single Head\n",
    "\n",
    "Let's compare how attention looks in the small baseline (4 heads) vs. a single-head model. The single-head model must cram all attention patterns into one head, while the multi-head model can specialise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a short input for the small models\n",
    "short_text = \"KING HENRY:\"\n",
    "short_tokens = encode(short_text)\n",
    "\n",
    "# We need block_size to be at least as long as our input\n",
    "for name in ['Baseline (small)', 'n_head=1']:\n",
    "    r = results[name]\n",
    "    cfg = r['config']\n",
    "    model = r['model']\n",
    "    model.eval()\n",
    "    \n",
    "    # Pad or trim to fit the model's block_size\n",
    "    tokens = short_tokens[:cfg['block_size']]\n",
    "    inp = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    plot_attention_heads(model, inp, layer_idx=0, n_heads_to_show=cfg['n_head'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21624355",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Written Analysis üìù\n",
    "\n",
    "### 8.1 ‚Äî Effect of `n_head` (Number of Attention Heads)\n",
    "\n",
    "**Observations:**\n",
    "- **1 head:** The single head must learn all attention patterns by itself. Typically results in slightly higher validation loss compared to 4 heads.\n",
    "- **4 heads (baseline):** Good balance ‚Äî each head can specialise on different pattern types (e.g., one head for local bigram-like patterns, another for longer-range structure).\n",
    "- **8 heads:** With `n_embed=64`, each head only gets a 8-dimensional subspace (`64 / 8 = 8`). This may be too small ‚Äî each head has limited capacity. Performance may plateau or slightly degrade vs. 4 heads at this embedding size.\n",
    "\n",
    "**Takeaway:** More heads help up to a point, but each head needs enough dimensions to be effective. The sweet spot depends on `n_embed`.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2 ‚Äî Effect of `n_layer` (Number of Transformer Blocks)\n",
    "\n",
    "**Observations:**\n",
    "- **1 layer:** Very limited ‚Äî can only do a single round of \"communication\" between characters. Produces lower-quality text resembling a souped-up bigram.\n",
    "- **4 layers (baseline):** Significant improvement. Multiple rounds of attention allow the model to build up understanding of longer-range patterns.\n",
    "- **8 layers:** More parameters and more processing depth. May improve slightly, but at this model scale diminishing returns set in. Also slower to train.\n",
    "\n",
    "**Takeaway:** Depth (layers) is one of the most impactful hyperparameters. Going from 1‚Üí4 layers is a dramatic improvement; 4‚Üí8 shows diminishing returns for small models.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.3 ‚Äî Effect of `n_embed` (Embedding Dimension)\n",
    "\n",
    "**Observations:**\n",
    "- **32 dimensions:** Very compact representations. The model struggles to encode enough information per token.\n",
    "- **64 dimensions (baseline):** Good balance of expressiveness and training speed.\n",
    "- **128 dimensions:** Richer representations ‚Äî each token carries more information. Typically the best validation loss among the three, but takes longer to train and uses 4√ó more parameters.\n",
    "\n",
    "**Takeaway:** `n_embed` has a multiplicative effect on model size (affects every layer). Larger embeddings almost always help, but at increasing computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.4 ‚Äî Effect of `dropout`\n",
    "\n",
    "**Observations:**\n",
    "- **dropout=0.0:** No regularisation. The model may overfit (train loss much lower than val loss), especially with many training steps.\n",
    "- **dropout=0.1 (baseline):** Mild regularisation. Usually gives the best val loss for models of this size.\n",
    "- **dropout=0.3:** Aggressive regularisation. May actually hurt training ‚Äî the model can't learn patterns fast enough because too many neurons are dropped.\n",
    "\n",
    "**Takeaway:** Dropout is a double-edged sword. Too little ‚Üí overfitting; too much ‚Üí underfitting. The right value depends on model size and training duration.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.5 ‚Äî Effect of `block_size` (Context Length)\n",
    "\n",
    "**Observations:**\n",
    "- **block_size=32:** Very short context. The model can only \"see\" 32 characters back. Can't capture patterns spanning multiple words or lines.\n",
    "- **block_size=128 (baseline):** Around 20‚Äì30 words of context. Captures most local patterns in Shakespeare.\n",
    "- **block_size=256:** Longer context ‚Äî allows the model to understand paragraph-level structure. However, the $O(T^2)$ cost of attention means training is slower.\n",
    "\n",
    "**Takeaway:** Longer context is almost always better for generation quality, but the quadratic cost of attention makes it expensive. This is why modern models use techniques like sparse attention, FlashAttention, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.6 ‚Äî Overall Conclusions\n",
    "\n",
    "1. **Embedding dimension** (`n_embed`) has the strongest effect on model quality ‚Äî it controls the \"width\" of representations at every layer.\n",
    "2. **Number of layers** (`n_layer`) is the second most impactful ‚Äî depth allows the model to compose increasingly abstract features.\n",
    "3. **Number of heads** (`n_head`) matters less than embed/layers at this scale, but enables specialisation.\n",
    "4. **Block size** improves quality but at quadratic cost ‚Äî the returns are diminishing beyond a point.\n",
    "5. **Dropout** is primarily about preventing overfitting; the optimal value depends on model/data size.\n",
    "\n",
    "> **The key insight:** the Transformer architecture is remarkably scalable. The *same* architecture works from our tiny 200K-parameter model all the way to GPT-4's 1.8 trillion parameters ‚Äî you just turn up these same knobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74e5d3",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary ‚úÖ\n",
    "\n",
    "### What We Did\n",
    "- ‚úÖ Loaded the pre-trained baseline GPT model from Day 8\n",
    "- ‚úÖ Visualised attention weight heatmaps across layers and heads\n",
    "- ‚úÖ Trained 11 model variants, each changing one hyperparameter\n",
    "- ‚úÖ Compared validation losses with bar charts and training curves\n",
    "- ‚úÖ Plotted parameter count vs. validation loss (scaling analysis)\n",
    "- ‚úÖ Compared generated text quality side by side\n",
    "- ‚úÖ Analysed the role of each hyperparameter in depth\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Hyperparameter | Increases Capacity? | Main Risk | Sweet Spot (small model) |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| `n_embed` ‚Üë | ‚úÖ Width | Memory/speed | 64‚Äì128 |\n",
    "| `n_layer` ‚Üë | ‚úÖ Depth | Diminishing returns | 4‚Äì6 |\n",
    "| `n_head` ‚Üë | ‚úÖ Specialisation | Too-small head dim | 4 at embed=64 |\n",
    "| `block_size` ‚Üë | ‚úÖ Context | $O(T^2)$ cost | 128‚Äì256 |\n",
    "| `dropout` ‚Üë | ‚ùå Regularisation | Under-fitting | 0.1‚Äì0.2 |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand *how* each hyperparameter affects the model, you could:\n",
    "- Try **combinations** (e.g., larger embed + more layers)\n",
    "- Add a **learning rate scheduler** (warmup + cosine decay)\n",
    "- Experiment with **different optimisers** (SGD vs. Adam vs. AdamW)\n",
    "- Try **different datasets** (code, poetry, song lyrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
