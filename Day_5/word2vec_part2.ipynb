{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239196ee",
   "metadata": {},
   "source": [
    "Now we'll create the (target, context) pairs that will train our model. This is where we transform unsupervised text into supervised learning data!\n",
    "\n",
    "**The Sliding Window Approach:**\n",
    "\n",
    "We slide a window across our text and for each target word, we look at surrounding words within the window.\n",
    "\n",
    "**Example with window_size = 2:**\n",
    "\n",
    "```\n",
    "Sentence: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "          [  0  ,   1  ,   2  ,  3 ,   4  ,   5  ]  (positions)\n",
    "\n",
    "Target word at position 2 (\"sat\"):\n",
    "  - Look left up to 2 positions: \"the\" (pos 0), \"cat\" (pos 1)\n",
    "  - Look right up to 2 positions: \"on\" (pos 3), \"the\" (pos 4)\n",
    "\n",
    "Training pairs created:\n",
    "  (sat, the)   # from position 0\n",
    "  (sat, cat)   # from position 1\n",
    "  (sat, on)    # from position 3\n",
    "  (sat, the)   # from position 4\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **window_size** determines how far we look for context\n",
    "- Larger windows capture broader context but may include less relevant words\n",
    "- Smaller windows focus on immediate neighbors\n",
    "- Common values: 2-5\n",
    "\n",
    "**Implementation Strategy:**\n",
    "1. Convert tokens to indices\n",
    "2. For each target word position:\n",
    "   - Get words within the window\n",
    "   - Create (target_idx, context_idx) pairs\n",
    "3. Return as PyTorch tensors for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7538b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_pairs(tokens, word_to_idx, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate (target, context) training pairs using Skip-Gram approach\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of word tokens\n",
    "        word_to_idx: Dictionary mapping words to indices\n",
    "        window_size: How many words to look at on each side\n",
    "    \n",
    "    Returns:\n",
    "        target_indices: Tensor of target word indices\n",
    "        context_indices: Tensor of context word indices\n",
    "    \"\"\"\n",
    "    target_indices = []\n",
    "    context_indices = []\n",
    "    \n",
    "    # Convert tokens to indices\n",
    "    token_indices = [word_to_idx[token] for token in tokens if token in word_to_idx]\n",
    "    \n",
    "    # Slide window across the text\n",
    "    for target_pos in range(len(token_indices)):\n",
    "        target_idx = token_indices[target_pos]\n",
    "        \n",
    "        # Define the window boundaries\n",
    "        start = max(0, target_pos - window_size)\n",
    "        end = min(len(token_indices), target_pos + window_size + 1)\n",
    "        \n",
    "        # Get context words (all words in window except target)\n",
    "        for context_pos in range(start, end):\n",
    "            if context_pos != target_pos:  # Don't use the target word as its own context\n",
    "                context_idx = token_indices[context_pos]\n",
    "                target_indices.append(target_idx)\n",
    "                context_indices.append(context_idx)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    target_tensor = torch.tensor(target_indices, dtype=torch.long)\n",
    "    context_tensor = torch.tensor(context_indices, dtype=torch.long)\n",
    "    \n",
    "    return target_tensor, context_tensor\n",
    "\n",
    "# Generate training pairs\n",
    "window_size = 2\n",
    "target_tensor, context_tensor = generate_training_pairs(tokens, word_to_idx, window_size)\n",
    "\n",
    "print(\"Training Pair Generation:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Total training pairs: {len(target_tensor):,}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c094e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample training pairs\n",
    "print(\"\\nSample Training Pairs:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Target Word':<15} | {'Context Word':<15} | {'Target Idx':<10} | {'Context Idx'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_samples = 20\n",
    "for i in range(min(num_samples, len(target_tensor))):\n",
    "    target_idx = target_tensor[i].item()\n",
    "    context_idx = context_tensor[i].item()\n",
    "    target_word = idx_to_word[target_idx]\n",
    "    context_word = idx_to_word[context_idx]\n",
    "    print(f\"{target_word:<15} | {context_word:<15} | {target_idx:<10} | {context_idx}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ… Generated {len(target_tensor):,} training pairs!\")\n",
    "print(f\"   These pairs will teach the model which words appear together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bb1ed",
   "metadata": {},
   "source": [
    "**Understanding the Training Pairs:**\n",
    "\n",
    "Looking at the pairs above, notice:\n",
    "- \"the\" appears with many different words (it's a common article)\n",
    "- \"cat\" appears with words like \"the\", \"sat\", \"mat\" (typical cat-related contexts)\n",
    "- \"dog\" appears with similar words to \"cat\" (helping the model learn they're related)\n",
    "- \"king\" and \"queen\" appear in similar contexts (both relate to \"rules\", \"kingdom\", etc.)\n",
    "\n",
    "The model will learn to give similar embeddings to words that share similar contexts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf2c65",
   "metadata": {},
   "source": [
    "### **9. Building the Skip-Gram Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b80fb",
   "metadata": {},
   "source": [
    "Now let's implement our Skip-Gram neural network in PyTorch!\n",
    "\n",
    "**Model Architecture Recap:**\n",
    "\n",
    "```\n",
    "Input: target_word_idx (scalar integer)\n",
    "   â†“\n",
    "Embedding Layer: [vocab_size Ã— embedding_dim]\n",
    "   â†“\n",
    "Word Vector: [embedding_dim]\n",
    "   â†“\n",
    "Linear Layer: [embedding_dim Ã— vocab_size]\n",
    "   â†“\n",
    "Output Scores: [vocab_size]\n",
    "```\n",
    "\n",
    "**The Magic of nn.Embedding:**\n",
    "\n",
    "`nn.Embedding` is essentially a lookup table:\n",
    "- It stores a matrix of size `(vocab_size, embedding_dim)`\n",
    "- When you pass in an index, it returns the corresponding row\n",
    "- During training, these rows (word vectors) are updated via backpropagation\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "embedding = nn.Embedding(vocab_size=1000, embedding_dim=50)\n",
    "# This creates a 1000 Ã— 50 matrix\n",
    "\n",
    "word_idx = torch.tensor([42])\n",
    "word_vector = embedding(word_idx)\n",
    "# Returns row 42 from the matrix (a 50-dimensional vector)\n",
    "```\n",
    "\n",
    "**After training, the weights of this embedding layer ARE our word vectors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b308453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Skip-Gram model\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Number of unique words in vocabulary\n",
    "            embedding_dim: Dimension of word embedding vectors\n",
    "        \"\"\"\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer: converts word indices to dense vectors\n",
    "        # This is the layer whose weights we want to extract after training!\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Linear layer: maps from embedding space to vocabulary space\n",
    "        # Used to predict context words\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, target_indices):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            target_indices: Indices of target words (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            Scores for each word in vocabulary (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Look up embeddings for target words\n",
    "        # Shape: (batch_size, embedding_dim)\n",
    "        embeds = self.embeddings(target_indices)\n",
    "        \n",
    "        # Project to vocabulary space\n",
    "        # Shape: (batch_size, vocab_size)\n",
    "        output = self.linear(embeds)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Extract the learned word embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Embedding matrix of shape (vocab_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embeddings.weight.data\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 50  # Dimension of word vectors (common choices: 50, 100, 300)\n",
    "\n",
    "# Create the model\n",
    "model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "print(\"Skip-Gram Model Architecture:\")\n",
    "print(\"=\"*70)\n",
    "print(model)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "embedding_params = model.embeddings.weight.numel()\n",
    "linear_params = sum(p.numel() for p in model.linear.parameters())\n",
    "\n",
    "print(f\"\\nParameter Breakdown:\")\n",
    "print(f\"  Embedding layer: {embedding_params:,} parameters ({vocab_size} Ã— {embedding_dim})\")\n",
    "print(f\"  Linear layer:    {linear_params:,} parameters ({embedding_dim} Ã— {vocab_size} + {vocab_size})\")\n",
    "print(f\"  Total:           {total_params:,} parameters\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd88de",
   "metadata": {},
   "source": [
    "### **10. Loss Function and Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4402c3",
   "metadata": {},
   "source": [
    "**Loss Function: Cross-Entropy Loss**\n",
    "\n",
    "Just like in classification tasks, we use Cross-Entropy Loss because:\n",
    "- We're predicting which word (from vocabulary) is in the context\n",
    "- This is a multi-class classification problem\n",
    "- Cross-Entropy measures how well predicted probabilities match the true context word\n",
    "\n",
    "**Training Objective:**\n",
    "\n",
    "For a (target, context) pair like `(\"cat\", \"sat\")`:\n",
    "1. Input: \"cat\" index\n",
    "2. Model outputs: probability distribution over all words\n",
    "3. Loss: How different is this from the true distribution (where \"sat\" = 1, others = 0)\n",
    "4. Backprop: Adjust embeddings to increase probability of \"sat\"\n",
    "\n",
    "**Optimizer: Adam**\n",
    "\n",
    "We'll use Adam optimizer (like in Day 4) because:\n",
    "- Adaptive learning rates work well for embeddings\n",
    "- Converges faster than vanilla SGD\n",
    "- Requires minimal hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Loss Function:  {criterion}\")\n",
    "print(f\"Optimizer:      Adam\")\n",
    "print(f\"Learning Rate:  {learning_rate}\")\n",
    "print(f\"Device:         {device}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db0f1b",
   "metadata": {},
   "source": [
    "### **11. Training the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405e634",
   "metadata": {},
   "source": [
    "Now we'll train our Word2Vec model! The training process is similar to what we've done before:\n",
    "\n",
    "**Training Loop:**\n",
    "1. **Forward Pass:** Feed target word indices â†’ get predictions\n",
    "2. **Calculate Loss:** Compare predictions with actual context words\n",
    "3. **Backward Pass:** Calculate gradients\n",
    "4. **Update Parameters:** Adjust embedding weights\n",
    "\n",
    "**What's Happening:**\n",
    "- The model learns to predict context words from target words\n",
    "- Words appearing in similar contexts get similar embeddings\n",
    "- After many updates, the embedding layer contains meaningful word vectors\n",
    "\n",
    "**Training Tips:**\n",
    "- For small datasets, 100-500 epochs is typical\n",
    "- Loss should decrease steadily\n",
    "- For real applications, you'd use millions of words and more sophisticated techniques (negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(model, target_tensor, context_tensor, criterion, optimizer, \n",
    "                   num_epochs=100, batch_size=128, print_every=10):\n",
    "    \"\"\"\n",
    "    Train the Word2Vec Skip-Gram model\n",
    "    \n",
    "    Args:\n",
    "        model: The Skip-Gram model\n",
    "        target_tensor: Target word indices\n",
    "        context_tensor: Context word indices\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Number of samples per batch\n",
    "        print_every: Print progress every N epochs\n",
    "    \n",
    "    Returns:\n",
    "        List of losses per epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    # Move data to device\n",
    "    target_tensor = target_tensor.to(device)\n",
    "    context_tensor = context_tensor.to(device)\n",
    "    \n",
    "    num_samples = len(target_tensor)\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # Get batch\n",
    "            batch_targets = target_tensor[i:i+batch_size]\n",
    "            batch_contexts = context_tensor[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_targets)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, batch_contexts)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average loss for this epoch\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1:4d}/{num_epochs}] | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Training Complete!\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "losses = train_word2vec(\n",
    "    model, \n",
    "    target_tensor, \n",
    "    context_tensor, \n",
    "    criterion, \n",
    "    optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    print_every=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe02ef",
   "metadata": {},
   "source": [
    "### **12. Visualizing Training Progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c53521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), losses, linewidth=2, color='#e74c3c')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Word2Vec Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Initial Loss: {losses[0]:.4f}\")\n",
    "print(f\"Final Loss:   {losses[-1]:.4f}\")\n",
    "print(f\"Reduction:    {losses[0] - losses[-1]:.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ… The model has learned word embeddings!\")\n",
    "print(\"   Words appearing in similar contexts now have similar vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da938c1",
   "metadata": {},
   "source": [
    "### **13. Extracting and Analyzing Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca448bb",
   "metadata": {},
   "source": [
    "The moment we've been waiting for! Let's extract the learned word vectors from the embedding layer.\n",
    "\n",
    "**Remember:** The weights of `model.embeddings` are our word vectors!\n",
    "- Each row corresponds to one word\n",
    "- Each row is a `embedding_dim` dimensional vector\n",
    "- Words with similar meanings should have similar vectors\n",
    "\n",
    "**How to measure similarity:**\n",
    "\n",
    "We use **Cosine Similarity**:\n",
    "\n",
    "$$\\text{similarity}(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| \\cdot ||\\vec{b}||} = \\cos(\\theta)$$\n",
    "\n",
    "- Returns values from -1 to 1\n",
    "- 1 = identical direction (very similar)\n",
    "- 0 = orthogonal (unrelated)\n",
    "- -1 = opposite direction (opposite meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf73382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the learned embeddings\n",
    "model.eval()\n",
    "embeddings = model.get_embeddings().cpu().numpy()\n",
    "\n",
    "print(\"Learned Word Embeddings:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"  â†’ {embeddings.shape[0]} words, each represented by {embeddings.shape[1]} numbers\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display embedding for a sample word\n",
    "sample_word = \"cat\"\n",
    "if sample_word in word_to_idx:\n",
    "    word_idx = word_to_idx[sample_word]\n",
    "    word_embedding = embeddings[word_idx]\n",
    "    \n",
    "    print(f\"\\nEmbedding for '{sample_word}':\")\n",
    "    print(f\"First 10 dimensions: {word_embedding[:10]}\")\n",
    "    print(f\"\\nThis {embedding_dim}-dimensional vector captures the meaning of '{sample_word}'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between all pairs of word embeddings\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Embedding matrix (vocab_size, embedding_dim)\n",
    "    \n",
    "    Returns:\n",
    "        Similarity matrix (vocab_size, vocab_size)\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / (norms + 1e-8)  # Add small value to avoid division by zero\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = np.dot(normalized, normalized.T)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def find_most_similar(word, word_to_idx, idx_to_word, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the most similar words to a given word\n",
    "    \n",
    "    Args:\n",
    "        word: Query word\n",
    "        word_to_idx: Word to index mapping\n",
    "        idx_to_word: Index to word mapping\n",
    "        embeddings: Embedding matrix\n",
    "        top_k: Number of similar words to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        return []\n",
    "    \n",
    "    # Get word index and embedding\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_embedding = embeddings[word_idx]\n",
    "    \n",
    "    # Compute similarities with all words\n",
    "    similarities = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        other_embedding = embeddings[idx]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(word_embedding, other_embedding) / (\n",
    "            np.linalg.norm(word_embedding) * np.linalg.norm(other_embedding) + 1e-8\n",
    "        )\n",
    "        \n",
    "        similarities.append((idx, similarity))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top k (excluding the word itself)\n",
    "    results = []\n",
    "    for idx, sim in similarities[1:top_k+1]:  # Skip first (the word itself)\n",
    "        results.append((idx_to_word[idx], sim))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the similarity function\n",
    "print(\"\\nTesting Word Similarity:\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words for different test cases\n",
    "test_words = [\"cat\", \"dog\", \"king\", \"queen\", \"happy\"]\n",
    "\n",
    "for test_word in test_words:\n",
    "    if test_word in word_to_idx:\n",
    "        print(f\"\\nMost similar words to '{test_word}':\")\n",
    "        print(\"-\" * 50)\n",
    "        similar_words = find_most_similar(test_word, word_to_idx, idx_to_word, embeddings, top_k=5)\n",
    "        \n",
    "        for i, (word, similarity) in enumerate(similar_words, 1):\n",
    "            print(f\"  {i}. {word:<15s} (similarity: {similarity:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e013a8",
   "metadata": {},
   "source": [
    "**Interpreting the Results:**\n",
    "\n",
    "If the model trained well, you should see:\n",
    "- **\"cat\"** is similar to **\"dog\"** (both animals, both pets)\n",
    "- **\"king\"** is similar to **\"queen\"** (both royalty)\n",
    "- **\"happy\"** is similar to words in emotional contexts\n",
    "\n",
    "The model learned these relationships **purely from seeing which words appear together** - we never told it that cats and dogs are animals!\n",
    "\n",
    "**Note:** With our small corpus, the similarities might not be perfect. In practice:\n",
    "- Use millions of words of text\n",
    "- Train for longer\n",
    "- Use larger embedding dimensions (100-300)\n",
    "- Use techniques like negative sampling for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb3c21",
   "metadata": {},
   "source": [
    "### **14. Visualizing Embeddings in 2D**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774d6ee",
   "metadata": {},
   "source": [
    "Our embeddings are 50-dimensional, which is impossible to visualize directly. Let's use **PCA (Principal Component Analysis)** to reduce them to 2D so we can see the relationships!\n",
    "\n",
    "**What is PCA?**\n",
    "- A dimensionality reduction technique\n",
    "- Finds the 2 most important directions (principal components) in the high-dimensional space\n",
    "- Projects the data onto these 2 dimensions\n",
    "- Preserves as much variance (information) as possible\n",
    "\n",
    "**What to look for:**\n",
    "- Similar words should cluster together\n",
    "- \"cat\" and \"dog\" should be near each other\n",
    "- \"king\" and \"queen\" should be near each other\n",
    "- Unrelated words should be far apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61738b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce embeddings to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "print(\"PCA Dimensionality Reduction:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original dimensions: {embeddings.shape[1]}D\")\n",
    "print(f\"Reduced dimensions:  {embeddings_2d.shape[1]}D\")\n",
    "print(f\"Variance explained:  {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5319d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all embeddings\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot all points\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5, s=50, color='steelblue')\n",
    "\n",
    "# Annotate each point with its word\n",
    "for idx, word in idx_to_word.items():\n",
    "    x, y = embeddings_2d[idx]\n",
    "    plt.annotate(word, (x, y), fontsize=9, alpha=0.8)\n",
    "\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('Word Embeddings Visualization (2D Projection via PCA)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Look for patterns:\")\n",
    "print(\"   - Are 'cat' and 'dog' close together?\")\n",
    "print(\"   - Are 'king' and 'queen' close together?\")\n",
    "print(\"   - Do related words form clusters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight specific word groups\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Define word groups to highlight\n",
    "animals = ['cat', 'dog', 'cats', 'dogs', 'animals']\n",
    "royalty = ['king', 'queen', 'kingdom', 'empire', 'royalty']\n",
    "emotions = ['happy', 'joyful']\n",
    "\n",
    "# Plot with colors for different groups\n",
    "for idx, word in idx_to_word.items():\n",
    "    x, y = embeddings_2d[idx]\n",
    "    \n",
    "    if word in animals:\n",
    "        color = 'green'\n",
    "        marker = 'o'\n",
    "        size = 150\n",
    "    elif word in royalty:\n",
    "        color = 'purple'\n",
    "        marker = 's'\n",
    "        size = 150\n",
    "    elif word in emotions:\n",
    "        color = 'orange'\n",
    "        marker = '^'\n",
    "        size = 150\n",
    "    else:\n",
    "        color = 'lightgray'\n",
    "        marker = '.'\n",
    "        size = 50\n",
    "    \n",
    "    plt.scatter(x, y, color=color, marker=marker, s=size, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Annotate\n",
    "    if word in animals + royalty + emotions:\n",
    "        plt.annotate(word, (x, y), fontsize=11, fontweight='bold', \n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3))\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Animals'),\n",
    "    Line2D([0], [0], marker='s', color='w', markerfacecolor='purple', markersize=10, label='Royalty'),\n",
    "    Line2D([0], [0], marker='^', color='w', markerfacecolor='orange', markersize=10, label='Emotions'),\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='best', fontsize=11)\n",
    "\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('Word Embeddings - Semantic Clusters', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb74cac",
   "metadata": {},
   "source": [
    "### **15. Word Analogies: The Magic of Vector Arithmetic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62abfadf",
   "metadata": {},
   "source": [
    "One of the most fascinating properties of word embeddings is that you can perform **mathematical operations on meaning**!\n",
    "\n",
    "**Famous Example:**\n",
    "\n",
    "$$\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$$\n",
    "\n",
    "**The Logic:**\n",
    "- $\\vec{king} - \\vec{man}$ = \"royalty\" concept (removes the \"male\" aspect)\n",
    "- Add $\\vec{woman}$ = \"female royalty\"\n",
    "- Result should be close to $\\vec{queen}$!\n",
    "\n",
    "**How it works:**\n",
    "1. Perform vector arithmetic on embeddings\n",
    "2. Find the word whose embedding is closest to the result\n",
    "3. This word is the \"answer\" to the analogy\n",
    "\n",
    "Let's try this with our trained embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word_a, word_b, word_c, word_to_idx, idx_to_word, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Solve word analogy: word_a is to word_b as word_c is to ?\n",
    "    Computes: embedding(word_b) - embedding(word_a) + embedding(word_c)\n",
    "    \n",
    "    Args:\n",
    "        word_a, word_b, word_c: Words in the analogy\n",
    "        word_to_idx: Word to index mapping\n",
    "        idx_to_word: Index to word mapping\n",
    "        embeddings: Embedding matrix\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, similarity) tuples\n",
    "    \"\"\"\n",
    "    # Check if all words are in vocabulary\n",
    "    if word_a not in word_to_idx or word_b not in word_to_idx or word_c not in word_to_idx:\n",
    "        return []\n",
    "    \n",
    "    # Get embeddings\n",
    "    vec_a = embeddings[word_to_idx[word_a]]\n",
    "    vec_b = embeddings[word_to_idx[word_b]]\n",
    "    vec_c = embeddings[word_to_idx[word_c]]\n",
    "    \n",
    "    # Compute analogy vector: b - a + c\n",
    "    target_vector = vec_b - vec_a + vec_c\n",
    "    \n",
    "    # Find closest words\n",
    "    similarities = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        # Skip the input words\n",
    "        if idx in [word_to_idx[word_a], word_to_idx[word_b], word_to_idx[word_c]]:\n",
    "            continue\n",
    "        \n",
    "        other_embedding = embeddings[idx]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(target_vector, other_embedding) / (\n",
    "            np.linalg.norm(target_vector) * np.linalg.norm(other_embedding) + 1e-8\n",
    "        )\n",
    "        \n",
    "        similarities.append((idx, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top k\n",
    "    results = [(idx_to_word[idx], sim) for idx, sim in similarities[:top_k]]\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Word Analogy Testing:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Format: 'A is to B as C is to ?'\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test analogies\n",
    "analogies = [\n",
    "    (\"king\", \"queen\", \"man\"),      # king:queen :: man:?\n",
    "    (\"cat\", \"cats\", \"dog\"),        # cat:cats :: dog:?\n",
    "    (\"happy\", \"joyful\", \"cat\"),    # happy:joyful :: cat:?\n",
    "]\n",
    "\n",
    "for word_a, word_b, word_c in analogies:\n",
    "    print(f\"\\n'{word_a}' is to '{word_b}' as '{word_c}' is to:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = word_analogy(word_a, word_b, word_c, word_to_idx, idx_to_word, embeddings, top_k=3)\n",
    "    \n",
    "    if results:\n",
    "        for i, (word, similarity) in enumerate(results, 1):\n",
    "            print(f\"  {i}. {word:<15s} (similarity: {similarity:.4f})\")\n",
    "    else:\n",
    "        print(\"  (Words not in vocabulary)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nðŸ’¡ Note: With our small corpus, analogies may not be perfect.\")\n",
    "print(\"   Larger datasets produce much better analogies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358dc0ab",
   "metadata": {},
   "source": [
    "### **16. Conclusion and Key Takeaways**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd671be",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully implemented Word2Vec from scratch and learned how machines can understand language!\n",
    "\n",
    "**ðŸŽ¯ What We Accomplished:**\n",
    "\n",
    "1. âœ… Understood the problem: How to represent words as numbers\n",
    "2. âœ… Learned about one-hot encoding vs dense embeddings\n",
    "3. âœ… Explored the distributional hypothesis (\"a word is known by its context\")\n",
    "4. âœ… Preprocessed text: tokenization, vocabulary building\n",
    "5. âœ… Generated training pairs using the Skip-Gram approach\n",
    "6. âœ… Built and trained a neural network to learn word embeddings\n",
    "7. âœ… Extracted learned embeddings and found similar words\n",
    "8. âœ… Visualized embeddings in 2D to see semantic clusters\n",
    "9. âœ… Performed word analogies using vector arithmetic\n",
    "\n",
    "**ðŸ”‘ Key Concepts:**\n",
    "\n",
    "1. **Word Embeddings:**\n",
    "   - Dense vector representations of words\n",
    "   - Capture semantic meaning automatically\n",
    "   - Much better than sparse one-hot encodings\n",
    "\n",
    "2. **Distributional Hypothesis:**\n",
    "   - Words appearing in similar contexts have similar meanings\n",
    "   - The foundation of Word2Vec and modern NLP\n",
    "\n",
    "3. **Skip-Gram Model:**\n",
    "   - Predicts context words from target words\n",
    "   - Simple 2-layer architecture\n",
    "   - The embedding layer weights ARE the word vectors\n",
    "\n",
    "4. **Training Process:**\n",
    "   - Transform text into (target, context) pairs\n",
    "   - Train like any supervised learning problem\n",
    "   - Similar words end up with similar vectors\n",
    "\n",
    "5. **Vector Arithmetic:**\n",
    "   - Can perform math on meanings: king - man + woman â‰ˆ queen\n",
    "   - Embeddings capture semantic relationships\n",
    "\n",
    "**ðŸ“Š From Simple to Powerful:**\n",
    "\n",
    "| Aspect | One-Hot Encoding | Word Embeddings |\n",
    "|--------|-----------------|------------------|\n",
    "| **Dimensions** | Vocabulary size (e.g., 100,000) | Fixed small size (50-300) |\n",
    "| **Sparsity** | 99.999% zeros | Dense (all values meaningful) |\n",
    "| **Semantic Meaning** | None (all words equally different) | Rich (similar words have similar vectors) |\n",
    "| **Generalization** | Poor | Excellent |\n",
    "| **Vector Arithmetic** | Meaningless | Meaningful analogies! |\n",
    "\n",
    "**ðŸš€ Real-World Applications:**\n",
    "\n",
    "Word embeddings are used in:\n",
    "- **Search Engines:** Understanding query intent\n",
    "- **Recommendation Systems:** Finding similar products/content\n",
    "- **Sentiment Analysis:** Understanding emotions in text\n",
    "- **Machine Translation:** Google Translate, DeepL\n",
    "- **Chatbots:** Understanding user messages\n",
    "- **Question Answering:** Finding relevant answers\n",
    "- **Document Classification:** Categorizing articles, emails\n",
    "\n",
    "**ðŸ’¡ Improvements for Production:**\n",
    "\n",
    "Our implementation is educational. For real applications:\n",
    "\n",
    "1. **Use More Data:**\n",
    "   - Millions/billions of words (Wikipedia, books, web pages)\n",
    "   - Our 200 words â†’ Real systems use billions\n",
    "\n",
    "2. **Negative Sampling:**\n",
    "   - Instead of predicting over entire vocabulary (slow)\n",
    "   - Sample a few \"negative\" examples\n",
    "   - Much faster and more efficient\n",
    "\n",
    "3. **Subword Information:**\n",
    "   - Handle rare/unknown words better\n",
    "   - FastText extends Word2Vec with character n-grams\n",
    "\n",
    "4. **Pre-trained Embeddings:**\n",
    "   - Use embeddings trained on huge corpora\n",
    "   - GloVe, FastText, Word2Vec pre-trained models\n",
    "   - Don't train from scratch for every task\n",
    "\n",
    "5. **Modern Alternatives:**\n",
    "   - **BERT, GPT, T5:** Contextual embeddings (different vectors for same word in different contexts)\n",
    "   - **Sentence Transformers:** Embeddings for entire sentences\n",
    "   - But Word2Vec is still widely used and very effective!\n",
    "\n",
    "**ðŸŽ“ From Images to Language:**\n",
    "\n",
    "Compare our journey:\n",
    "\n",
    "| Day | Task | Input | Output | Key Learning |\n",
    "|-----|------|-------|--------|---------------|\n",
    "| **4** | Fashion MNIST | Images (28Ã—28 pixels) | 10 clothing classes | CNNs for vision |\n",
    "| **5** | Word2Vec | Text (words) | Word vectors | Embeddings for NLP |\n",
    "\n",
    "Both use neural networks but handle different data types!\n",
    "\n",
    "**ðŸŒŸ You've now learned the foundation of Natural Language Processing!**\n",
    "\n",
    "From here, you can explore:\n",
    "- Recurrent Neural Networks (RNNs) for sequences\n",
    "- Transformers (the architecture behind ChatGPT)\n",
    "- BERT, GPT, and other large language models\n",
    "- Sentiment analysis, machine translation, text generation\n",
    "\n",
    "All of these build on the concept of word embeddings that you learned today!\n",
    "\n",
    "---\n",
    "\n",
    "**Keep exploring and building! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
