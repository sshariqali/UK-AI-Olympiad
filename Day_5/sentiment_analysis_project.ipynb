{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3f3bda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 1: Setup and Understanding the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab7912",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Import necessary libraries and load the IMDB movie review dataset.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Import libraries:**\n",
    "   - `torch` and `torch.nn`\n",
    "   - `numpy`\n",
    "   - `pandas` (for loading CSV data)\n",
    "   - `matplotlib.pyplot`\n",
    "   - `sklearn.model_selection` (train_test_split)\n",
    "   - `sklearn.metrics` (accuracy_score, classification_report, confusion_matrix)\n",
    "   - `re` (for text cleaning)\n",
    "\n",
    "2. **Check device availability** (GPU or CPU)\n",
    "\n",
    "3. **Load the IMDB movie review dataset**:\n",
    "   - Load data from `IMDB Dataset.csv`\n",
    "   - The CSV has two columns: `review` (text) and `sentiment` (positive/negative)\n",
    "   - Convert sentiment labels to numeric: 0=negative, 1=positive\n",
    "   - Explore the dataset structure\n",
    "\n",
    "### üí° Hints\n",
    "- Use `torch.cuda.is_available()` to check for GPU\n",
    "- Use `pandas.read_csv()` to load the CSV file\n",
    "- Labels: 0 = negative sentiment, 1 = positive sentiment\n",
    "- The IMDB dataset contains 50,000 real movie reviews from IMDB\n",
    "\n",
    "### ü§î Think About It\n",
    "What makes a review positive vs negative? The words used, right? That's why embeddings are perfect - they capture word meaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b50b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all necessary libraries\n",
    "# Import torch, numpy, pandas, matplotlib, sklearn utilities, and re\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ad45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check device availability\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load IMDB movie review dataset\n",
    "# Load data from 'IMDB Dataset.csv'\n",
    "# Convert sentiment labels: 'positive' -> 1, 'negative' -> 0\n",
    "# Your code here\n",
    "\n",
    "# Example structure:\n",
    "# df = pd.read_csv('IMDB Dataset.csv')\n",
    "# reviews = df['review'].tolist()\n",
    "# labels = df['sentiment'].map({'positive': 1, 'negative': 0}).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print dataset statistics\n",
    "# Print total number of reviews, number of positive, number of negative\n",
    "# Print a few example reviews from the dataset\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a15ae5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 2: Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4342bf",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Clean and prepare the text data for our model.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Create a preprocessing function** that:\n",
    "   - Converts text to lowercase\n",
    "   - Removes punctuation (except keeping spaces)\n",
    "   - Removes extra whitespace\n",
    "   - Splits into words (tokens)\n",
    "   - Returns a list of clean tokens\n",
    "\n",
    "2. **Process all reviews**:\n",
    "   - Apply your preprocessing function to each review\n",
    "   - Store the tokenized reviews in a list\n",
    "\n",
    "3. **Build vocabulary**:\n",
    "   - Collect all unique words from all reviews\n",
    "   - Create `word_to_idx` and `idx_to_word` dictionaries\n",
    "   - Calculate vocabulary size\n",
    "\n",
    "### üí° Hints\n",
    "- Use `text.lower()` for lowercase\n",
    "- Use `re.sub(r'[^a-z\\s]', '', text)` to remove punctuation\n",
    "- Use `text.split()` to tokenize\n",
    "- Get unique words: `set()` all tokens from all reviews\n",
    "- Sort vocabulary for consistency: `sorted(vocab_set)`\n",
    "\n",
    "### ü§î Think About It\n",
    "Why preprocess? Because \"Movie\", \"movie\", and \"movie!\" should all be treated as the same word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create preprocessing function\n",
    "# Your code here\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"\n",
    "#     Clean and tokenize text\n",
    "#     Args: text (string)\n",
    "#     Returns: list of tokens\n",
    "#     \"\"\"\n",
    "#     # Your implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Process all reviews\n",
    "# Apply preprocessing to each review and store in a list\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build vocabulary\n",
    "# Create word_to_idx and idx_to_word dictionaries\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86967ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print vocabulary statistics\n",
    "# Print vocabulary size, show sample words and their indices\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad5a12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 3: Loading Pre-trained GloVe Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46f32d",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Load pre-trained word embeddings that were trained on billions of words!\n",
    "\n",
    "### üìù About GloVe Embeddings\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)** is similar to Word2Vec but trained differently:\n",
    "- Trained on billions of tokens from Wikipedia + Gigaword corpus\n",
    "- Contains hundreds of thousands of words\n",
    "- Each word is represented by 50 numbers (50-dimensional vectors)\n",
    "- File format: Each line is: `word number1 number2 ... number50`\n",
    "\n",
    "**Why use pre-trained embeddings?**\n",
    "- ‚úÖ Trained on massive datasets\n",
    "- ‚úÖ Captures rich semantic relationships\n",
    "- ‚úÖ Works well even with limited training data\n",
    "- ‚úÖ Saves training time\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Load GloVe embeddings from file**:\n",
    "   - File location: `wiki_giga_2024_50_MFT20_vectors_seed_123_alpha_0.75_eta_0.075_combined.txt`\n",
    "   - Read line by line\n",
    "   - Each line: first item is word, rest are 50 numbers\n",
    "   - Store in a dictionary: `{word: numpy_array_of_50_numbers}`\n",
    "\n",
    "2. **Create embedding matrix for YOUR vocabulary**:\n",
    "   - For each word in your vocabulary:\n",
    "     - If word exists in GloVe: use GloVe vector\n",
    "     - If word NOT in GloVe: use random vector\n",
    "   - Create a matrix of shape: `(vocab_size, 50)`\n",
    "   - Row i = embedding for word with index i\n",
    "\n",
    "3. **Print statistics**:\n",
    "   - How many GloVe vectors loaded?\n",
    "   - How many of your words have GloVe embeddings?\n",
    "   - How many words need random initialization?\n",
    "\n",
    "### üí° Hints\n",
    "```python\n",
    "# Loading GloVe\n",
    "glove_embeddings = {}\n",
    "with open('wiki_giga_2024_50_MFT20_vectors_seed_123_alpha_0.75_eta_0.075_combined.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "```\n",
    "\n",
    "- Embedding dimension: 50\n",
    "- Random initialization: `np.random.randn(50) * 0.01`\n",
    "- Convert to float32 for PyTorch compatibility\n",
    "\n",
    "### ü§î Think About It\n",
    "GloVe knows that \"excellent\" and \"fantastic\" are similar because it was trained on billions of words. These pre-trained embeddings give us powerful semantic representations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load GloVe embeddings from file\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create embedding matrix for your vocabulary\n",
    "# For each word in word_to_idx, get its GloVe embedding or create random vector\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18508dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print embedding statistics\n",
    "# How many words have GloVe embeddings vs random initialization?\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150981a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 4: Preparing Data for Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d52695",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Convert reviews to fixed-length sequences and split into train/test sets.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Convert reviews to index sequences**:\n",
    "   - For each tokenized review, convert words to indices using `word_to_idx`\n",
    "   - Handle different length reviews by:\n",
    "     - **Padding**: If review is shorter than max_length, add 0s at the end\n",
    "     - **Truncating**: If review is longer than max_length, cut it off\n",
    "   - Choose a reasonable `max_length` (e.g., 200 or 300 for IMDB reviews)\n",
    "\n",
    "2. **Create PyTorch tensors**:\n",
    "   - Convert padded sequences to torch.LongTensor\n",
    "   - Convert labels to torch.LongTensor\n",
    "\n",
    "3. **Split into train and test sets**:\n",
    "   - Use 80% for training, 20% for testing\n",
    "   - Use `train_test_split` from sklearn\n",
    "   - Set `random_state=42` for reproducibility\n",
    "\n",
    "4. **Print dataset information**:\n",
    "   - Shape of train/test data\n",
    "   - Number of train/test samples\n",
    "   - Show example of padded sequence\n",
    "\n",
    "### üí° Hints\n",
    "```python\n",
    "# Padding example\n",
    "def pad_sequence(sequence, max_length):\n",
    "    if len(sequence) < max_length:\n",
    "        # Add zeros at the end\n",
    "        sequence = sequence + [0] * (max_length - len(sequence))\n",
    "    else:\n",
    "        # Cut off at max_length\n",
    "        sequence = sequence[:max_length]\n",
    "    return sequence\n",
    "```\n",
    "\n",
    "- Use 0 as padding index\n",
    "- `train_test_split(X, y, test_size=0.2, random_state=42)`\n",
    "- IMDB reviews tend to be longer, so consider max_length of 200-300\n",
    "\n",
    "### ü§î Think About It\n",
    "Why fixed length? Neural networks need consistent input shapes. Padding lets us keep all reviews without losing information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3656ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define max_length and create padding function\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf55ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert all reviews to padded index sequences\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53588844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert to PyTorch tensors\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28250f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split into train and test sets\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print dataset information\n",
    "# Print shapes, sizes, show example\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677824a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 5: Building the Sentiment Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6fb82",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Create a neural network that uses embeddings to classify sentiment.\n",
    "\n",
    "### üìù Architecture Overview\n",
    "\n",
    "Our model will have these layers:\n",
    "\n",
    "```\n",
    "Input: Review as indices [batch_size, max_length]\n",
    "   ‚Üì\n",
    "Embedding Layer: Lookup word vectors [batch_size, max_length, 50]\n",
    "   ‚Üì\n",
    "Average Pooling: Take mean across words [batch_size, 50]\n",
    "   ‚Üì\n",
    "Fully Connected Layer 1: [batch_size, 128]\n",
    "   ‚Üì\n",
    "ReLU Activation\n",
    "   ‚Üì\n",
    "Dropout (0.3)\n",
    "   ‚Üì\n",
    "Fully Connected Layer 2: [batch_size, 2]\n",
    "   ‚Üì\n",
    "Output: Logits for [negative, positive]\n",
    "```\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "Create a class `SentimentClassifier` that inherits from `nn.Module`:\n",
    "\n",
    "1. **`__init__` method** - Initialize layers:\n",
    "   - `self.embedding`: Use `nn.Embedding.from_pretrained()`\n",
    "     - Pass your embedding matrix (convert to torch tensor)\n",
    "     - Set `freeze=False` to allow fine-tuning\n",
    "     - Set `padding_idx=0`\n",
    "   - `self.fc1`: Linear layer (50 ‚Üí 128)\n",
    "   - `self.fc2`: Linear layer (128 ‚Üí 2)\n",
    "   - `self.dropout`: Dropout(0.3)\n",
    "   - `self.relu`: ReLU activation\n",
    "\n",
    "2. **`forward` method**:\n",
    "   - Input: `x` of shape `(batch_size, max_length)`\n",
    "   - Steps:\n",
    "     1. Get embeddings: `embedded = self.embedding(x)`  # (batch, max_length, 50)\n",
    "     2. Average pool: `pooled = torch.mean(embedded, dim=1)`  # (batch, 50)\n",
    "     3. First layer: `x = self.relu(self.fc1(pooled))`\n",
    "     4. Dropout: `x = self.dropout(x)`\n",
    "     5. Output layer: `out = self.fc2(x)`  # (batch, 2)\n",
    "   - Return: `out`\n",
    "\n",
    "3. **Instantiate the model**:\n",
    "   - Create model and move to device\n",
    "   - Print model architecture\n",
    "   - Count parameters\n",
    "\n",
    "### üí° Hints\n",
    "- Convert numpy array to tensor: `torch.FloatTensor(embedding_matrix)`\n",
    "- `nn.Embedding.from_pretrained(embeddings, freeze=False, padding_idx=0)`\n",
    "- Average pooling across dimension 1: `torch.mean(x, dim=1)`\n",
    "- Model to device: `model.to(device)`\n",
    "\n",
    "### ü§î Key Concepts\n",
    "\n",
    "**Why average pooling?**\n",
    "- We have embeddings for each word: `[word1_vec, word2_vec, ..., wordN_vec]`\n",
    "- We need ONE vector for the entire review\n",
    "- Average pooling: Take the mean of all word vectors\n",
    "- Result: A single 50-dimensional vector representing the review\n",
    "\n",
    "**Why freeze=False?**\n",
    "- Starts with GloVe embeddings (good general knowledge)\n",
    "- Fine-tunes them for sentiment analysis specifically\n",
    "- Best of both worlds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create SentimentClassifier class\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate the model and print architecture\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b9405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 6: Training Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7f485",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Set up loss function, optimizer, and evaluation metrics.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Define loss function**:\n",
    "   - Use `nn.CrossEntropyLoss()`\n",
    "   - This is perfect for binary classification\n",
    "\n",
    "2. **Define optimizer**:\n",
    "   - Use `torch.optim.Adam(model.parameters(), lr=0.001)`\n",
    "   - Learning rate: 0.001 is a good starting point\n",
    "\n",
    "3. **Print training configuration**:\n",
    "   - Loss function\n",
    "   - Optimizer\n",
    "   - Learning rate\n",
    "   - Number of epochs you plan to train\n",
    "   - Device being used\n",
    "\n",
    "### üí° Hints\n",
    "- `criterion = nn.CrossEntropyLoss()`\n",
    "- `optimizer = torch.optim.Adam(...)`\n",
    "- Recommended epochs: 3-5 (since IMDB dataset is large, training takes longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define loss function and optimizer\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print training configuration\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5a861",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 7: The Training Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee1e67",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Train your sentiment classifier and track performance.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "Create a training function with this structure:\n",
    "\n",
    "1. **Function signature**: `train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, num_epochs)`\n",
    "\n",
    "2. **Training loop**:\n",
    "   ```\n",
    "   for each epoch:\n",
    "       # Training phase\n",
    "       model.train()\n",
    "       1. Move data to device\n",
    "       2. Zero gradients\n",
    "       3. Forward pass\n",
    "       4. Calculate loss\n",
    "       5. Backward pass\n",
    "       6. Update weights\n",
    "       7. Calculate accuracy\n",
    "       \n",
    "       # Validation phase\n",
    "       model.eval()\n",
    "       with torch.no_grad():\n",
    "           1. Forward pass on test data\n",
    "           2. Calculate test loss\n",
    "           3. Calculate test accuracy\n",
    "   ```\n",
    "\n",
    "3. **Track metrics**:\n",
    "   - Store train loss, train accuracy per epoch\n",
    "   - Store test loss, test accuracy per epoch\n",
    "   - Print progress every epoch\n",
    "\n",
    "4. **Return**:\n",
    "   - Dictionary with loss and accuracy histories\n",
    "\n",
    "5. **Train the model**:\n",
    "   - Call your training function\n",
    "   - Use 3-5 epochs (IMDB is large, so fewer epochs needed)\n",
    "\n",
    "### üí° Hints\n",
    "```python\n",
    "# Calculate accuracy\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "correct = (predicted == labels).sum().item()\n",
    "accuracy = correct / labels.size(0)\n",
    "```\n",
    "\n",
    "- Use `model.train()` before training\n",
    "- Use `model.eval()` before testing\n",
    "- Use `torch.no_grad()` during validation\n",
    "- Move data: `X.to(device)`, `y.to(device)`\n",
    "\n",
    "### ü§î What to Expect\n",
    "- Training accuracy should increase over epochs\n",
    "- Test accuracy should also increase\n",
    "- With the full IMDB dataset, you can expect 85-90% accuracy or higher\n",
    "- Each epoch will take some time due to dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ce7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create training function\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model\n",
    "# Call your training function with 3-5 epochs\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db86e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 8: Visualizing Training Progress**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f1301",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Plot training curves to understand model performance.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Plot Loss Curves**:\n",
    "   - Create a figure with training loss and test loss\n",
    "   - X-axis: Epochs\n",
    "   - Y-axis: Loss\n",
    "   - Use different colors for train vs test\n",
    "   - Add legend, labels, title, grid\n",
    "\n",
    "2. **Plot Accuracy Curves**:\n",
    "   - Create a figure with training accuracy and test accuracy\n",
    "   - X-axis: Epochs\n",
    "   - Y-axis: Accuracy\n",
    "   - Use different colors for train vs test\n",
    "   - Add legend, labels, title, grid\n",
    "\n",
    "3. **Print final metrics**:\n",
    "   - Final training accuracy\n",
    "   - Final test accuracy\n",
    "   - Best test accuracy achieved\n",
    "\n",
    "### üí° Hints\n",
    "```python\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, test_losses, label='Test Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "# ... similar for accuracy\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### ü§î What to Look For\n",
    "- **Loss should decrease** over time\n",
    "- **Accuracy should increase** over time\n",
    "- **Gap between train and test**: Small gap = good, large gap = overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training curves\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bff5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print final metrics\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37e9f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 9: Detailed Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c6292",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Evaluate model performance with detailed metrics.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Get predictions on test set**:\n",
    "   - Set model to eval mode\n",
    "   - Get predictions for all test samples\n",
    "   - Convert predictions and labels to numpy arrays\n",
    "\n",
    "2. **Print classification report**:\n",
    "   - Use `classification_report` from sklearn\n",
    "   - Shows precision, recall, F1-score for each class\n",
    "   - Target names: ['Negative', 'Positive']\n",
    "\n",
    "3. **Create confusion matrix**:\n",
    "   - Use `confusion_matrix` from sklearn\n",
    "   - Visualize as a heatmap\n",
    "   - Show how many predictions were correct/incorrect\n",
    "\n",
    "4. **Analyze errors**:\n",
    "   - Find misclassified reviews\n",
    "   - Print a few examples\n",
    "   - Try to understand why the model made mistakes\n",
    "\n",
    "### üí° Hints\n",
    "```python\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test.to(device))\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test.cpu().numpy()\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "```\n",
    "\n",
    "### ü§î Understanding Metrics\n",
    "- **Precision**: Of all predicted positive, how many were actually positive?\n",
    "- **Recall**: Of all actual positives, how many did we find?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: Shows where the model confuses classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90050f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get predictions on test set\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print classification report\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78441e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and visualize confusion matrix\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98180ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze errors - find and print misclassified reviews\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91758439",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 10: Testing on New Reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c8874",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Create a function to predict sentiment for any new movie review.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Create prediction function**:\n",
    "   - Function signature: `predict_sentiment(review_text, model, word_to_idx, max_length)`\n",
    "   - Steps:\n",
    "     1. Preprocess the review text\n",
    "     2. Convert to indices\n",
    "     3. Pad/truncate to max_length\n",
    "     4. Convert to tensor\n",
    "     5. Get model prediction\n",
    "     6. Return predicted class and probability\n",
    "\n",
    "2. **Test on new reviews**:\n",
    "   - Create 5-10 new movie reviews (not in training data)\n",
    "   - Mix of positive and negative\n",
    "   - Use your prediction function\n",
    "   - Print review, prediction, and confidence\n",
    "\n",
    "3. **Interactive testing** (optional):\n",
    "   - Allow user to input their own review\n",
    "   - Predict and display sentiment\n",
    "\n",
    "### üí° Hints\n",
    "```python\n",
    "def predict_sentiment(review_text, model, word_to_idx, max_length):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess\n",
    "    tokens = preprocess_text(review_text)\n",
    "    \n",
    "    # Convert to indices\n",
    "    indices = [word_to_idx.get(token, 0) for token in tokens]  # 0 for unknown words\n",
    "    \n",
    "    # Pad\n",
    "    indices = pad_sequence(indices, max_length)\n",
    "    \n",
    "    # To tensor\n",
    "    x = torch.LongTensor([indices]).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        predicted = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][predicted].item()\n",
    "    \n",
    "    sentiment = \"Positive üòä\" if predicted == 1 else \"Negative üòû\"\n",
    "    return sentiment, confidence\n",
    "```\n",
    "\n",
    "### ü§î Think About It\n",
    "Does your model work well on new reviews? Can it handle different writing styles? What kinds of reviews does it struggle with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38240054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create prediction function\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test on new reviews\n",
    "# Create list of new reviews and test them\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a188441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Interactive testing (optional)\n",
    "# Allow user to input their own review\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find similar words for sentiment keywords\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f19a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize embeddings in 2D\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fcfb62",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 12: Saving Your Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487a6bb",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Save the trained model for future use.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Create checkpoint dictionary**:\n",
    "   - Model state dict\n",
    "   - Optimizer state dict\n",
    "   - Vocabulary mappings\n",
    "   - Max length\n",
    "   - Training history\n",
    "   - Any other important information\n",
    "\n",
    "2. **Save using torch.save()**:\n",
    "   - Filename: 'sentiment_classifier.pth'\n",
    "\n",
    "3. **Create a function to load the model**:\n",
    "   - Load checkpoint\n",
    "   - Recreate model\n",
    "   - Load weights\n",
    "   - Return ready-to-use model\n",
    "\n",
    "### üí° Hints\n",
    "```python\n",
    "# Save\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'word_to_idx': word_to_idx,\n",
    "    'idx_to_word': idx_to_word,\n",
    "    'max_length': max_length,\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': 50,\n",
    "    'history': history\n",
    "}\n",
    "torch.save(checkpoint, 'sentiment_classifier.pth')\n",
    "\n",
    "# Load\n",
    "checkpoint = torch.load('sentiment_classifier.pth')\n",
    "model = SentimentClassifier(embedding_matrix)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the model\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create function to load the model\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
