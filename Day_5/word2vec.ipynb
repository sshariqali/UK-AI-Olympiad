{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046afb1a",
   "metadata": {},
   "source": [
    "# **Day 5: Word2Vec - Teaching Machines to Understand Language**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1149d2d",
   "metadata": {},
   "source": [
    "### **1. Introduction: The Challenge of Understanding Language**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a09cf",
   "metadata": {},
   "source": [
    "**The Problem:**\n",
    "\n",
    "So far, we've worked with numerical data (exam scores) and images (pixels). But how do we make machines understand **words** and **language**?\n",
    "\n",
    "Words aren't numbers. We can't just feed \"cat\" or \"king\" into a neural network. Computers only understand numbers, so we need a way to represent words mathematically.\n",
    "\n",
    "**The Evolution of Word Representation:**\n",
    "\n",
    "Humans understand that:\n",
    "- \"king\" and \"queen\" are related (both royalty)\n",
    "- \"cat\" and \"dog\" are similar (both animals, both pets)\n",
    "- \"happy\" and \"joyful\" have similar meanings\n",
    "\n",
    "But how do we teach machines these relationships?\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*sXNXYfAqfLUeiDXPCo130w.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Our Goal Today:**\n",
    "\n",
    "Build a system that learns word meanings from text and represents them as vectors (lists of numbers) such that:\n",
    "- Similar words have similar vectors\n",
    "- We can perform mathematical operations on meaning (e.g., king - man + woman ≈ queen)\n",
    "- The machine discovers these relationships automatically from data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f339b",
   "metadata": {},
   "source": [
    "### **2. From One-Hot Encoding to Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11574bec",
   "metadata": {},
   "source": [
    "Let's explore two fundamental ways to represent words as numbers.\n",
    "\n",
    "**Method 1: One-Hot Encoding (The Naive Approach)**\n",
    "\n",
    "Imagine we have a vocabulary of 5 words: `[\"cat\", \"dog\", \"king\", \"queen\", \"happy\"]`\n",
    "\n",
    "One-hot encoding represents each word as a vector with:\n",
    "- A `1` at the word's position\n",
    "- `0` everywhere else\n",
    "\n",
    "```\n",
    "cat   = [1, 0, 0, 0, 0]\n",
    "dog   = [0, 1, 0, 0, 0]\n",
    "king  = [0, 0, 1, 0, 0]\n",
    "queen = [0, 0, 0, 1, 0]\n",
    "happy = [0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "**Problems with One-Hot Encoding:**\n",
    "\n",
    "1. **No Semantic Meaning:** All words are equally different from each other\n",
    "   - Distance between \"cat\" and \"dog\" = Distance between \"cat\" and \"king\"\n",
    "   - The model can't learn that some words are more similar than others\n",
    "\n",
    "2. **Huge Dimensions:** With 100,000 words in a vocabulary, each word is a 100,000-dimensional vector!\n",
    "   - Wastes memory and computation\n",
    "   - 99.999% of the values are zeros (sparse representation)\n",
    "\n",
    "3. **Can't Generalize:** If the model sees \"cat\" during training but not \"dog\", it has no way to know they're related\n",
    "\n",
    "**Method 2: Word Embeddings (The Smart Approach)**\n",
    "\n",
    "Word embeddings represent each word as a **dense vector** of real numbers (typically 50-300 dimensions):\n",
    "\n",
    "```\n",
    "cat   = [0.2,  0.8, -0.3,  0.1, ...]  (50-300 numbers)\n",
    "dog   = [0.3,  0.7, -0.2,  0.2, ...]  (similar to cat!)\n",
    "king  = [0.9, -0.1,  0.4,  0.8, ...]\n",
    "queen = [0.8, -0.2,  0.3,  0.9, ...]  (similar to king!)\n",
    "```\n",
    "\n",
    "**Advantages of Word Embeddings:**\n",
    "\n",
    "1. **Captures Semantic Meaning:** Similar words have similar vectors\n",
    "2. **Compact:** 300 dimensions instead of 100,000\n",
    "3. **Learned from Data:** The model discovers relationships automatically\n",
    "4. **Generalizable:** Understanding of \"cat\" helps with \"dog\"\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*OEmWDt4eztOcm5pr2QbxfA.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c224105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4421563",
   "metadata": {},
   "source": [
    "**Demonstration: One-Hot vs Embeddings**\n",
    "\n",
    "Let's create a simple example to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vocabulary\n",
    "vocab = [\"cat\", \"dog\", \"king\", \"queen\", \"happy\"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create word to index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(word_to_idx)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Method 1: One-hot encoding\n",
    "def one_hot_encode(word, word_to_idx, vocab_size):\n",
    "    \"\"\"Convert a word to one-hot vector\"\"\"\n",
    "    vector = np.zeros(vocab_size)\n",
    "    vector[word_to_idx[word]] = 1\n",
    "    return vector\n",
    "\n",
    "print(\"\\nOne-Hot Encoding:\")\n",
    "print(\"=\"*60)\n",
    "for word in vocab:\n",
    "    one_hot = one_hot_encode(word, word_to_idx, vocab_size)\n",
    "    print(f\"{word:8s}: {one_hot}\")\n",
    "\n",
    "# Calculate similarity between one-hot vectors\n",
    "cat_onehot = one_hot_encode(\"cat\", word_to_idx, vocab_size)\n",
    "dog_onehot = one_hot_encode(\"dog\", word_to_idx, vocab_size)\n",
    "king_onehot = one_hot_encode(\"king\", word_to_idx, vocab_size)\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "print(\"\\nOne-Hot Similarities:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"cat ↔ dog:  {cosine_similarity(cat_onehot, dog_onehot):.4f}\")\n",
    "print(f\"cat ↔ king: {cosine_similarity(cat_onehot, king_onehot):.4f}\")\n",
    "print(\"\\n⚠️  All words are equally dissimilar (0.0) - No semantic meaning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Dense embeddings (example - these would be learned)\n",
    "# Let's create hypothetical learned embeddings to demonstrate the concept\n",
    "print(\"\\nDense Word Embeddings (Example - would be learned):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hypothetical 5-dimensional embeddings\n",
    "# In reality, we'd have 50-300 dimensions, but 5 is easier to visualize\n",
    "embeddings_example = {\n",
    "    \"cat\":   np.array([0.8,  0.7, -0.2,  0.1,  0.3]),  # Animals, pets\n",
    "    \"dog\":   np.array([0.7,  0.8, -0.1,  0.2,  0.4]),  # Similar to cat\n",
    "    \"king\":  np.array([0.1, -0.2,  0.9,  0.8,  0.1]),  # Royalty, male\n",
    "    \"queen\": np.array([0.0, -0.1,  0.9,  0.7,  0.2]),  # Royalty, female\n",
    "    \"happy\": np.array([0.2,  0.1,  0.1,  0.1,  0.9]),  # Emotion\n",
    "}\n",
    "\n",
    "for word, embedding in embeddings_example.items():\n",
    "    print(f\"{word:8s}: [{', '.join([f'{x:5.2f}' for x in embedding])}]\")\n",
    "\n",
    "print(\"\\nEmbedding Similarities:\")\n",
    "print(\"=\"*60)\n",
    "cat_emb = embeddings_example[\"cat\"]\n",
    "dog_emb = embeddings_example[\"dog\"]\n",
    "king_emb = embeddings_example[\"king\"]\n",
    "\n",
    "print(f\"cat ↔ dog:  {cosine_similarity(cat_emb, dog_emb):.4f} (High - both animals!)\")\n",
    "print(f\"cat ↔ king: {cosine_similarity(cat_emb, king_emb):.4f} (Low - different concepts)\")\n",
    "print(f\"king ↔ queen: {cosine_similarity(king_emb, embeddings_example['queen']):.4f} (High - both royalty!)\")\n",
    "print(\"\\n✅ Embeddings capture semantic relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d4fdd",
   "metadata": {},
   "source": [
    "### **3. The Distributional Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e2b31",
   "metadata": {},
   "source": [
    "**\"You shall know a word by the company it keeps\"** - J.R. Firth (1957)\n",
    "\n",
    "This is the fundamental idea behind Word2Vec. The meaning of a word is determined by the words that frequently appear near it.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1. **\"The cat sat on the mat\"**\n",
    "   - Words near \"cat\": the, sat, on\n",
    "\n",
    "2. **\"The dog sat on the rug\"**\n",
    "   - Words near \"dog\": the, sat, on\n",
    "\n",
    "Notice that \"cat\" and \"dog\" appear in similar contexts! This is how the model learns they're related.\n",
    "\n",
    "**More Examples:**\n",
    "\n",
    "```\n",
    "\"The king ruled the kingdom\"\n",
    "\"The queen ruled the empire\"\n",
    "```\n",
    "→ \"king\" and \"queen\" appear in similar contexts (both rule)\n",
    "\n",
    "```\n",
    "\"I am very happy today\"\n",
    "\"I am very joyful today\"\n",
    "```\n",
    "→ \"happy\" and \"joyful\" are interchangeable (similar meaning)\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "If we can build a model that:\n",
    "1. Looks at what words appear near each other\n",
    "2. Learns to predict context words from target words (or vice versa)\n",
    "3. Adjusts word vectors to make these predictions accurate\n",
    "\n",
    "Then the learned vectors will automatically capture semantic meaning!\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*SR6l59udY05_bUICAjb6-w.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6a966",
   "metadata": {},
   "source": [
    "### **4. Word2Vec: Two Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9042af8",
   "metadata": {},
   "source": [
    "Word2Vec has two main architectures:\n",
    "\n",
    "**1. CBOW (Continuous Bag of Words):**\n",
    "- **Input:** Context words (surrounding words)\n",
    "- **Output:** Target word (center word)\n",
    "- **Task:** Predict the center word from its context\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "Context: [\"The\", \"cat\", \"on\", \"the\"] → Target: \"sat\"\n",
    "```\n",
    "\n",
    "**2. Skip-Gram (Our Focus Today):**\n",
    "- **Input:** Target word (center word)\n",
    "- **Output:** Context words (surrounding words)\n",
    "- **Task:** Predict context words from the center word\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "Target: \"sat\" → Context: [\"cat\", \"on\"] (window size = 1)\n",
    "Target: \"sat\" → Context: [\"The\", \"cat\", \"on\", \"the\"] (window size = 2)\n",
    "```\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*cuOmGT7NevP9oJFJfVpRKA.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "**Why Skip-Gram?**\n",
    "\n",
    "1. **Better for rare words:** Learns better representations for infrequent words\n",
    "2. **More training examples:** One target word creates multiple (target, context) pairs\n",
    "3. **Better performance:** Generally produces higher quality embeddings\n",
    "\n",
    "**The Skip-Gram Process:**\n",
    "\n",
    "```\n",
    "Sentence: \"The quick brown fox jumps\"\n",
    "Window size: 2\n",
    "\n",
    "For target word \"brown\" (index 2):\n",
    "  - Context word 1: \"The\" (2 positions left)\n",
    "  - Context word 2: \"quick\" (1 position left)\n",
    "  - Context word 3: \"fox\" (1 position right)\n",
    "  - Context word 4: \"jumps\" (2 positions right)\n",
    "\n",
    "Training pairs created:\n",
    "  (brown, The)\n",
    "  (brown, quick)\n",
    "  (brown, fox)\n",
    "  (brown, jumps)\n",
    "```\n",
    "\n",
    "This transforms unsupervised text into supervised learning pairs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2057f",
   "metadata": {},
   "source": [
    "### **5. Understanding the Skip-Gram Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87a743",
   "metadata": {},
   "source": [
    "The Skip-Gram model is surprisingly simple - just two layers:\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "```\n",
    "Input: Target word index (scalar)\n",
    "   ↓\n",
    "Embedding Layer: vocab_size × embedding_dim\n",
    "   ↓ (This is what we want! The learned word vectors)\n",
    "Dense word vector (embedding_dim dimensions)\n",
    "   ↓\n",
    "Linear Layer: embedding_dim × vocab_size\n",
    "   ↓\n",
    "Output: Scores for all words (vocab_size)\n",
    "   ↓\n",
    "Softmax: Convert to probabilities\n",
    "   ↓\n",
    "Predicted context word\n",
    "```\n",
    "\n",
    "**Layer Details:**\n",
    "\n",
    "1. **Embedding Layer (`nn.Embedding`):**\n",
    "   - A lookup table that converts word indices to dense vectors\n",
    "   - Shape: `(vocab_size, embedding_dim)`\n",
    "   - Example: If vocab_size=10,000 and embedding_dim=100:\n",
    "     - This is a 10,000 × 100 matrix\n",
    "     - Each row is the embedding vector for one word\n",
    "   - **This layer's weights ARE the word embeddings we want to learn!**\n",
    "\n",
    "2. **Linear Layer (`nn.Linear`):**\n",
    "   - Maps from embedding space back to vocabulary space\n",
    "   - Shape: `(embedding_dim, vocab_size)`\n",
    "   - Produces a score for each possible context word\n",
    "\n",
    "**Example with Concrete Numbers:**\n",
    "\n",
    "```python\n",
    "# Suppose:\n",
    "vocab_size = 10,000 words\n",
    "embedding_dim = 100\n",
    "target_word_idx = 542  # Index for \"cat\"\n",
    "\n",
    "# Step 1: Embedding lookup\n",
    "embedding_layer = nn.Embedding(10000, 100)\n",
    "word_vector = embedding_layer(torch.tensor([542]))\n",
    "# Result: [100] dimensional vector (the embedding for \"cat\")\n",
    "\n",
    "# Step 2: Linear transformation\n",
    "linear_layer = nn.Linear(100, 10000)\n",
    "scores = linear_layer(word_vector)\n",
    "# Result: [10000] scores, one for each word in vocabulary\n",
    "\n",
    "# Step 3: Softmax (done by loss function)\n",
    "# Converts scores to probabilities\n",
    "# Model predicts which word is likely to be in context\n",
    "```\n",
    "\n",
    "**Training Objective:**\n",
    "\n",
    "Given a (target, context) pair like `(\"cat\", \"sat\")`:\n",
    "1. Feed \"cat\" index into the network\n",
    "2. Get probability distribution over all vocabulary words\n",
    "3. We want high probability for \"sat\" (the actual context word)\n",
    "4. Use Cross-Entropy Loss to measure error\n",
    "5. Backpropagation adjusts the embedding weights\n",
    "\n",
    "After many training examples, words that appear in similar contexts will have similar embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209630eb",
   "metadata": {},
   "source": [
    "### **6. Preparing Our Text Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0b43c",
   "metadata": {},
   "source": [
    "Let's start our implementation! We'll use a sample text corpus to train our Word2Vec model.\n",
    "\n",
    "**The Workflow:**\n",
    "1. **Get text data** - A collection of sentences\n",
    "2. **Preprocess** - Clean and tokenize the text\n",
    "3. **Build vocabulary** - Create word ↔ index mappings\n",
    "4. **Generate training pairs** - Create (target, context) pairs\n",
    "5. **Train model** - Learn the embeddings\n",
    "6. **Extract embeddings** - Get the learned word vectors\n",
    "\n",
    "For this tutorial, we'll use a small corpus of sentences. In practice, you'd use much larger datasets (Wikipedia, books, news articles, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2de6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus\n",
    "# In practice, you'd load this from a file or dataset\n",
    "corpus = \"\"\"\n",
    "The cat sat on the mat.\n",
    "The dog sat on the log.\n",
    "Cats and dogs are animals.\n",
    "The cat and the dog are friends.\n",
    "A king rules a kingdom.\n",
    "A queen rules an empire.\n",
    "The king and queen are royalty.\n",
    "The happy cat played with the ball.\n",
    "The joyful dog ran in the park.\n",
    "Animals are happy when they play.\n",
    "The cat sleeps on the warm mat.\n",
    "The dog loves to run and jump.\n",
    "Kings and queens live in castles.\n",
    "Happy animals are healthy animals.\n",
    "The small cat climbed the tall tree.\n",
    "The big dog guarded the house.\n",
    "Cats like to chase mice.\n",
    "Dogs like to catch balls.\n",
    "The wise king made fair decisions.\n",
    "The kind queen helped her people.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Text Corpus:\")\n",
    "print(\"=\"*70)\n",
    "print(corpus.strip())\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCorpus length: {len(corpus)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b32f7e",
   "metadata": {},
   "source": [
    "### **7. Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac78a37",
   "metadata": {},
   "source": [
    "Before we can work with text, we need to preprocess it:\n",
    "\n",
    "**Preprocessing Steps:**\n",
    "\n",
    "1. **Lowercase:** Convert all text to lowercase\n",
    "   - \"The Cat\" → \"the cat\"\n",
    "   - Ensures \"Cat\" and \"cat\" are treated as the same word\n",
    "\n",
    "2. **Remove Punctuation:** Remove periods, commas, etc.\n",
    "   - \"cat.\" → \"cat\"\n",
    "   - Simplifies tokenization\n",
    "\n",
    "3. **Tokenization:** Split text into individual words\n",
    "   - \"the cat sat\" → [\"the\", \"cat\", \"sat\"]\n",
    "   - Each word becomes a token\n",
    "\n",
    "4. **Build Vocabulary:** Create mappings between words and indices\n",
    "   - word_to_idx: {\"cat\": 0, \"dog\": 1, ...}\n",
    "   - idx_to_word: {0: \"cat\", 1: \"dog\", ...}\n",
    "\n",
    "These mappings allow us to convert between words and the numerical indices our neural network needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a78196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: lowercase, remove punctuation, tokenize\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens (words)\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Split into words (tokenization)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Preprocess our corpus\n",
    "tokens = preprocess_text(corpus)\n",
    "\n",
    "print(\"Tokenized Text (first 50 tokens):\")\n",
    "print(\"=\"*70)\n",
    "print(tokens[:50])\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df526efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, min_count=1):\n",
    "    \"\"\"\n",
    "    Build vocabulary from tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of word tokens\n",
    "        min_count: Minimum frequency for a word to be included\n",
    "    \n",
    "    Returns:\n",
    "        word_to_idx: Dictionary mapping words to indices\n",
    "        idx_to_word: Dictionary mapping indices to words\n",
    "        word_counts: Counter object with word frequencies\n",
    "    \"\"\"\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(tokens)\n",
    "    \n",
    "    # Filter words by minimum count\n",
    "    vocab_words = [word for word, count in word_counts.items() if count >= min_count]\n",
    "    \n",
    "    # Sort for consistency\n",
    "    vocab_words = sorted(vocab_words)\n",
    "    \n",
    "    # Create mappings\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return word_to_idx, idx_to_word, word_counts\n",
    "\n",
    "# Build vocabulary\n",
    "word_to_idx, idx_to_word, word_counts = build_vocabulary(tokens, min_count=2)\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "print(\"Vocabulary Statistics:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Vocabulary size: {vocab_size} unique words\")\n",
    "print(f\"Total tokens: {len(tokens)} words\")\n",
    "print(\"\\nMost common words:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"  {word:15s}: {count:3d} occurrences\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of word-to-index mappings\n",
    "print(\"\\nSample Word-to-Index Mappings:\")\n",
    "print(\"=\"*70)\n",
    "sample_words = list(word_to_idx.items())[:15]\n",
    "for word, idx in sample_words:\n",
    "    print(f\"  '{word:10s}' → index {idx:3d}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n✅ Preprocessing complete!\")\n",
    "print(f\"   We now have {vocab_size} unique words in our vocabulary.\")\n",
    "print(f\"   Each word has a unique index from 0 to {vocab_size-1}.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
