{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df5c18a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "const firstCell = document.querySelector('.cell.code_cell');\n",
       "if (firstCell) {\n",
       "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
       "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
       "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
       "\n",
       "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
       "       alt=\"Algopath Coding Academy Logo\"\n",
       "       width=\"400\"\n",
       "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
       "\n",
       "  <p style=\"font-size:16px; margin:0;\">\n",
       "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
       "  </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1149d2d",
   "metadata": {},
   "source": [
    "### **1. Introduction: The Challenge of Understanding Language**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a09cf",
   "metadata": {},
   "source": [
    "**The Problem:**\n",
    "\n",
    "So far, we've worked with numerical data (exam scores) and images (pixels). But how do we make machines understand **words** and **language**?\n",
    "\n",
    "Words aren't numbers. We can't just feed \"cat\" or \"king\" into a neural network. Computers only understand numbers, so we need a way to represent words mathematically.\n",
    "\n",
    "**The Evolution of Word Representation:**\n",
    "\n",
    "Humans understand that:\n",
    "- \"king\" and \"queen\" are related (both royalty)\n",
    "- \"cat\" and \"dog\" are similar (both animals, both pets)\n",
    "- \"happy\" and \"joyful\" have similar meanings\n",
    "\n",
    "But how do we teach machines these relationships?\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*sXNXYfAqfLUeiDXPCo130w.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Our Goal Today:**\n",
    "\n",
    "Build a system that learns word meanings from text and represents them as vectors (lists of numbers) such that:\n",
    "- Similar words have similar vectors\n",
    "- We can perform mathematical operations on meaning (e.g., king - man + woman ‚âà queen)\n",
    "- The machine discovers these relationships automatically from data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f339b",
   "metadata": {},
   "source": [
    "### **2. From One-Hot Encoding to Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11574bec",
   "metadata": {},
   "source": [
    "Let's explore two fundamental ways to represent words as numbers.\n",
    "\n",
    "**Method 1: One-Hot Encoding (The Naive Approach)**\n",
    "\n",
    "Imagine we have a vocabulary of 5 words: `[\"cat\", \"dog\", \"king\", \"queen\", \"happy\"]`\n",
    "\n",
    "One-hot encoding represents each word as a vector with:\n",
    "- A `1` at the word's position\n",
    "- `0` everywhere else\n",
    "\n",
    "```\n",
    "cat   = [1, 0, 0, 0, 0]\n",
    "dog   = [0, 1, 0, 0, 0]\n",
    "king  = [0, 0, 1, 0, 0]\n",
    "queen = [0, 0, 0, 1, 0]\n",
    "happy = [0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "**Problems with One-Hot Encoding:**\n",
    "\n",
    "1. **No Semantic Meaning:** All words are equally different from each other\n",
    "   - Distance between \"cat\" and \"dog\" = Distance between \"cat\" and \"king\"\n",
    "   - The model can't learn that some words are more similar than others\n",
    "\n",
    "2. **Huge Dimensions:** With 100,000 words in a vocabulary, each word is a 100,000-dimensional vector!\n",
    "   - Wastes memory and computation\n",
    "   - 99.999% of the values are zeros (sparse representation)\n",
    "\n",
    "3. **Can't Generalize:** If the model sees \"cat\" during training but not \"dog\", it has no way to know they're related\n",
    "\n",
    "**Method 2: Word Embeddings (The Smart Approach)**\n",
    "\n",
    "Word embeddings represent each word as a **dense vector** of real numbers (typically 50-300 dimensions):\n",
    "\n",
    "```\n",
    "cat   = [0.2,  0.8, -0.3,  0.1, ...]  (50-300 numbers)\n",
    "dog   = [0.3,  0.7, -0.2,  0.2, ...]  (similar to cat!)\n",
    "king  = [0.9, -0.1,  0.4,  0.8, ...]\n",
    "queen = [0.8, -0.2,  0.3,  0.9, ...]  (similar to king!)\n",
    "```\n",
    "\n",
    "**Advantages of Word Embeddings:**\n",
    "\n",
    "1. **Captures Semantic Meaning:** Similar words have similar vectors\n",
    "2. **Compact:** 300 dimensions instead of 100,000\n",
    "3. **Learned from Data:** The model discovers relationships automatically\n",
    "4. **Generalizable:** Understanding of \"cat\" helps with \"dog\"\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*OEmWDt4eztOcm5pr2QbxfA.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c224105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4421563",
   "metadata": {},
   "source": [
    "**Demonstration: One-Hot vs Embeddings**\n",
    "\n",
    "Let's create a simple example to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vocabulary\n",
    "vocab = [\"cat\", \"dog\", \"king\", \"queen\", \"happy\"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create word to index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(word_to_idx)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Method 1: One-hot encoding\n",
    "def one_hot_encode(word, word_to_idx, vocab_size):\n",
    "    \"\"\"Convert a word to one-hot vector\"\"\"\n",
    "    vector = np.zeros(vocab_size)\n",
    "    vector[word_to_idx[word]] = 1\n",
    "    return vector\n",
    "\n",
    "print(\"\\nOne-Hot Encoding:\")\n",
    "print(\"=\"*60)\n",
    "for word in vocab:\n",
    "    one_hot = one_hot_encode(word, word_to_idx, vocab_size)\n",
    "    print(f\"{word:8s}: {one_hot}\")\n",
    "\n",
    "# Calculate similarity between one-hot vectors\n",
    "cat_onehot = one_hot_encode(\"cat\", word_to_idx, vocab_size)\n",
    "dog_onehot = one_hot_encode(\"dog\", word_to_idx, vocab_size)\n",
    "king_onehot = one_hot_encode(\"king\", word_to_idx, vocab_size)\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "print(\"\\nOne-Hot Similarities:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"cat ‚Üî dog:  {cosine_similarity(cat_onehot, dog_onehot):.4f}\")\n",
    "print(f\"cat ‚Üî king: {cosine_similarity(cat_onehot, king_onehot):.4f}\")\n",
    "print(\"\\n‚ö†Ô∏è  All words are equally dissimilar (0.0) - No semantic meaning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Dense embeddings (example - these would be learned)\n",
    "# Let's create hypothetical learned embeddings to demonstrate the concept\n",
    "print(\"\\nDense Word Embeddings (Example - would be learned):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Hypothetical 5-dimensional embeddings\n",
    "# In reality, we'd have 50-300 dimensions, but 5 is easier to visualize\n",
    "embeddings_example = {\n",
    "    \"cat\":   np.array([0.8,  0.7, -0.2,  0.1,  0.3]),  # Animals, pets\n",
    "    \"dog\":   np.array([0.7,  0.8, -0.1,  0.2,  0.4]),  # Similar to cat\n",
    "    \"king\":  np.array([0.1, -0.2,  0.9,  0.8,  0.1]),  # Royalty, male\n",
    "    \"queen\": np.array([0.0, -0.1,  0.9,  0.7,  0.2]),  # Royalty, female\n",
    "    \"happy\": np.array([0.2,  0.1,  0.1,  0.1,  0.9]),  # Emotion\n",
    "}\n",
    "\n",
    "for word, embedding in embeddings_example.items():\n",
    "    print(f\"{word:8s}: [{', '.join([f'{x:5.2f}' for x in embedding])}]\")\n",
    "\n",
    "print(\"\\nEmbedding Similarities:\")\n",
    "print(\"=\"*60)\n",
    "cat_emb = embeddings_example[\"cat\"]\n",
    "dog_emb = embeddings_example[\"dog\"]\n",
    "king_emb = embeddings_example[\"king\"]\n",
    "\n",
    "print(f\"cat ‚Üî dog:  {cosine_similarity(cat_emb, dog_emb):.4f} (High - both animals!)\")\n",
    "print(f\"cat ‚Üî king: {cosine_similarity(cat_emb, king_emb):.4f} (Low - different concepts)\")\n",
    "print(f\"king ‚Üî queen: {cosine_similarity(king_emb, embeddings_example['queen']):.4f} (High - both royalty!)\")\n",
    "print(\"\\n‚úÖ Embeddings capture semantic relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d4fdd",
   "metadata": {},
   "source": [
    "### **3. The Distributional Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e2b31",
   "metadata": {},
   "source": [
    "**\"You shall know a word by the company it keeps\"** - J.R. Firth (1957)\n",
    "\n",
    "This is the fundamental idea behind Word2Vec. The meaning of a word is determined by the words that frequently appear near it.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1. **\"The cat sat on the mat\"**\n",
    "   - Words near \"cat\": the, sat, on\n",
    "\n",
    "2. **\"The dog sat on the rug\"**\n",
    "   - Words near \"dog\": the, sat, on\n",
    "\n",
    "Notice that \"cat\" and \"dog\" appear in similar contexts! This is how the model learns they're related.\n",
    "\n",
    "**More Examples:**\n",
    "\n",
    "```\n",
    "\"The king ruled the kingdom\"\n",
    "\"The queen ruled the empire\"\n",
    "```\n",
    "‚Üí \"king\" and \"queen\" appear in similar contexts (both rule)\n",
    "\n",
    "```\n",
    "\"I am very happy today\"\n",
    "\"I am very joyful today\"\n",
    "```\n",
    "‚Üí \"happy\" and \"joyful\" are interchangeable (similar meaning)\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "If we can build a model that:\n",
    "1. Looks at what words appear near each other\n",
    "2. Learns to predict context words from target words (or vice versa)\n",
    "3. Adjusts word vectors to make these predictions accurate\n",
    "\n",
    "Then the learned vectors will automatically capture semantic meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6a966",
   "metadata": {},
   "source": [
    "### **4. Word2Vec: Two Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9042af8",
   "metadata": {},
   "source": [
    "Word2Vec has two main architectures:\n",
    "\n",
    "**1. CBOW (Continuous Bag of Words):**\n",
    "- **Input:** Context words (surrounding words)\n",
    "- **Output:** Target word (center word)\n",
    "- **Task:** Predict the center word from its context\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "Context: [\"The\", \"cat\", \"on\", \"the\"] ‚Üí Target: \"sat\"\n",
    "```\n",
    "\n",
    "**2. Skip-Gram (Our Focus Today):**\n",
    "- **Input:** Target word (center word)\n",
    "- **Output:** Context words (surrounding words)\n",
    "- **Task:** Predict context words from the center word\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "Target: \"sat\" ‚Üí Context: [\"cat\", \"on\"] (window size = 1)\n",
    "Target: \"sat\" ‚Üí Context: [\"The\", \"cat\", \"on\", \"the\"] (window size = 2)\n",
    "```\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*cuOmGT7NevP9oJFJfVpRKA.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "**Why Skip-Gram?**\n",
    "\n",
    "1. **Better for rare words:** Learns better representations for infrequent words\n",
    "2. **More training examples:** One target word creates multiple (target, context) pairs\n",
    "3. **Better performance:** Generally produces higher quality embeddings\n",
    "\n",
    "**The Skip-Gram Process:**\n",
    "\n",
    "```\n",
    "Sentence: \"The quick brown fox jumps\"\n",
    "Window size: 2\n",
    "\n",
    "For target word \"brown\" (index 2):\n",
    "  - Context word 1: \"The\" (2 positions left)\n",
    "  - Context word 2: \"quick\" (1 position left)\n",
    "  - Context word 3: \"fox\" (1 position right)\n",
    "  - Context word 4: \"jumps\" (2 positions right)\n",
    "\n",
    "Training pairs created:\n",
    "  (brown, The)\n",
    "  (brown, quick)\n",
    "  (brown, fox)\n",
    "  (brown, jumps)\n",
    "```\n",
    "\n",
    "This transforms unsupervised text into supervised learning pairs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2057f",
   "metadata": {},
   "source": [
    "### **5. Understanding the Skip-Gram Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87a743",
   "metadata": {},
   "source": [
    "The Skip-Gram model is surprisingly simple - just two layers:\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "```\n",
    "Input: Target word index (scalar)\n",
    "   ‚Üì\n",
    "Embedding Layer: vocab_size √ó embedding_dim\n",
    "   ‚Üì (This is what we want! The learned word vectors)\n",
    "Dense word vector (embedding_dim dimensions)\n",
    "   ‚Üì\n",
    "Linear Layer: embedding_dim √ó vocab_size\n",
    "   ‚Üì\n",
    "Output: Scores for all words (vocab_size)\n",
    "   ‚Üì\n",
    "Softmax: Convert to probabilities\n",
    "   ‚Üì\n",
    "Predicted context word\n",
    "```\n",
    "\n",
    "**Layer Details:**\n",
    "\n",
    "1. **Embedding Layer (`nn.Embedding`):**\n",
    "   - A lookup table that converts word indices to dense vectors\n",
    "   - Shape: `(vocab_size, embedding_dim)`\n",
    "   - Example: If vocab_size=10,000 and embedding_dim=100:\n",
    "     - This is a 10,000 √ó 100 matrix\n",
    "     - Each row is the embedding vector for one word\n",
    "   - **This layer's weights ARE the word embeddings we want to learn!**\n",
    "\n",
    "2. **Linear Layer (`nn.Linear`):**\n",
    "   - Maps from embedding space back to vocabulary space\n",
    "   - Shape: `(embedding_dim, vocab_size)`\n",
    "   - Produces a score for each possible context word\n",
    "\n",
    "**Example with Concrete Numbers:**\n",
    "\n",
    "```python\n",
    "# Suppose:\n",
    "vocab_size = 10,000 words\n",
    "embedding_dim = 100\n",
    "target_word_idx = 542  # Index for \"cat\"\n",
    "\n",
    "# Step 1: Embedding lookup\n",
    "embedding_layer = nn.Embedding(10000, 100)\n",
    "word_vector = embedding_layer(torch.tensor([542]))\n",
    "# Result: [100] dimensional vector (the embedding for \"cat\")\n",
    "\n",
    "# Step 2: Linear transformation\n",
    "linear_layer = nn.Linear(100, 10000)\n",
    "scores = linear_layer(word_vector)\n",
    "# Result: [10000] scores, one for each word in vocabulary\n",
    "\n",
    "# Step 3: Softmax (done by loss function)\n",
    "# Converts scores to probabilities\n",
    "# Model predicts which word is likely to be in context\n",
    "```\n",
    "\n",
    "**Training Objective:**\n",
    "\n",
    "Given a (target, context) pair like `(\"cat\", \"sat\")`:\n",
    "1. Feed \"cat\" index into the network\n",
    "2. Get probability distribution over all vocabulary words\n",
    "3. We want high probability for \"sat\" (the actual context word)\n",
    "4. Use Cross-Entropy Loss to measure error\n",
    "5. Backpropagation adjusts the embedding weights\n",
    "\n",
    "After many training examples, words that appear in similar contexts will have similar embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209630eb",
   "metadata": {},
   "source": [
    "### **6. Preparing Our Text Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0b43c",
   "metadata": {},
   "source": [
    "Let's start our implementation! We'll use a sample text corpus to train our Word2Vec model.\n",
    "\n",
    "**The Workflow:**\n",
    "1. **Get text data** - A collection of sentences\n",
    "2. **Preprocess** - Clean and tokenize the text\n",
    "3. **Build vocabulary** - Create word ‚Üî index mappings\n",
    "4. **Generate training pairs** - Create (target, context) pairs\n",
    "5. **Train model** - Learn the embeddings\n",
    "6. **Extract embeddings** - Get the learned word vectors\n",
    "\n",
    "For this tutorial, we'll use a small corpus of sentences. In practice, you'd use much larger datasets (Wikipedia, books, news articles, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2de6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus\n",
    "# In practice, you'd load this from a file or dataset\n",
    "corpus = \"\"\"\n",
    "The cat sat on the mat.\n",
    "The dog sat on the log.\n",
    "Cats and dogs are animals.\n",
    "The cat and the dog are friends.\n",
    "A king rules a kingdom.\n",
    "A queen rules an empire.\n",
    "The king and queen are royalty.\n",
    "The happy cat played with the ball.\n",
    "The joyful dog ran in the park.\n",
    "Animals are happy when they play.\n",
    "The cat sleeps on the warm mat.\n",
    "The dog loves to run and jump.\n",
    "Kings and queens live in castles.\n",
    "Happy animals are healthy animals.\n",
    "The small cat climbed the tall tree.\n",
    "The big dog guarded the house.\n",
    "Cats like to chase mice.\n",
    "Dogs like to catch balls.\n",
    "The wise king made fair decisions.\n",
    "The kind queen helped her people.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Text Corpus:\")\n",
    "print(\"=\"*70)\n",
    "print(corpus.strip())\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCorpus length: {len(corpus)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b32f7e",
   "metadata": {},
   "source": [
    "### **7. Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac78a37",
   "metadata": {},
   "source": [
    "Before we can work with text, we need to preprocess it:\n",
    "\n",
    "**Preprocessing Steps:**\n",
    "\n",
    "1. **Lowercase:** Convert all text to lowercase\n",
    "   - \"The Cat\" ‚Üí \"the cat\"\n",
    "   - Ensures \"Cat\" and \"cat\" are treated as the same word\n",
    "\n",
    "2. **Remove Punctuation:** Remove periods, commas, etc.\n",
    "   - \"cat.\" ‚Üí \"cat\"\n",
    "   - Simplifies tokenization\n",
    "\n",
    "3. **Tokenization:** Split text into individual words\n",
    "   - \"the cat sat\" ‚Üí [\"the\", \"cat\", \"sat\"]\n",
    "   - Each word becomes a token\n",
    "\n",
    "4. **Build Vocabulary:** Create mappings between words and indices\n",
    "   - word_to_idx: {\"cat\": 0, \"dog\": 1, ...}\n",
    "   - idx_to_word: {0: \"cat\", 1: \"dog\", ...}\n",
    "\n",
    "These mappings allow us to convert between words and the numerical indices our neural network needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a78196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: lowercase, remove punctuation, tokenize\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens (words)\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Split into words (tokenization)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Preprocess our corpus\n",
    "tokens = preprocess_text(corpus)\n",
    "\n",
    "print(\"Tokenized Text (first 50 tokens):\")\n",
    "print(\"=\"*70)\n",
    "print(tokens[:50])\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df526efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, min_count=1):\n",
    "    \"\"\"\n",
    "    Build vocabulary from tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of word tokens\n",
    "        min_count: Minimum frequency for a word to be included\n",
    "    \n",
    "    Returns:\n",
    "        word_to_idx: Dictionary mapping words to indices\n",
    "        idx_to_word: Dictionary mapping indices to words\n",
    "        word_counts: Counter object with word frequencies\n",
    "    \"\"\"\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(tokens)\n",
    "    \n",
    "    # Filter words by minimum count\n",
    "    vocab_words = [word for word, count in word_counts.items() if count >= min_count]\n",
    "    \n",
    "    # Sort for consistency\n",
    "    vocab_words = sorted(vocab_words)\n",
    "    \n",
    "    # Create mappings\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return word_to_idx, idx_to_word, word_counts\n",
    "\n",
    "# Build vocabulary\n",
    "word_to_idx, idx_to_word, word_counts = build_vocabulary(tokens, min_count=2)\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "print(\"Vocabulary Statistics:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Vocabulary size: {vocab_size} unique words\")\n",
    "print(f\"Total tokens: {len(tokens)} words\")\n",
    "print(\"\\nMost common words:\")\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"  {word:15s}: {count:3d} occurrences\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of word-to-index mappings\n",
    "print(\"\\nSample Word-to-Index Mappings:\")\n",
    "print(\"=\"*70)\n",
    "sample_words = list(word_to_idx.items())[:15]\n",
    "for word, idx in sample_words:\n",
    "    print(f\"  '{word:10s}' ‚Üí index {idx:3d}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "print(f\"   We now have {vocab_size} unique words in our vocabulary.\")\n",
    "print(f\"   Each word has a unique index from 0 to {vocab_size-1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2912763",
   "metadata": {},
   "source": [
    "Now we'll create the (target, context) pairs that will train our model. This is where we transform unsupervised text into supervised learning data!\n",
    "\n",
    "**The Sliding Window Approach:**\n",
    "\n",
    "We slide a window across our text and for each target word, we look at surrounding words within the window.\n",
    "\n",
    "**Example with window_size = 2:**\n",
    "\n",
    "```\n",
    "Sentence: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "          [  0  ,   1  ,   2  ,  3 ,   4  ,   5  ]  (positions)\n",
    "\n",
    "Target word at position 2 (\"sat\"):\n",
    "  - Look left up to 2 positions: \"the\" (pos 0), \"cat\" (pos 1)\n",
    "  - Look right up to 2 positions: \"on\" (pos 3), \"the\" (pos 4)\n",
    "\n",
    "Training pairs created:\n",
    "  (sat, the)   # from position 0\n",
    "  (sat, cat)   # from position 1\n",
    "  (sat, on)    # from position 3\n",
    "  (sat, the)   # from position 4\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **window_size** determines how far we look for context\n",
    "- Larger windows capture broader context but may include less relevant words\n",
    "- Smaller windows focus on immediate neighbors\n",
    "- Common values: 2-5\n",
    "\n",
    "**Implementation Strategy:**\n",
    "1. Convert tokens to indices\n",
    "2. For each target word position:\n",
    "   - Get words within the window\n",
    "   - Create (target_idx, context_idx) pairs\n",
    "3. Return as PyTorch tensors for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_pairs(tokens, word_to_idx, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate (target, context) training pairs using Skip-Gram approach\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of word tokens\n",
    "        word_to_idx: Dictionary mapping words to indices\n",
    "        window_size: How many words to look at on each side\n",
    "    \n",
    "    Returns:\n",
    "        target_indices: Tensor of target word indices\n",
    "        context_indices: Tensor of context word indices\n",
    "    \"\"\"\n",
    "    target_indices = []\n",
    "    context_indices = []\n",
    "    \n",
    "    # Convert tokens to indices\n",
    "    token_indices = [word_to_idx[token] for token in tokens if token in word_to_idx]\n",
    "    \n",
    "    # Slide window across the text\n",
    "    for target_pos in range(len(token_indices)):\n",
    "        target_idx = token_indices[target_pos]\n",
    "        \n",
    "        # Define the window boundaries\n",
    "        start = max(0, target_pos - window_size)\n",
    "        end = min(len(token_indices), target_pos + window_size + 1)\n",
    "        \n",
    "        # Get context words (all words in window except target)\n",
    "        for context_pos in range(start, end):\n",
    "            if context_pos != target_pos:  # Don't use the target word as its own context\n",
    "                context_idx = token_indices[context_pos]\n",
    "                target_indices.append(target_idx)\n",
    "                context_indices.append(context_idx)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    target_tensor = torch.tensor(target_indices, dtype=torch.long)\n",
    "    context_tensor = torch.tensor(context_indices, dtype=torch.long)\n",
    "    \n",
    "    return target_tensor, context_tensor\n",
    "\n",
    "# Generate training pairs\n",
    "window_size = 2\n",
    "target_tensor, context_tensor = generate_training_pairs(tokens, word_to_idx, window_size)\n",
    "\n",
    "print(\"Training Pair Generation:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Total training pairs: {len(target_tensor):,}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample training pairs\n",
    "print(\"\\nSample Training Pairs:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Target Word':<15} | {'Context Word':<15} | {'Target Idx':<10} | {'Context Idx'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_samples = 20\n",
    "for i in range(min(num_samples, len(target_tensor))):\n",
    "    target_idx = target_tensor[i].item()\n",
    "    context_idx = context_tensor[i].item()\n",
    "    target_word = idx_to_word[target_idx]\n",
    "    context_word = idx_to_word[context_idx]\n",
    "    print(f\"{target_word:<15} | {context_word:<15} | {target_idx:<10} | {context_idx}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Generated {len(target_tensor):,} training pairs!\")\n",
    "print(f\"   These pairs will teach the model which words appear together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344edec1",
   "metadata": {},
   "source": [
    "**Understanding the Training Pairs:**\n",
    "\n",
    "Looking at the pairs above, notice:\n",
    "- \"the\" appears with many different words (it's a common article)\n",
    "- \"cat\" appears with words like \"the\", \"sat\", \"mat\" (typical cat-related contexts)\n",
    "- \"dog\" appears with similar words to \"cat\" (helping the model learn they're related)\n",
    "- \"king\" and \"queen\" appear in similar contexts (both relate to \"rules\", \"kingdom\", etc.)\n",
    "\n",
    "The model will learn to give similar embeddings to words that share similar contexts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a46d69",
   "metadata": {},
   "source": [
    "### **9. Building the Skip-Gram Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17416f",
   "metadata": {},
   "source": [
    "Now let's implement our Skip-Gram neural network in PyTorch!\n",
    "\n",
    "**Model Architecture Recap:**\n",
    "\n",
    "```\n",
    "Input: target_word_idx (scalar integer)\n",
    "   ‚Üì\n",
    "Embedding Layer: [vocab_size √ó embedding_dim]\n",
    "   ‚Üì\n",
    "Word Vector: [embedding_dim]\n",
    "   ‚Üì\n",
    "Linear Layer: [embedding_dim √ó vocab_size]\n",
    "   ‚Üì\n",
    "Output Scores: [vocab_size]\n",
    "```\n",
    "\n",
    "**The Magic of nn.Embedding:**\n",
    "\n",
    "`nn.Embedding` is essentially a lookup table:\n",
    "- It stores a matrix of size `(vocab_size, embedding_dim)`\n",
    "- When you pass in an index, it returns the corresponding row\n",
    "- During training, these rows (word vectors) are updated via backpropagation\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "embedding = nn.Embedding(vocab_size=1000, embedding_dim=50)\n",
    "# This creates a 1000 √ó 50 matrix\n",
    "\n",
    "word_idx = torch.tensor([42])\n",
    "word_vector = embedding(word_idx)\n",
    "# Returns row 42 from the matrix (a 50-dimensional vector)\n",
    "```\n",
    "\n",
    "**After training, the weights of this embedding layer ARE our word vectors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Skip-Gram model\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Number of unique words in vocabulary\n",
    "            embedding_dim: Dimension of word embedding vectors\n",
    "        \"\"\"\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer: converts word indices to dense vectors\n",
    "        # This is the layer whose weights we want to extract after training!\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Linear layer: maps from embedding space to vocabulary space\n",
    "        # Used to predict context words\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, target_indices):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            target_indices: Indices of target words (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            Scores for each word in vocabulary (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Look up embeddings for target words\n",
    "        # Shape: (batch_size, embedding_dim)\n",
    "        embeds = self.embeddings(target_indices)\n",
    "        \n",
    "        # Project to vocabulary space\n",
    "        # Shape: (batch_size, vocab_size)\n",
    "        output = self.linear(embeds)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Extract the learned word embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Embedding matrix of shape (vocab_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embeddings.weight.data\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 50  # Dimension of word vectors (common choices: 50, 100, 300)\n",
    "\n",
    "# Create the model\n",
    "model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "print(\"Skip-Gram Model Architecture:\")\n",
    "print(\"=\"*70)\n",
    "print(model)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "embedding_params = model.embeddings.weight.numel()\n",
    "linear_params = sum(p.numel() for p in model.linear.parameters())\n",
    "\n",
    "print(f\"\\nParameter Breakdown:\")\n",
    "print(f\"  Embedding layer: {embedding_params:,} parameters ({vocab_size} √ó {embedding_dim})\")\n",
    "print(f\"  Linear layer:    {linear_params:,} parameters ({embedding_dim} √ó {vocab_size} + {vocab_size})\")\n",
    "print(f\"  Total:           {total_params:,} parameters\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0431393",
   "metadata": {},
   "source": [
    "### **10. Loss Function and Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc9c5b",
   "metadata": {},
   "source": [
    "**Loss Function: Cross-Entropy Loss**\n",
    "\n",
    "Just like in classification tasks, we use Cross-Entropy Loss because:\n",
    "- We're predicting which word (from vocabulary) is in the context\n",
    "- This is a multi-class classification problem\n",
    "- Cross-Entropy measures how well predicted probabilities match the true context word\n",
    "\n",
    "**Training Objective:**\n",
    "\n",
    "For a (target, context) pair like `(\"cat\", \"sat\")`:\n",
    "1. Input: \"cat\" index\n",
    "2. Model outputs: probability distribution over all words\n",
    "3. Loss: How different is this from the true distribution (where \"sat\" = 1, others = 0)\n",
    "4. Backprop: Adjust embeddings to increase probability of \"sat\"\n",
    "\n",
    "**Optimizer: Adam**\n",
    "\n",
    "We'll use Adam optimizer (like in Day 4) because:\n",
    "- Adaptive learning rates work well for embeddings\n",
    "- Converges faster than vanilla SGD\n",
    "- Requires minimal hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Loss Function:  {criterion}\")\n",
    "print(f\"Optimizer:      Adam\")\n",
    "print(f\"Learning Rate:  {learning_rate}\")\n",
    "print(f\"Device:         {device}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40befa",
   "metadata": {},
   "source": [
    "### **11. Training the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb0971",
   "metadata": {},
   "source": [
    "Now we'll train our Word2Vec model! The training process is similar to what we've done before:\n",
    "\n",
    "**Training Loop:**\n",
    "1. **Forward Pass:** Feed target word indices ‚Üí get predictions\n",
    "2. **Calculate Loss:** Compare predictions with actual context words\n",
    "3. **Backward Pass:** Calculate gradients\n",
    "4. **Update Parameters:** Adjust embedding weights\n",
    "\n",
    "**What's Happening:**\n",
    "- The model learns to predict context words from target words\n",
    "- Words appearing in similar contexts get similar embeddings\n",
    "- After many updates, the embedding layer contains meaningful word vectors\n",
    "\n",
    "**Training Tips:**\n",
    "- For small datasets, 100-500 epochs is typical\n",
    "- Loss should decrease steadily\n",
    "- For real applications, you'd use millions of words and more sophisticated techniques (negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(model, target_tensor, context_tensor, criterion, optimizer, \n",
    "                   num_epochs=100, batch_size=128, print_every=10):\n",
    "    \"\"\"\n",
    "    Train the Word2Vec Skip-Gram model\n",
    "    \n",
    "    Args:\n",
    "        model: The Skip-Gram model\n",
    "        target_tensor: Target word indices\n",
    "        context_tensor: Context word indices\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Number of samples per batch\n",
    "        print_every: Print progress every N epochs\n",
    "    \n",
    "    Returns:\n",
    "        List of losses per epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    # Move data to device\n",
    "    target_tensor = target_tensor.to(device)\n",
    "    context_tensor = context_tensor.to(device)\n",
    "    \n",
    "    num_samples = len(target_tensor)\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # Get batch\n",
    "            batch_targets = target_tensor[i:i+batch_size]\n",
    "            batch_contexts = context_tensor[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_targets)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, batch_contexts)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average loss for this epoch\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1:4d}/{num_epochs}] | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Training Complete!\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "losses = train_word2vec(\n",
    "    model, \n",
    "    target_tensor, \n",
    "    context_tensor, \n",
    "    criterion, \n",
    "    optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    print_every=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc28dd1",
   "metadata": {},
   "source": [
    "### **12. Visualizing Training Progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa197a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), losses, linewidth=2, color='#e74c3c')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Word2Vec Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Initial Loss: {losses[0]:.4f}\")\n",
    "print(f\"Final Loss:   {losses[-1]:.4f}\")\n",
    "print(f\"Reduction:    {losses[0] - losses[-1]:.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ The model has learned word embeddings!\")\n",
    "print(\"   Words appearing in similar contexts now have similar vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b0c82",
   "metadata": {},
   "source": [
    "### **13. Extracting and Analyzing Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ccece",
   "metadata": {},
   "source": [
    "The moment we've been waiting for! Let's extract the learned word vectors from the embedding layer.\n",
    "\n",
    "**Remember:** The weights of `model.embeddings` are our word vectors!\n",
    "- Each row corresponds to one word\n",
    "- Each row is a `embedding_dim` dimensional vector\n",
    "- Words with similar meanings should have similar vectors\n",
    "\n",
    "**How to measure similarity:**\n",
    "\n",
    "We use **Cosine Similarity**:\n",
    "\n",
    "$$\\text{similarity}(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| \\cdot ||\\vec{b}||} = \\cos(\\theta)$$\n",
    "\n",
    "- Returns values from -1 to 1\n",
    "- 1 = identical direction (very similar)\n",
    "- 0 = orthogonal (unrelated)\n",
    "- -1 = opposite direction (opposite meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb09b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the learned embeddings\n",
    "model.eval()\n",
    "embeddings = model.get_embeddings().cpu().numpy()\n",
    "\n",
    "print(\"Learned Word Embeddings:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"  ‚Üí {embeddings.shape[0]} words, each represented by {embeddings.shape[1]} numbers\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display embedding for a sample word\n",
    "sample_word = \"cat\"\n",
    "if sample_word in word_to_idx:\n",
    "    word_idx = word_to_idx[sample_word]\n",
    "    word_embedding = embeddings[word_idx]\n",
    "    \n",
    "    print(f\"\\nEmbedding for '{sample_word}':\")\n",
    "    print(f\"First 10 dimensions: {word_embedding[:10]}\")\n",
    "    print(f\"\\nThis {embedding_dim}-dimensional vector captures the meaning of '{sample_word}'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between all pairs of word embeddings\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Embedding matrix (vocab_size, embedding_dim)\n",
    "    \n",
    "    Returns:\n",
    "        Similarity matrix (vocab_size, vocab_size)\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / (norms + 1e-8)  # Add small value to avoid division by zero\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = np.dot(normalized, normalized.T)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def find_most_similar(word, word_to_idx, idx_to_word, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the most similar words to a given word\n",
    "    \n",
    "    Args:\n",
    "        word: Query word\n",
    "        word_to_idx: Word to index mapping\n",
    "        idx_to_word: Index to word mapping\n",
    "        embeddings: Embedding matrix\n",
    "        top_k: Number of similar words to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        return []\n",
    "    \n",
    "    # Get word index and embedding\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_embedding = embeddings[word_idx]\n",
    "    \n",
    "    # Compute similarities with all words\n",
    "    similarities = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        other_embedding = embeddings[idx]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(word_embedding, other_embedding) / (\n",
    "            np.linalg.norm(word_embedding) * np.linalg.norm(other_embedding) + 1e-8\n",
    "        )\n",
    "        \n",
    "        similarities.append((idx, similarity))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top k (excluding the word itself)\n",
    "    results = []\n",
    "    for idx, sim in similarities[1:top_k+1]:  # Skip first (the word itself)\n",
    "        results.append((idx_to_word[idx], sim))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the similarity function\n",
    "print(\"\\nTesting Word Similarity:\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words for different test cases\n",
    "test_words = [\"cat\", \"dog\", \"king\", \"queen\", \"happy\"]\n",
    "\n",
    "for test_word in test_words:\n",
    "    if test_word in word_to_idx:\n",
    "        print(f\"\\nMost similar words to '{test_word}':\")\n",
    "        print(\"-\" * 50)\n",
    "        similar_words = find_most_similar(test_word, word_to_idx, idx_to_word, embeddings, top_k=5)\n",
    "        \n",
    "        for i, (word, similarity) in enumerate(similar_words, 1):\n",
    "            print(f\"  {i}. {word:<15s} (similarity: {similarity:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7345d2c",
   "metadata": {},
   "source": [
    "**Interpreting the Results:**\n",
    "\n",
    "If the model trained well, you should see:\n",
    "- **\"cat\"** is similar to **\"dog\"** (both animals, both pets)\n",
    "- **\"king\"** is similar to **\"queen\"** (both royalty)\n",
    "- **\"happy\"** is similar to words in emotional contexts\n",
    "\n",
    "The model learned these relationships **purely from seeing which words appear together** - we never told it that cats and dogs are animals!\n",
    "\n",
    "**Note:** With our small corpus, the similarities might not be perfect. In practice:\n",
    "- Use millions of words of text\n",
    "- Train for longer\n",
    "- Use larger embedding dimensions (100-300)\n",
    "- Use techniques like negative sampling for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d58125e",
   "metadata": {},
   "source": [
    "### **16. Conclusion and Key Takeaways**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e3013",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully implemented Word2Vec from scratch and learned how machines can understand language!\n",
    "\n",
    "**üéØ What We Accomplished:**\n",
    "\n",
    "1. ‚úÖ Understood the problem: How to represent words as numbers\n",
    "2. ‚úÖ Learned about one-hot encoding vs dense embeddings\n",
    "3. ‚úÖ Explored the distributional hypothesis (\"a word is known by its context\")\n",
    "4. ‚úÖ Preprocessed text: tokenization, vocabulary building\n",
    "5. ‚úÖ Generated training pairs using the Skip-Gram approach\n",
    "6. ‚úÖ Built and trained a neural network to learn word embeddings\n",
    "7. ‚úÖ Extracted learned embeddings and found similar words\n",
    "8. ‚úÖ Visualized embeddings in 2D to see semantic clusters\n",
    "9. ‚úÖ Performed word analogies using vector arithmetic\n",
    "\n",
    "**üîë Key Concepts:**\n",
    "\n",
    "1. **Word Embeddings:**\n",
    "   - Dense vector representations of words\n",
    "   - Capture semantic meaning automatically\n",
    "   - Much better than sparse one-hot encodings\n",
    "\n",
    "2. **Distributional Hypothesis:**\n",
    "   - Words appearing in similar contexts have similar meanings\n",
    "   - The foundation of Word2Vec and modern NLP\n",
    "\n",
    "3. **Skip-Gram Model:**\n",
    "   - Predicts context words from target words\n",
    "   - Simple 2-layer architecture\n",
    "   - The embedding layer weights ARE the word vectors\n",
    "\n",
    "4. **Training Process:**\n",
    "   - Transform text into (target, context) pairs\n",
    "   - Train like any supervised learning problem\n",
    "   - Similar words end up with similar vectors\n",
    "\n",
    "5. **Vector Arithmetic:**\n",
    "   - Can perform math on meanings: king - man + woman ‚âà queen\n",
    "   - Embeddings capture semantic relationships\n",
    "\n",
    "**üìä From Simple to Powerful:**\n",
    "\n",
    "| Aspect | One-Hot Encoding | Word Embeddings |\n",
    "|--------|-----------------|------------------|\n",
    "| **Dimensions** | Vocabulary size (e.g., 100,000) | Fixed small size (50-300) |\n",
    "| **Sparsity** | 99.999% zeros | Dense (all values meaningful) |\n",
    "| **Semantic Meaning** | None (all words equally different) | Rich (similar words have similar vectors) |\n",
    "| **Generalization** | Poor | Excellent |\n",
    "| **Vector Arithmetic** | Meaningless | Meaningful analogies! |\n",
    "\n",
    "**üöÄ Real-World Applications:**\n",
    "\n",
    "Word embeddings are used in:\n",
    "- **Search Engines:** Understanding query intent\n",
    "- **Recommendation Systems:** Finding similar products/content\n",
    "- **Sentiment Analysis:** Understanding emotions in text\n",
    "- **Machine Translation:** Google Translate, DeepL\n",
    "- **Chatbots:** Understanding user messages\n",
    "- **Question Answering:** Finding relevant answers\n",
    "- **Document Classification:** Categorizing articles, emails\n",
    "\n",
    "**üí° Improvements for Production:**\n",
    "\n",
    "Our implementation is educational. For real applications:\n",
    "\n",
    "1. **Use More Data:**\n",
    "   - Millions/billions of words (Wikipedia, books, web pages)\n",
    "   - Our 200 words ‚Üí Real systems use billions\n",
    "\n",
    "2. **Negative Sampling:**\n",
    "   - Instead of predicting over entire vocabulary (slow)\n",
    "   - Sample a few \"negative\" examples\n",
    "   - Much faster and more efficient\n",
    "\n",
    "3. **Subword Information:**\n",
    "   - Handle rare/unknown words better\n",
    "   - FastText extends Word2Vec with character n-grams\n",
    "\n",
    "4. **Pre-trained Embeddings:**\n",
    "   - Use embeddings trained on huge corpora\n",
    "   - GloVe, FastText, Word2Vec pre-trained models\n",
    "   - Don't train from scratch for every task\n",
    "\n",
    "5. **Modern Alternatives:**\n",
    "   - **BERT, GPT, T5:** Contextual embeddings (different vectors for same word in different contexts)\n",
    "   - **Sentence Transformers:** Embeddings for entire sentences\n",
    "   - But Word2Vec is still widely used and very effective!\n",
    "\n",
    "**üéì From Images to Language:**\n",
    "\n",
    "Compare our journey:\n",
    "\n",
    "| Day | Task | Input | Output | Key Learning |\n",
    "|-----|------|-------|--------|---------------|\n",
    "| **4** | Fashion MNIST | Images (28√ó28 pixels) | 10 clothing classes | CNNs for vision |\n",
    "| **5** | Word2Vec | Text (words) | Word vectors | Embeddings for NLP |\n",
    "\n",
    "Both use neural networks but handle different data types!\n",
    "\n",
    "**üåü You've now learned the foundation of Natural Language Processing!**\n",
    "\n",
    "From here, you can explore:\n",
    "- Recurrent Neural Networks (RNNs) for sequences\n",
    "- Transformers (the architecture behind ChatGPT)\n",
    "- BERT, GPT, and other large language models\n",
    "- Sentiment analysis, machine translation, text generation\n",
    "\n",
    "All of these build on the concept of word embeddings that you learned today!\n",
    "\n",
    "---\n",
    "\n",
    "**Keep exploring and building! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc859782",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
