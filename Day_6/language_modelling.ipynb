{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7065262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "const firstCell = document.querySelector('.cell.code_cell');\n",
       "if (firstCell) {\n",
       "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
       "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
       "  <h1 style=\"margin:0;\">ðŸ‘‹ Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
       "\n",
       "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
       "       alt=\"Algopath Coding Academy Logo\"\n",
       "       width=\"400\"\n",
       "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
       "\n",
       "  <p style=\"font-size:16px; margin:0;\">\n",
       "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
       "  </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">ðŸ‘‹ Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d6451",
   "metadata": {},
   "source": [
    "## **Part 1: The Assembly Line â€“ Data Preprocessing & Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31383c",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "Build a robust data preprocessing pipeline for sequence data to prepare it for training a language model. In Part 1, you will:\n",
    "\n",
    "- Learn how to tokenize raw text and build vocabulary mappings (word â†” index)\n",
    "- Create input-target pairs using a sliding window approach for next word prediction\n",
    "- Handle variable-length sequences using padding and attention masks\n",
    "- Construct efficient PyTorch Datasets and DataLoaders for batching and shuffling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8df83",
   "metadata": {},
   "source": [
    "### **1. Introduction to Sequence Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde5e5e",
   "metadata": {},
   "source": [
    "**What is Sequence Data?**\n",
    "\n",
    "Unlike the tabular data we worked with in previous days (where each row was independent), **sequence data** has an inherent order where the position of each element matters. Examples include:\n",
    "- Text: \"To be or not to be\"\n",
    "- Time series: Stock prices over time\n",
    "- DNA sequences: \"ATCGATCG\"\n",
    "\n",
    "**Our Goal: Next Word Prediction**\n",
    "\n",
    "We want to build a model that can predict the next word given previous words. Mathematically, we're trying to estimate:\n",
    "\n",
    "$$P(w_t | w_{t-1}, w_{t-2}, ..., w_{t-n})$$\n",
    "\n",
    "Where:\n",
    "- $w_t$ is the word we want to predict (at position $t$)\n",
    "- $w_{t-1}, w_{t-2}, ..., w_{t-n}$ are the previous words (context)\n",
    "\n",
    "For example:\n",
    "- **Input:** \"To be or not to\"\n",
    "- **Output:** \"be\" (the next word)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://camo.githubusercontent.com/5840714d7f7d8283b758b40c08a1c5423768fed426be28a69153db6cf7d45c04/68747470733a2f2f626c6f676765722e676f6f676c6575736572636f6e74656e742e636f6d2f696d672f612f41567658734569456173376a454c454232374964786c4a34466a39726d506638783554652d6e4d5874714751566b4e566856475f62496c2d787252747653726f4a534b6634526270586b32317a5a334663664d4d34306a5f4765793749726f56596a6c624f36445865324e55475a775546355036624f6571565f36716566734d6451594a66724c627461685475304e3257754e616437775a712d6a7a684c564e744a414f73664a587a757441546b6775434d325a64394d68484e65364850617556394d533d77313230302d6831323030\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "This is the foundation of language models like ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87b248",
   "metadata": {},
   "source": [
    "### **2. The Dataset: Tiny Shakespeare**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff9cad4",
   "metadata": {},
   "source": [
    "We'll use a collection of Shakespeare's works as our training data. This dataset is perfect for learning because:\n",
    "- It's small enough to train quickly\n",
    "- It has consistent style and vocabulary\n",
    "- It demonstrates the challenge of long-range dependencies in text\n",
    "\n",
    "Let's start by loading and exploring the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70504d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "Using device: cuda\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "\n",
    "# Check PyTorch version and device\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67065458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Downloaded tinyshakespeare.txt\n",
      "\n",
      "âœ“ Loaded text data\n",
      "Total characters: 1,115,394\n",
      "\n",
      "First 500 characters:\n",
      "--------------------------------------------------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download the Tiny Shakespeare dataset\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filename = 'tinyshakespeare.txt'\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"âœ“ Downloaded {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading file: {e}\")\n",
    "    \n",
    "# Load the text data\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"\\nâœ“ Loaded text data\")\n",
    "print(f\"Total characters: {len(text):,}\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(\"-\" * 80)\n",
    "print(text[:500])\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbcef3",
   "metadata": {},
   "source": [
    "**Tokenization: Converting Text to Numbers**\n",
    "\n",
    "Neural networks can't process text directlyâ€”they need numbers. **Tokenization** is the process of:\n",
    "1. Splitting text into units (words, subwords, or characters)\n",
    "2. Creating a mapping from tokens to unique integers\n",
    "3. Converting text sequences into sequences of integers\n",
    "\n",
    "For this notebook, we'll use **word-level tokenization** (splitting by spaces and punctuation).\n",
    "\n",
    "**Building the Vocabulary**\n",
    "\n",
    "The vocabulary is our \"dictionary\" that maps:\n",
    "- **word â†’ index** (word2idx): Used during training to convert words to numbers\n",
    "- **index â†’ word** (idx2word): Used during generation to convert numbers back to words\n",
    "\n",
    "We'll also add three special tokens:\n",
    "- `<PAD>`: Padding token (for making sequences the same length)\n",
    "- `<UNK>`: Unknown token (for words not in our vocabulary)\n",
    "- `<EOS>`: End of sequence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8aae495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 249,435\n",
      "Unique tokens: 13,388\n",
      "\n",
      "First 50 tokens:\n",
      "['first', 'citizen', ':', 'before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.', 'all', ':', 'speak', ',', 'speak', '.', 'first', 'citizen', ':', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish', '?', 'all', ':', 'resolved', '.', 'resolved', '.', 'first', 'citizen', ':', 'first', ',', 'you', 'know', 'caius', 'marcius', 'is', 'chief']\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenization: split on whitespace and basic punctuation\n",
    "import re\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Basic word tokenization that splits on whitespace and handles punctuation.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Add spaces around punctuation\n",
    "    text = re.sub(r'([.,!?;:])', r' \\1 ', text)\n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the entire text\n",
    "tokens = simple_tokenize(text)\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "print(f\"Unique tokens: {len(set(tokens)):,}\")\n",
    "print(f\"\\nFirst 50 tokens:\")\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e323d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13,391\n",
      "\n",
      "Special tokens:\n",
      "  <PAD> â†’ 0\n",
      "  <UNK> â†’ 1\n",
      "  <EOS> â†’ 2\n",
      "\n",
      "First 20 words in vocabulary:\n",
      "[',', ':', '.', 'the', 'and', 'to', 'i', 'of', ';', 'you', 'my', 'a', 'that', '?', 'in', '!', 'is', 'not', 'for', 'with']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary: word to index and index to word mappings\n",
    "def build_vocabulary(tokens, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Build vocabulary from tokens, keeping only the most frequent words.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        max_vocab_size: Maximum vocabulary size (excluding special tokens)\n",
    "    \n",
    "    Returns:\n",
    "        word2idx: Dictionary mapping words to indices\n",
    "        idx2word: Dictionary mapping indices to words\n",
    "    \"\"\"\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(tokens)\n",
    "    \n",
    "    # Get most common words\n",
    "    most_common = word_counts.most_common(max_vocab_size)\n",
    "    \n",
    "    # Create vocabulary with special tokens\n",
    "    vocab = ['<PAD>', '<UNK>', '<EOS>'] + [word for word, _ in most_common]\n",
    "    \n",
    "    # Create mappings\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word = build_vocabulary(tokens, max_vocab_size = len(set(tokens)))\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"\\nSpecial tokens:\")\n",
    "print(f\"  <PAD> â†’ {word2idx['<PAD>']}\")\n",
    "print(f\"  <UNK> â†’ {word2idx['<UNK>']}\")\n",
    "print(f\"  <EOS> â†’ {word2idx['<EOS>']}\")\n",
    "print(f\"\\nFirst 20 words in vocabulary:\")\n",
    "print([idx2word[i] for i in range(3, 23)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beca0ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token indices: 249,435\n",
      "\n",
      "Example conversion:\n",
      "Tokens:  ['first', 'citizen', ':', 'before', 'we', 'proceed', 'any', 'further', ',', 'hear']\n",
      "Indices: [95, 275, 4, 145, 43, 974, 148, 670, 3, 132]\n",
      "\n",
      "Verifying back-conversion:\n",
      "Back to words: ['first', 'citizen', ':', 'before', 'we', 'proceed', 'any', 'further', ',', 'hear']\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to indices\n",
    "def tokens_to_indices(tokens, word2idx):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to a list of indices.\n",
    "    Unknown words are mapped to <UNK>.\n",
    "    \"\"\"\n",
    "    return [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
    "\n",
    "# Convert our tokens to indices\n",
    "token_indices = tokens_to_indices(tokens, word2idx)\n",
    "\n",
    "print(f\"Total token indices: {len(token_indices):,}\")\n",
    "print(f\"\\nExample conversion:\")\n",
    "print(f\"Tokens:  {tokens[:10]}\")\n",
    "print(f\"Indices: {token_indices[:10]}\")\n",
    "print(f\"\\nVerifying back-conversion:\")\n",
    "print(f\"Back to words: {[idx2word[idx] for idx in token_indices[:10]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d462c",
   "metadata": {},
   "source": [
    "### **3. The Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4fe466",
   "metadata": {},
   "source": [
    "**What is a PyTorch Dataset?**\n",
    "\n",
    "PyTorch provides a `torch.utils.data.Dataset` class that helps organize data for training. A custom dataset must implement three methods:\n",
    "1. `__init__`: Initialize the dataset (load data, set parameters)\n",
    "2. `__len__`: Return the number of samples in the dataset\n",
    "3. `__getitem__`: Return a single sample (input and target) at a given index\n",
    "\n",
    "**The Sliding Window Approach**\n",
    "\n",
    "To train our model, we need to create input-target pairs from our sequence of tokens. We use a \"sliding window\" that moves through the text:\n",
    "\n",
    "```\n",
    "Full sequence: \"to be or not to be that is the question\"\n",
    "Window size: 5\n",
    "\n",
    "Window 1:\n",
    "  Input:  \"to be or not to\"\n",
    "  Target: \"be or not to be\"\n",
    "\n",
    "Window 2:\n",
    "  Input:  \"be or not to be\"\n",
    "  Target: \"or not to be that\"\n",
    "\n",
    "Window 3:\n",
    "  Input:  \"or not to be that\"\n",
    "  Target: \"not to be that is\"\n",
    "...\n",
    "```\n",
    "\n",
    "Notice that:\n",
    "- The **input** is a sequence of `n` words\n",
    "- The **target** is the same sequence shifted by 1 position (the next word for each position)\n",
    "- The model learns to predict each word based on all previous words in the sequence\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7077bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created!\n",
      "Sequence length: 20\n",
      "Total sequences: 249,415\n",
      "\n",
      "Each sequence consists of:\n",
      "  - Input: 20 tokens\n",
      "  - Target: 20 tokens (shifted by 1)\n"
     ]
    }
   ],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for the Tiny Shakespeare text.\n",
    "    Creates input-target pairs using a sliding window approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token_indices, seq_length):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            token_indices: List of token indices (already converted from words)\n",
    "            seq_length: Length of each sequence (window size)\n",
    "        \"\"\"\n",
    "        self.token_indices = token_indices\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Calculate how many sequences we can create\n",
    "        # We need seq_length + 1 tokens for each sample (input + target)\n",
    "        self.num_sequences = len(token_indices) - seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return self.num_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single input-target pair.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sequence to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            input_seq: Tensor of input token indices\n",
    "            target_seq: Tensor of target token indices (shifted by 1)\n",
    "        \"\"\"\n",
    "        # Extract a window of seq_length + 1 tokens\n",
    "        sequence = self.token_indices[idx:idx + self.seq_length + 1]\n",
    "        \n",
    "        # Split into input and target\n",
    "        input_seq = torch.tensor(sequence[:-1], dtype=torch.long)   # First seq_length tokens\n",
    "        target_seq = torch.tensor(sequence[1:], dtype=torch.long)   # Last seq_length tokens\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Create dataset with sequence length of 20\n",
    "seq_length = 20\n",
    "dataset = ShakespeareDataset(token_indices, seq_length)\n",
    "\n",
    "print(f\"Dataset created!\")\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "print(f\"Total sequences: {len(dataset):,}\")\n",
    "print(f\"\\nEach sequence consists of:\")\n",
    "print(f\"  - Input: {seq_length} tokens\")\n",
    "print(f\"  - Target: {seq_length} tokens (shifted by 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82cb3afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sequences from the dataset:\n",
      "\n",
      "Sample 1:\n",
      "  Input:  first citizen : before we proceed any further , hear me speak . all : speak , speak . first\n",
      "  Target: citizen : before we proceed any further , hear me speak . all : speak , speak . first citizen\n",
      "  Indices (first 10): [95, 275, 4, 145, 43, 974, 148, 670, 3, 132]\n",
      "\n",
      "Sample 2:\n",
      "  Input:  citizen : before we proceed any further , hear me speak . all : speak , speak . first citizen\n",
      "  Target: : before we proceed any further , hear me speak . all : speak , speak . first citizen :\n",
      "  Indices (first 10): [275, 4, 145, 43, 974, 148, 670, 3, 132, 23]\n",
      "\n",
      "Sample 3:\n",
      "  Input:  : before we proceed any further , hear me speak . all : speak , speak . first citizen :\n",
      "  Target: before we proceed any further , hear me speak . all : speak , speak . first citizen : you\n",
      "  Indices (first 10): [4, 145, 43, 974, 148, 670, 3, 132, 23, 111]\n",
      "\n",
      "Detailed view of the first sample:\n",
      "Position   Input Word      Target Word    \n",
      "----------------------------------------\n",
      "0          first           citizen        \n",
      "1          citizen         :              \n",
      "2          :               before         \n",
      "3          before          we             \n",
      "4          we              proceed        \n",
      "5          proceed         any            \n",
      "6          any             further        \n",
      "7          further         ,              \n",
      "8          ,               hear           \n",
      "9          hear            me             \n"
     ]
    }
   ],
   "source": [
    "# Let's examine a few samples to understand the sliding window\n",
    "print(\"Example sequences from the dataset:\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    input_seq, target_seq = dataset[i]\n",
    "    \n",
    "    # Convert indices back to words for visualization\n",
    "    input_words = [idx2word[idx.item()] for idx in input_seq]\n",
    "    target_words = [idx2word[idx.item()] for idx in target_seq]\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Input:  {' '.join(input_words)}\")\n",
    "    print(f\"  Target: {' '.join(target_words)}\")\n",
    "    print(f\"  Indices (first 10): {input_seq[:10].tolist()}\")\n",
    "    print()\n",
    "\n",
    "# Verify the relationship between input and target\n",
    "input_seq, target_seq = dataset[0]\n",
    "print(\"Detailed view of the first sample:\")\n",
    "print(f\"{'Position':<10} {'Input Word':<15} {'Target Word':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(min(10, len(input_seq))):\n",
    "    print(f\"{i:<10} {idx2word[input_seq[i].item()]:<15} {idx2word[target_seq[i].item()]:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08dbbc",
   "metadata": {},
   "source": [
    "### **4. Handling Variable Lengths: The collate_fn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fbac5",
   "metadata": {},
   "source": [
    "**The Problem: Variable Sequence Lengths**\n",
    "\n",
    "In real-world scenarios, text sequences often have different lengths:\n",
    "- \"Hello\" (1 word)\n",
    "- \"To be or not to be\" (6 words)\n",
    "- \"The quick brown fox jumps over the lazy dog\" (9 words)\n",
    "\n",
    "However, neural networks process data in **batches** (multiple samples at once for efficiency), and tensors in a batch must have the same shape. We can't stack these sequences into a single tensor:\n",
    "\n",
    "```python\n",
    "# This won't work!\n",
    "batch = [\n",
    "    [1, 2, 3],           # length 3\n",
    "    [4, 5, 6, 7, 8],     # length 5\n",
    "    [9, 10]              # length 2\n",
    "]\n",
    "# â†’ Cannot create a rectangular tensor!\n",
    "```\n",
    "\n",
    "**The Solution: Padding**\n",
    "\n",
    "We solve this by **padding** shorter sequences with a special `<PAD>` token (index 0) to match the longest sequence in the batch:\n",
    "\n",
    "```python\n",
    "# After padding to length 5:\n",
    "batch = [\n",
    "    [1, 2, 3, 0, 0],     # length 3 â†’ padded with 2 zeros\n",
    "    [4, 5, 6, 7, 8],     # length 5 â†’ no padding needed\n",
    "    [9, 10, 0, 0, 0]     # length 2 â†’ padded with 3 zeros\n",
    "]\n",
    "# â†’ Now we have a rectangular tensor! âœ“\n",
    "```\n",
    "\n",
    "**The collate_fn**\n",
    "\n",
    "The `collate_fn` is a custom function that PyTorch's DataLoader calls to combine individual samples into a batch. We'll use it to:\n",
    "1. Find the longest sequence in the batch\n",
    "2. Pad all sequences to that length\n",
    "3. Create an **attention mask** that marks which tokens are real (1) and which are padding (0)\n",
    "\n",
    "The attention mask helps the model ignore padding tokens during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deacd1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Custom collate_fn defined!\n",
      "\n",
      "This function will:\n",
      "  1. Find the longest sequence in each batch\n",
      "  2. Pad shorter sequences with <PAD> tokens (index 0)\n",
      "  3. Create attention masks to mark real vs. padding tokens\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length sequences.\n",
    "    Pads all sequences in a batch to the length of the longest sequence.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples (input_seq, target_seq)\n",
    "        \n",
    "    Returns:\n",
    "        padded_inputs: Tensor of shape (batch_size, max_length)\n",
    "        padded_targets: Tensor of shape (batch_size, max_length)\n",
    "        attention_mask: Tensor of shape (batch_size, max_length) with 1s for real tokens, 0s for padding\n",
    "    \"\"\"\n",
    "    # Separate inputs and targets\n",
    "    inputs, targets = zip(*batch)\n",
    "    \n",
    "    # Find the maximum length in this batch\n",
    "    max_length = max(len(seq) for seq in inputs)\n",
    "    \n",
    "    # Initialize lists for padded sequences and masks\n",
    "    padded_inputs = []\n",
    "    padded_targets = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # Pad each sequence\n",
    "    for input_seq, target_seq in zip(inputs, targets):\n",
    "        seq_length = len(input_seq)\n",
    "        \n",
    "        # Calculate padding needed\n",
    "        padding_length = max_length - seq_length\n",
    "        \n",
    "        # Pad with 0 (the <PAD> token index)\n",
    "        padded_input = torch.cat([input_seq, torch.zeros(padding_length, dtype=torch.long)])\n",
    "        padded_target = torch.cat([target_seq, torch.zeros(padding_length, dtype=torch.long)])\n",
    "        \n",
    "        # Create attention mask: 1 for real tokens, 0 for padding\n",
    "        mask = torch.cat([torch.ones(seq_length, dtype=torch.long), \n",
    "                         torch.zeros(padding_length, dtype=torch.long)])\n",
    "        \n",
    "        padded_inputs.append(padded_input)\n",
    "        padded_targets.append(padded_target)\n",
    "        attention_masks.append(mask)\n",
    "    \n",
    "    # Stack into tensors\n",
    "    padded_inputs = torch.stack(padded_inputs)\n",
    "    padded_targets = torch.stack(padded_targets)\n",
    "    attention_masks = torch.stack(attention_masks)\n",
    "    \n",
    "    return padded_inputs, padded_targets, attention_masks\n",
    "\n",
    "print(\"âœ“ Custom collate_fn defined!\")\n",
    "print(\"\\nThis function will:\")\n",
    "print(\"  1. Find the longest sequence in each batch\")\n",
    "print(\"  2. Pad shorter sequences with <PAD> tokens (index 0)\")\n",
    "print(\"  3. Create attention masks to mark real vs. padding tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9d8f1",
   "metadata": {},
   "source": [
    "**Creating the DataLoader**\n",
    "\n",
    "Now we'll create a `DataLoader` that uses our custom dataset and collate function. The DataLoader:\n",
    "- Automatically batches samples together\n",
    "- Shuffles data between epochs (for better training)\n",
    "- Applies our custom padding through the collate_fn\n",
    "- Can load data in parallel (using multiple workers) for faster training\n",
    "\n",
    "This is the \"assembly line\" that feeds data to our model during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57249c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DataLoader created!\n",
      "\n",
      "DataLoader configuration:\n",
      "  - Batch size: 32\n",
      "  - Total batches: 7,795\n",
      "  - Shuffle: True\n",
      "  - Custom padding: Yes\n",
      "\n",
      "Total samples that will be seen in one epoch: 249,440\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,           # Shuffle data for better training\n",
    "    collate_fn = collate_fn   # Use our custom padding function\n",
    ")\n",
    "\n",
    "print(f\"âœ“ DataLoader created!\")\n",
    "print(f\"\\nDataLoader configuration:\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Total batches: {len(dataloader):,}\")\n",
    "print(f\"  - Shuffle: True\")\n",
    "print(f\"  - Custom padding: Yes\")\n",
    "print(f\"\\nTotal samples that will be seen in one epoch: {len(dataloader) * batch_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469148e",
   "metadata": {},
   "source": [
    "**Visualizing a Batch**\n",
    "\n",
    "Let's fetch one batch from the DataLoader and examine its structure to confirm everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f329093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Successfully loaded a batch!\n",
      "\n",
      "Batch shapes:\n",
      "  - Inputs:  torch.Size([32, 20])  (batch_size, sequence_length)\n",
      "  - Targets: torch.Size([32, 20])  (batch_size, sequence_length)\n",
      "  - Masks:   torch.Size([32, 20])  (batch_size, sequence_length)\n",
      "\n",
      "Batch statistics:\n",
      "  - Batch size: 32\n",
      "  - Sequence length: 20\n",
      "  - Total tokens in batch: 640\n",
      "  - Padding tokens: 0\n",
      "  - Real tokens: 640\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch from the DataLoader\n",
    "batch_iterator = iter(dataloader)\n",
    "inputs, targets, masks = next(batch_iterator)\n",
    "\n",
    "print(\"âœ“ Successfully loaded a batch!\\n\")\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  - Inputs:  {inputs.shape}  (batch_size, sequence_length)\")\n",
    "print(f\"  - Targets: {targets.shape}  (batch_size, sequence_length)\")\n",
    "print(f\"  - Masks:   {masks.shape}  (batch_size, sequence_length)\")\n",
    "\n",
    "print(f\"\\nBatch statistics:\")\n",
    "print(f\"  - Batch size: {inputs.shape[0]}\")\n",
    "print(f\"  - Sequence length: {inputs.shape[1]}\")\n",
    "print(f\"  - Total tokens in batch: {inputs.numel():,}\")\n",
    "print(f\"  - Padding tokens: {(inputs == 0).sum().item():,}\")\n",
    "print(f\"  - Real tokens: {(inputs != 0).sum().item():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bd5641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed view of first 3 sequences in the batch:\n",
      "\n",
      "Sequence 1:\n",
      "  Length: 20 tokens\n",
      "  Input:  your bum is the greatest thing about you ; so that in the beastliest sense you are pompey the great\n",
      "  Target: bum is the greatest thing about you ; so that in the beastliest sense you are pompey the great .\n",
      "  Input indices:  [26, 6749, 19, 6, 1746, 245, 304, 12, 11, 35, 15, 17, 6, 11922, 930, 12, 48, 334, 6, 176]\n",
      "  Target indices: [6749, 19, 6, 1746, 245, 304, 12, 11, 35, 15, 17, 6, 11922, 930, 12, 48, 334, 6, 176, 5]\n",
      "  Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Sequence 2:\n",
      "  Length: 20 tokens\n",
      "  Input:  of heaven . be he the fire , i'll be the yielding water : the rage be his , whilst\n",
      "  Target: heaven . be he the fire , i'll be the yielding water : the rage be his , whilst on\n",
      "  Input indices:  [10, 190, 5, 25, 30, 6, 478, 3, 80, 25, 6, 3007, 817, 4, 6, 766, 25, 27, 3, 718]\n",
      "  Target indices: [190, 5, 25, 30, 6, 478, 3, 80, 25, 6, 3007, 817, 4, 6, 766, 25, 27, 3, 718, 53]\n",
      "  Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Sequence 3:\n",
      "  Length: 20 tokens\n",
      "  Input:  is pressing to death , whipping , and hanging . duke vincentio : slandering a prince deserves it . she\n",
      "  Target: pressing to death , whipping , and hanging . duke vincentio : slandering a prince deserves it . she ,\n",
      "  Input indices:  [19, 12411, 8, 109, 3, 6789, 3, 7, 1385, 5, 74, 131, 4, 12412, 14, 184, 1415, 24, 5, 70]\n",
      "  Target indices: [12411, 8, 109, 3, 6789, 3, 7, 1385, 5, 74, 131, 4, 12412, 14, 184, 1415, 24, 5, 70, 3]\n",
      "  Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine the first 3 sequences in the batch\n",
    "print(\"Detailed view of first 3 sequences in the batch:\\n\")\n",
    "\n",
    "for i in range(min(3, inputs.shape[0])):\n",
    "    print(f\"Sequence {i+1}:\")\n",
    "    \n",
    "    # Get the sequence\n",
    "    input_seq = inputs[i]\n",
    "    target_seq = targets[i]\n",
    "    mask_seq = masks[i]\n",
    "    \n",
    "    # Find where padding starts\n",
    "    real_length = mask_seq.sum().item()\n",
    "    \n",
    "    # Convert to words (only real tokens, not padding)\n",
    "    input_words = [idx2word[idx.item()] for idx in input_seq[:real_length]]\n",
    "    target_words = [idx2word[idx.item()] for idx in target_seq[:real_length]]\n",
    "    \n",
    "    print(f\"  Length: {real_length} tokens\")\n",
    "    print(f\"  Input:  {' '.join(input_words)}\")\n",
    "    print(f\"  Target: {' '.join(target_words)}\")\n",
    "    print(f\"  Input indices:  {input_seq.tolist()}\")\n",
    "    print(f\"  Target indices: {target_seq.tolist()}\")\n",
    "    print(f\"  Mask: {mask_seq.tolist()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8917fe",
   "metadata": {},
   "source": [
    "### **5. Summary: The Complete Data Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea2c1b",
   "metadata": {},
   "source": [
    "Congratulations! You've built a complete data preprocessing pipeline for sequence data. Let's recap what we've accomplished:\n",
    "\n",
    "**1. Text â†’ Tokens**\n",
    "- Downloaded and loaded the Tiny Shakespeare dataset\n",
    "- Implemented word-level tokenization\n",
    "- Built vocabulary mappings (word â†” index)\n",
    "\n",
    "**2. Tokens â†’ Sequences**\n",
    "- Created a custom `ShakespeareDataset` class\n",
    "- Implemented the sliding window approach to generate input-target pairs\n",
    "- Each input predicts the next word at every position\n",
    "\n",
    "**3. Sequences â†’ Batches**\n",
    "- Implemented a custom `collate_fn` to handle variable-length sequences\n",
    "- Added padding to create rectangular tensors\n",
    "- Created attention masks to distinguish real tokens from padding\n",
    "- Built a `DataLoader` for efficient batch processing\n",
    "\n",
    "**What's Next?**\n",
    "\n",
    "Our data pipeline is complete and ready to feed batches to a model. In **Part 2**, we'll:\n",
    "- Build a Recurrent Neural Network (RNN/LSTM/GRU)\n",
    "- Implement the training loop\n",
    "- Watch the model learn to predict the next word\n",
    "\n",
    "The assembly line is readyâ€”let's build the engine!\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Sequence data requires special handling because order matters\n",
    "- Padding allows us to batch variable-length sequences\n",
    "- The sliding window creates training pairs: (input, target)\n",
    "- DataLoaders automate batching and shuffling\n",
    "- Attention masks help models ignore padding tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f7008",
   "metadata": {},
   "source": [
    "### **6. Bonus: Testing with Variable-Length Sequences**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3eca8e",
   "metadata": {},
   "source": [
    "Let's create a modified dataset with truly variable-length sequences to see our padding in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that returns sequences of random lengths to demonstrate padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token_indices, min_length=5, max_length=30, num_samples=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_indices: List of all token indices\n",
    "            min_length: Minimum sequence length\n",
    "            max_length: Maximum sequence length\n",
    "            num_samples: Number of samples to generate\n",
    "        \"\"\"\n",
    "        self.token_indices = token_indices\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Random sequence length for this sample\n",
    "        seq_len = torch.randint(self.min_length, self.max_length + 1, (1,)).item()\n",
    "        \n",
    "        # Random starting position\n",
    "        max_start = len(self.token_indices) - seq_len - 1\n",
    "        start_idx = torch.randint(0, max_start, (1,)).item()\n",
    "        \n",
    "        # Extract sequence\n",
    "        sequence = self.token_indices[start_idx:start_idx + seq_len + 1]\n",
    "        \n",
    "        input_seq = torch.tensor(sequence[:-1], dtype=torch.long)\n",
    "        target_seq = torch.tensor(sequence[1:], dtype=torch.long)\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Create variable-length dataset\n",
    "var_dataset = VariableLengthDataset(token_indices, min_length=5, max_length=25, num_samples=50)\n",
    "var_dataloader = DataLoader(var_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"âœ“ Created variable-length dataset\")\n",
    "print(f\"  - Samples: {len(var_dataset)}\")\n",
    "print(f\"  - Length range: 5-25 tokens\")\n",
    "print(f\"  - Batch size: 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67dd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize variable-length batch\n",
    "var_inputs, var_targets, var_masks = next(iter(var_dataloader))\n",
    "\n",
    "print(\"Variable-length batch analysis:\\n\")\n",
    "print(f\"Batch shape: {var_inputs.shape}\")\n",
    "\n",
    "# Analyze each sequence in the batch\n",
    "print(f\"\\n{'Seq':<5} {'Real Tokens':<12} {'Padding':<10} {'Total':<8}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(var_inputs.shape[0]):\n",
    "    real_tokens = var_masks[i].sum().item()\n",
    "    padding = var_masks[i].shape[0] - real_tokens\n",
    "    total = var_masks[i].shape[0]\n",
    "    print(f\"{i+1:<5} {real_tokens:<12} {padding:<10} {total:<8}\")\n",
    "\n",
    "# Visualize the padding pattern\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(var_inputs.numpy(), cmap='viridis', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Token Index')\n",
    "plt.xlabel('Sequence Position')\n",
    "plt.ylabel('Batch Sample')\n",
    "plt.title('Variable-Length Sequences with Dynamic Padding\\n(Notice different amounts of padding for each sequence)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Padding successfully handles sequences of different lengths!\")\n",
    "print(\"âœ“ The collate_fn dynamically adjusts to the longest sequence in each batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6dcbe",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ **Part 1 Complete!**\n",
    "\n",
    "You've successfully built a robust data preprocessing pipeline for sequence data. You now understand:\n",
    "\n",
    "âœ… How to tokenize text and build vocabularies  \n",
    "âœ… How to create training pairs using sliding windows  \n",
    "âœ… How to handle variable-length sequences with padding  \n",
    "âœ… How to create efficient DataLoaders for batch processing  \n",
    "âœ… The importance of attention masks  \n",
    "\n",
    "**Your data assembly line is production-ready!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26583068",
   "metadata": {},
   "source": [
    "## **Part 2: The Engine â€“ Building and Training the RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60f7d1",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "Construct a recurrent neural network module and train it to minimize cross-entropy loss on the predicted next word.\n",
    "\n",
    "In Part 1, we built the data pipeline. Now we'll build the \"engine\"â€”the RNN model that learns to predict the next word in a sequence. By the end of Part 2, you'll understand:\n",
    "- How RNNs process sequential data\n",
    "- Why we use LSTM/GRU instead of vanilla RNN\n",
    "- How to implement and train a language model\n",
    "- Truncated Backpropagation Through Time (TBPTT)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe948b2f",
   "metadata": {},
   "source": [
    "### **7. Understanding Recurrent Neural Networks (RNNs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94583f2d",
   "metadata": {},
   "source": [
    "**What makes RNNs special?**\n",
    "\n",
    "Unlike the feedforward networks we've seen before, RNNs have **memory**. They process sequences one element at a time while maintaining a **hidden state** that captures information about previous elements.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*AQ52bwW55GsJt6HTxPDuMA.gif\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "**How RNNs work:**\n",
    "\n",
    "At each time step $t$:\n",
    "1. Take the current input $x_t$ (word embedding)\n",
    "2. Take the previous hidden state $h_{t-1}$ (memory from previous words)\n",
    "3. Compute the new hidden state: $h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$\n",
    "4. Compute the output: $y_t = W_{hy}h_t + b_y$\n",
    "\n",
    "The hidden state $h_t$ acts as the network's \"memory\" of what it has seen so far.\n",
    "\n",
    "**The Vanishing Gradient Problem**\n",
    "\n",
    "Traditional RNNs have a critical flaw: when we backpropagate through many time steps, gradients either:\n",
    "- **Vanish** â†’ become too small, preventing learning of long-term dependencies\n",
    "- **Explode** â†’ become too large, causing unstable training\n",
    "\n",
    "This makes it hard for vanilla RNNs to learn relationships between words that are far apart (e.g., \"The cat, which had been sleeping all day, **was** hungry\").\n",
    "\n",
    "**The Solution: LSTM and GRU**\n",
    "\n",
    "**LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)** solve this problem using **gates** that control information flow:\n",
    "- **Forget gate**: What to forget from previous memory\n",
    "- **Input gate**: What new information to store\n",
    "- **Output gate**: What to output from memory\n",
    "\n",
    "These gates allow the network to maintain information over much longer sequences.\n",
    "\n",
    "We'll use **GRU** for our model because it's simpler than LSTM but equally effective for most tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa477b",
   "metadata": {},
   "source": [
    "### **8. Building Our RNN Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57f862",
   "metadata": {},
   "source": [
    "Our RNN model will have three main components:\n",
    "\n",
    "**1. Embedding Layer (`nn.Embedding`)**\n",
    "- Converts word indices to dense vectors\n",
    "- Each word gets a learnable vector representation\n",
    "- Input: `(batch_size, seq_length)` â†’ Output: `(batch_size, seq_length, embedding_dim)`\n",
    "\n",
    "**2. Recurrent Layer (`nn.GRU`)**\n",
    "- Processes the sequence while maintaining hidden state\n",
    "- Captures dependencies between words\n",
    "- Input: `(batch_size, seq_length, embedding_dim)` â†’ Output: `(batch_size, seq_length, hidden_dim)`\n",
    "\n",
    "**3. Output Layer (`nn.Linear`)**\n",
    "- Maps hidden states to vocabulary logits\n",
    "- Produces probability distribution over all possible next words\n",
    "- Input: `(batch_size, seq_length, hidden_dim)` â†’ Output: `(batch_size, seq_length, vocab_size)`\n",
    "\n",
    "Let's implement this architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f49c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RNN Model created!\n",
      "\n",
      "Model architecture:\n",
      "RNNPredictor(\n",
      "  (embedding): Embedding(13391, 128, padding_idx=0)\n",
      "  (gru): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=13391, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 5,846,735\n"
     ]
    }
   ],
   "source": [
    "class RNNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN-based language model for next word prediction.\n",
    "    Uses GRU (Gated Recurrent Unit) to handle long-term dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the RNN model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            embedding_dim: Dimension of word embeddings\n",
    "            hidden_dim: Dimension of hidden state\n",
    "            num_layers: Number of stacked GRU layers\n",
    "            dropout: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super(RNNPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layer 1: Embedding layer - converts word indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Layer 2: GRU layer - processes sequences with memory\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True  # Input shape: (batch, seq, feature)\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer 3: Output layer - maps hidden states to vocabulary logits\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length)\n",
    "            hidden: Previous hidden state (optional)\n",
    "            \n",
    "        Returns:\n",
    "            output: Logits of shape (batch_size, seq_length, vocab_size)\n",
    "            hidden: New hidden state\n",
    "        \"\"\"\n",
    "        # 1. Embed the input words\n",
    "        # Shape: (batch_size, seq_length) â†’ (batch_size, seq_length, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # 2. Pass through GRU\n",
    "        # Shape: (batch_size, seq_length, embedding_dim) â†’ (batch_size, seq_length, hidden_dim)\n",
    "        if hidden is not None:\n",
    "            # Detach hidden state to implement Truncated BPTT\n",
    "            hidden = hidden.detach()\n",
    "        \n",
    "        gru_out, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        # 3. Apply dropout\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        \n",
    "        # 4. Map to vocabulary\n",
    "        # Shape: (batch_size, seq_length, hidden_dim) â†’ (batch_size, seq_length, vocab_size)\n",
    "        output = self.fc(gru_out)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state with zeros.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Size of the batch\n",
    "            \n",
    "        Returns:\n",
    "            Hidden state tensor of shape (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "\n",
    "# Create the model\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "model = RNNPredictor(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "\n",
    "print(\"âœ“ RNN Model created!\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517d286",
   "metadata": {},
   "source": [
    "**Understanding Truncated Backpropagation Through Time (TBPTT)**\n",
    "\n",
    "When training RNNs on long sequences, we face a computational challenge: backpropagating through the entire sequence history would require storing all intermediate states in memory.\n",
    "\n",
    "**The solution:** We **detach** the hidden state between batches:\n",
    "```python\n",
    "if hidden is not None:\n",
    "    hidden = hidden.detach()\n",
    "```\n",
    "\n",
    "This breaks the computational graph, meaning:\n",
    "- âœ… The hidden state carries **information** forward (maintaining memory)\n",
    "- âœ… But gradients **don't** flow back beyond the current batch (saving memory)\n",
    "\n",
    "This technique is called **Truncated Backpropagation Through Time** and is essential for training RNNs on long sequences efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bbc9e",
   "metadata": {},
   "source": [
    "### **9. Setting Up Training: Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64ac2af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training setup complete!\n",
      "\n",
      "Loss function: CrossEntropyLoss (ignoring padding)\n",
      "Optimizer: Adam\n",
      "Learning rate: 0.001\n",
      "\n",
      "Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens (index 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"âœ“ Training setup complete!\")\n",
    "print(f\"\\nLoss function: CrossEntropyLoss (ignoring padding)\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Learning rate: 0.001\")\n",
    "print(f\"\\nReady to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb761a9",
   "metadata": {},
   "source": [
    "### **10. The Training Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a1001",
   "metadata": {},
   "source": [
    "Now let's train our model! The training loop follows these steps for each batch:\n",
    "\n",
    "1. **Forward Pass**: Feed inputs through the model to get predictions\n",
    "2. **Compute Loss**: Compare predictions with targets\n",
    "3. **Backward Pass**: Compute gradients\n",
    "4. **Update Weights**: Apply optimizer step\n",
    "\n",
    "We'll also track the loss over time to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae61f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch    Batch      Loss         Time      \n",
      "---------------------------------------------\n",
      "1        500        3.9188       5.10      s\n",
      "1        1000       3.8831       10.26     s\n",
      "1        1500       3.8586       15.33     s\n",
      "1        2000       3.8371       20.54     s\n",
      "1        2500       3.8174       25.61     s\n",
      "1        3000       3.7975       30.84     s\n",
      "1        3500       3.7800       35.89     s\n",
      "1        4000       3.7626       40.98     s\n",
      "1        4500       3.7469       46.15     s\n",
      "1        5000       3.7307       51.21     s\n",
      "1        5500       3.7160       56.41     s\n",
      "1        6000       3.7010       61.45     s\n",
      "1        6500       3.6869       66.59     s\n",
      "1        7000       3.6738       71.62     s\n",
      "1        7500       3.6613       76.68     s\n",
      "Epoch 1 complete - Avg Loss: 3.6537 - Time: 79.75s\n",
      "---------------------------------------------\n",
      "2        500        3.4008       5.02      s\n",
      "2        1000       3.3998       10.17     s\n",
      "2        1500       3.3927       15.20     s\n",
      "2        2000       3.3892       20.25     s\n",
      "2        2500       3.3841       25.45     s\n",
      "2        3000       3.3791       30.50     s\n",
      "2        3500       3.3743       35.69     s\n",
      "2        4000       3.3686       40.75     s\n",
      "2        4500       3.3626       45.94     s\n",
      "2        5000       3.3573       50.99     s\n",
      "2        5500       3.3518       56.06     s\n",
      "2        6000       3.3472       61.25     s\n",
      "2        6500       3.3419       66.31     s\n",
      "2        7000       3.3369       71.50     s\n",
      "2        7500       3.3313       76.54     s\n",
      "Epoch 2 complete - Avg Loss: 3.3282 - Time: 79.53s\n",
      "---------------------------------------------\n",
      "3        500        3.1947       5.20      s\n",
      "3        1000       3.1943       10.25     s\n",
      "3        1500       3.1952       15.45     s\n",
      "3        2000       3.1919       20.50     s\n",
      "3        2500       3.1898       25.65     s\n",
      "3        3000       3.1877       30.74     s\n",
      "3        3500       3.1855       35.80     s\n",
      "3        4000       3.1829       41.01     s\n",
      "3        4500       3.1804       46.08     s\n",
      "3        5000       3.1777       51.26     s\n",
      "3        5500       3.1750       56.33     s\n",
      "3        6000       3.1729       61.52     s\n",
      "3        6500       3.1700       66.59     s\n",
      "3        7000       3.1670       71.64     s\n",
      "3        7500       3.1633       76.84     s\n",
      "Epoch 3 complete - Avg Loss: 3.1614 - Time: 79.84s\n",
      "---------------------------------------------\n",
      "4        500        3.0664       5.15      s\n",
      "4        1000       3.0654       10.23     s\n",
      "4        1500       3.0676       15.28     s\n",
      "4        2000       3.0665       20.47     s\n",
      "4        2500       3.0659       25.51     s\n",
      "4        3000       3.0654       30.70     s\n",
      "4        3500       3.0637       35.74     s\n",
      "4        4000       3.0624       40.88     s\n",
      "4        4500       3.0607       45.99     s\n",
      "4        5000       3.0587       51.05     s\n",
      "4        5500       3.0573       56.23     s\n",
      "4        6000       3.0556       61.28     s\n",
      "4        6500       3.0534       66.46     s\n",
      "4        7000       3.0514       71.51     s\n",
      "4        7500       3.0493       76.67     s\n",
      "Epoch 4 complete - Avg Loss: 3.0478 - Time: 79.71s\n",
      "---------------------------------------------\n",
      "5        500        2.9640       5.06      s\n",
      "5        1000       2.9632       10.24     s\n",
      "5        1500       2.9654       15.30     s\n",
      "5        2000       2.9670       20.42     s\n",
      "5        2500       2.9685       25.54     s\n",
      "5        3000       2.9686       30.60     s\n",
      "5        3500       2.9681       35.79     s\n",
      "5        4000       2.9674       40.84     s\n",
      "5        4500       2.9672       46.04     s\n",
      "5        5000       2.9668       51.10     s\n",
      "5        5500       2.9645       56.23     s\n",
      "5        6000       2.9641       61.35     s\n",
      "5        6500       2.9621       66.40     s\n",
      "5        7000       2.9612       71.59     s\n",
      "5        7500       2.9602       76.65     s\n",
      "Epoch 5 complete - Avg Loss: 2.9592 - Time: 79.70s\n",
      "---------------------------------------------\n",
      "6        500        2.8840       5.15      s\n",
      "6        1000       2.8879       10.20     s\n",
      "6        1500       2.8922       15.39     s\n",
      "6        2000       2.8952       20.45     s\n",
      "6        2500       2.8948       25.63     s\n",
      "6        3000       2.8953       30.69     s\n",
      "6        3500       2.8951       35.81     s\n",
      "6        4000       2.8943       40.93     s\n",
      "6        4500       2.8936       45.99     s\n",
      "6        5000       2.8923       51.17     s\n",
      "6        5500       2.8911       56.23     s\n",
      "6        6000       2.8900       61.41     s\n",
      "6        6500       2.8892       66.47     s\n",
      "6        7000       2.8882       71.60     s\n",
      "6        7500       2.8871       76.71     s\n",
      "Epoch 6 complete - Avg Loss: 2.8867 - Time: 79.71s\n",
      "---------------------------------------------\n",
      "7        500        2.8204       5.18      s\n",
      "7        1000       2.8276       10.23     s\n",
      "7        1500       2.8302       15.32     s\n",
      "7        2000       2.8315       20.47     s\n",
      "7        2500       2.8311       25.53     s\n",
      "7        3000       2.8312       30.71     s\n",
      "7        3500       2.8310       35.76     s\n",
      "7        4000       2.8308       40.95     s\n",
      "7        4500       2.8310       46.00     s\n",
      "7        5000       2.8300       51.11     s\n",
      "7        5500       2.8289       56.23     s\n",
      "7        6000       2.8274       61.29     s\n",
      "7        6500       2.8267       66.46     s\n",
      "7        7000       2.8260       71.52     s\n",
      "7        7500       2.8247       76.71     s\n",
      "Epoch 7 complete - Avg Loss: 2.8242 - Time: 79.70s\n",
      "---------------------------------------------\n",
      "8        500        2.7622       5.05      s\n",
      "8        1000       2.7670       10.23     s\n",
      "8        1500       2.7679       15.29     s\n",
      "8        2000       2.7711       20.47     s\n",
      "8        2500       2.7725       25.52     s\n",
      "8        3000       2.7734       30.62     s\n",
      "8        3500       2.7735       35.77     s\n",
      "8        4000       2.7736       40.84     s\n",
      "8        4500       2.7734       46.02     s\n",
      "8        5000       2.7728       51.07     s\n",
      "8        5500       2.7724       56.25     s\n",
      "8        6000       2.7727       61.32     s\n",
      "8        6500       2.7729       66.40     s\n",
      "8        7000       2.7723       71.53     s\n",
      "8        7500       2.7716       76.57     s\n",
      "Epoch 8 complete - Avg Loss: 2.7715 - Time: 79.69s\n",
      "---------------------------------------------\n",
      "9        500        2.7098       5.05      s\n",
      "9        1000       2.7146       10.11     s\n",
      "9        1500       2.7196       15.27     s\n",
      "9        2000       2.7206       20.31     s\n",
      "9        2500       2.7254       25.49     s\n",
      "9        3000       2.7265       30.53     s\n",
      "9        3500       2.7274       35.70     s\n",
      "9        4000       2.7272       40.76     s\n",
      "9        4500       2.7272       45.86     s\n",
      "9        5000       2.7266       51.02     s\n",
      "9        5500       2.7259       56.07     s\n",
      "9        6000       2.7256       61.27     s\n",
      "9        6500       2.7253       66.30     s\n",
      "9        7000       2.7247       71.51     s\n",
      "9        7500       2.7243       76.57     s\n",
      "Epoch 9 complete - Avg Loss: 2.7239 - Time: 79.56s\n",
      "---------------------------------------------\n",
      "10       500        2.6656       5.19      s\n",
      "10       1000       2.6694       10.24     s\n",
      "10       1500       2.6723       15.44     s\n",
      "10       2000       2.6721       20.49     s\n",
      "10       2500       2.6758       25.57     s\n",
      "10       3000       2.6763       30.77     s\n",
      "10       3500       2.6787       35.83     s\n",
      "10       4000       2.6805       41.02     s\n",
      "10       4500       2.6815       46.08     s\n",
      "10       5000       2.6822       51.26     s\n",
      "10       5500       2.6820       56.33     s\n",
      "10       6000       2.6821       61.40     s\n",
      "10       6500       2.6826       66.55     s\n",
      "10       7000       2.6819       71.60     s\n",
      "10       7500       2.6818       76.80     s\n",
      "Epoch 10 complete - Avg Loss: 2.6814 - Time: 79.79s\n",
      "---------------------------------------------\n",
      "\n",
      "âœ“ Training complete!\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train the RNN model.\n",
    "    \n",
    "    Args:\n",
    "        model: The RNN model to train\n",
    "        dataloader: DataLoader providing batches\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer for updating weights\n",
    "        num_epochs: Number of training epochs\n",
    "        \n",
    "    Returns:\n",
    "        loss_history: List of average losses per epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    loss_history = []\n",
    "    \n",
    "    print(\"Starting training...\\n\")\n",
    "    print(f\"{'Epoch':<8} {'Batch':<10} {'Loss':<12} {'Time':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, masks) in enumerate(dataloader):\n",
    "            hidden = None  # Initialize hidden state\n",
    "\n",
    "            # Move data to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            # outputs: (batch_size, seq_length, vocab_size)\n",
    "            # targets: (batch_size, seq_length)\n",
    "            # CrossEntropyLoss expects: (batch_size * seq_length, vocab_size)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Print progress every 500 batches\n",
    "            if (batch_idx + 1) % 500 == 0:\n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "                elapsed = time.time() - epoch_start_time\n",
    "                print(f\"{epoch+1:<8} {batch_idx+1:<10} {avg_loss:<12.4f} {elapsed:<10.2f}s\")\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_epoch_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_epoch_loss)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1} complete - Avg Loss: {avg_epoch_loss:.4f} - Time: {epoch_time:.2f}s\")\n",
    "        print(\"-\" * 45)\n",
    "    \n",
    "    print(\"\\nâœ“ Training complete!\")\n",
    "    return loss_history\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "loss_history = train_model(model, dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "977440ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final training loss: 2.6814\n",
      "Loss improvement: 0.9724\n"
     ]
    }
   ],
   "source": [
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), loss_history, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal training loss: {loss_history[-1]:.4f}\")\n",
    "print(f\"Loss improvement: {loss_history[0] - loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5e22d",
   "metadata": {},
   "source": [
    "### **11. Saving the Trained Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12143a07",
   "metadata": {},
   "source": [
    "Now that we've trained our model, let's save it so we can use it later for text generation without retraining.\n",
    "\n",
    "We'll save:\n",
    "- The model's learned parameters (weights and biases)\n",
    "- The vocabulary mappings (word2idx and idx2word)\n",
    "- Model hyperparameters for reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1001dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model saved to shakespeare_rnn_model.pth\n",
      "\n",
      "Checkpoint contains:\n",
      "  - Model weights and biases\n",
      "  - Optimizer state\n",
      "  - Vocabulary mappings (13,391 words)\n",
      "  - Training history (10 epochs)\n",
      "\n",
      "Model can now be loaded for inference without retraining!\n"
     ]
    }
   ],
   "source": [
    "# Save the model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'word2idx': word2idx,\n",
    "    'idx2word': idx2word,\n",
    "    'loss_history': loss_history\n",
    "}\n",
    "\n",
    "checkpoint_path = 'shakespeare_rnn_model.pth'\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(f\"âœ“ Model saved to {checkpoint_path}\")\n",
    "print(f\"\\nCheckpoint contains:\")\n",
    "print(f\"  - Model weights and biases\")\n",
    "print(f\"  - Optimizer state\")\n",
    "print(f\"  - Vocabulary mappings ({vocab_size:,} words)\")\n",
    "print(f\"  - Training history ({len(loss_history)} epochs)\")\n",
    "print(f\"\\nModel can now be loaded for inference without retraining!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee308641",
   "metadata": {},
   "source": [
    "### **12. Summary: The Complete RNN Engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9afa6",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully built and trained a Recurrent Neural Network for language modeling. Let's recap what we've accomplished:\n",
    "\n",
    "**1. Understanding RNNs**\n",
    "- Learned how RNNs maintain memory through hidden states\n",
    "- Understood the vanishing gradient problem\n",
    "- Discovered why LSTM/GRU are superior to vanilla RNNs\n",
    "\n",
    "**2. Building the Architecture**\n",
    "- Implemented an `RNNPredictor` with three layers:\n",
    "  - Embedding layer (words â†’ vectors)\n",
    "  - GRU layer (sequential processing with memory)\n",
    "  - Linear layer (hidden states â†’ vocabulary logits)\n",
    "\n",
    "**3. Training the Model**\n",
    "- Used CrossEntropyLoss for next-word prediction\n",
    "- Implemented Truncated Backpropagation Through Time (TBPTT)\n",
    "- Applied gradient clipping to prevent exploding gradients\n",
    "- Tracked training progress and visualized loss curves\n",
    "\n",
    "**4. Saving for Later**\n",
    "- Checkpointed the trained model\n",
    "- Saved vocabulary mappings for inference\n",
    "\n",
    "**What's Next?**\n",
    "\n",
    "Our RNN engine is trained and ready! In **Part 3**, we'll:\n",
    "- Generate Shakespeare-like text using our trained model\n",
    "- Explore temperature sampling for creativity\n",
    "- Analyze where the RNN succeeds (local grammar, style)\n",
    "- Discover the critical \"bottleneck\" problem that motivates Transformers\n",
    "\n",
    "The engine is runningâ€”let's see what it can create!\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways:**\n",
    "- RNNs process sequences while maintaining hidden state (memory)\n",
    "- GRU/LSTM gates solve the vanishing gradient problem\n",
    "- Truncated BPTT makes training efficient on long sequences\n",
    "- Gradient clipping prevents training instability\n",
    "- Cross-entropy loss is perfect for classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcec679",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ **Part 2 Complete!**\n",
    "\n",
    "You've successfully built and trained an RNN language model! You now understand:\n",
    "\n",
    "âœ… How RNNs process sequential data with memory  \n",
    "âœ… Why GRU/LSTM are essential for long-term dependencies  \n",
    "âœ… How to implement a complete language model architecture  \n",
    "âœ… Truncated Backpropagation Through Time (TBPTT)  \n",
    "âœ… Training techniques: gradient clipping, dropout, loss tracking  \n",
    "\n",
    "**Your RNN engine is trained and ready to generate text!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f53bd",
   "metadata": {},
   "source": [
    "## **Part 3: The Critique â€“ Generation, Analysis, and Limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc33b1",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "Generate text to verify learning, then critically analyze where the RNN fails to prepare for Transformers.\n",
    "\n",
    "In Parts 1 and 2, we built and trained our RNN. Now it's time to put it to the test! By the end of Part 3, you'll:\n",
    "- Generate Shakespeare-like text using the trained model\n",
    "- Understand temperature sampling and creativity control\n",
    "- Discover what RNNs do well (local patterns, grammar, style)\n",
    "- Uncover the critical **bottleneck problem** that motivates Transformers\n",
    "\n",
    "This is where we transition from \"RNNs are cool\" to \"why we need Transformers\"!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbcea4",
   "metadata": {},
   "source": [
    "### **13. Loading the Trained Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559b4f0",
   "metadata": {},
   "source": [
    "Before we can generate text, we need to load our trained model from Part 2. We'll reconstruct the model architecture and load the saved weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88ce5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model class defined\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN model class (same as Part 2)\n",
    "class RNNPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
    "        super(RNNPredictor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                          dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        if hidden is not None:\n",
    "            hidden = hidden.detach()\n",
    "        gru_out, hidden = self.gru(embedded, hidden)\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        output = self.fc(gru_out)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "\n",
    "print(\"âœ“ Model class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8437d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded checkpoint from shakespeare_rnn_model.pth\n",
      "\n",
      "Model configuration:\n",
      "  - Vocabulary size: 13,391\n",
      "  - Embedding dimension: 128\n",
      "  - Hidden dimension: 256\n",
      "  - Number of layers: 2\n"
     ]
    }
   ],
   "source": [
    "# Load the saved checkpoint\n",
    "checkpoint_path = 'shakespeare_rnn_model.pth'\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"âœ“ Loaded checkpoint from {checkpoint_path}\")\n",
    "    \n",
    "    # Extract saved information\n",
    "    vocab_size = checkpoint['vocab_size']\n",
    "    embedding_dim = checkpoint['embedding_dim']\n",
    "    hidden_dim = checkpoint['hidden_dim']\n",
    "    num_layers = checkpoint['num_layers']\n",
    "    word2idx = checkpoint['word2idx']\n",
    "    idx2word = checkpoint['idx2word']\n",
    "    \n",
    "    print(f\"\\nModel configuration:\")\n",
    "    print(f\"  - Vocabulary size: {vocab_size:,}\")\n",
    "    print(f\"  - Embedding dimension: {embedding_dim}\")\n",
    "    print(f\"  - Hidden dimension: {hidden_dim}\")\n",
    "    print(f\"  - Number of layers: {num_layers}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Model file '{checkpoint_path}' not found!\")\n",
    "    print(\"Please run Part 2 to train and save the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f88f4dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded and ready for inference!\n",
      "\n",
      "Total parameters: 5,846,735\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model and load weights\n",
    "model = RNNPredictor(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"âœ“ Model loaded and ready for inference!\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dbe0ae",
   "metadata": {},
   "source": [
    "### **14. Text Generation: Auto-regressive Sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a915f0a",
   "metadata": {},
   "source": [
    "**How Text Generation Works**\n",
    "\n",
    "Text generation is an **auto-regressive process**:\n",
    "\n",
    "1. Start with a \"seed\" text (e.g., \"The king said\")\n",
    "2. Feed it through the model to predict the next word\n",
    "3. Add the predicted word to the sequence\n",
    "4. Feed the extended sequence back into the model\n",
    "5. Repeat until we reach the desired length\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://substackcdn.com/image/fetch/$s_!dLfh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943efc65-c436-4914-b6f9-13c39a8cedc5_1896x2109.jpeg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**The Greedy Problem**\n",
    "\n",
    "If we always pick the most likely word, the text becomes repetitive and boring:\n",
    "```\n",
    "\"The king said the king said the king said...\"\n",
    "```\n",
    "\n",
    "**Temperature Sampling: Controlling Creativity**\n",
    "\n",
    "Instead of always picking the most probable word, we use **temperature sampling** to introduce controlled randomness:\n",
    "\n",
    "$$P_{\\text{temp}}(w_i) = \\frac{\\exp(\\text{logit}_i / T)}{\\sum_j \\exp(\\text{logit}_j / T)}$$\n",
    "\n",
    "Where temperature $T$ controls randomness:\n",
    "- **T = 0.5**: More deterministic, safer choices (conservative)\n",
    "- **T = 1.0**: Balanced creativity and coherence\n",
    "- **T = 1.5**: More random, creative but potentially less coherent\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26dfcc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Text generation function ready!\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_text, word2idx, idx2word, max_length=100, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using the trained RNN model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained RNN model\n",
    "        seed_text: Starting text to continue from\n",
    "        word2idx: Dictionary mapping words to indices\n",
    "        idx2word: Dictionary mapping indices to words\n",
    "        max_length: Maximum number of words to generate\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        generated_text: The complete generated text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize seed text\n",
    "    def tokenize(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'([.,!?;:])', r' \\1 ', text)\n",
    "        return text.split()\n",
    "    \n",
    "    words = tokenize(seed_text)\n",
    "    \n",
    "    # Convert to indices\n",
    "    input_seq = [word2idx.get(word, word2idx['<UNK>']) for word in words]\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # Generate words one by one\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert to tensor\n",
    "            x = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, hidden = model(x, hidden)\n",
    "            \n",
    "            # Get logits for the last word\n",
    "            logits = output[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = torch.softmax(logits, dim=0)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            next_word_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            # Skip padding token\n",
    "            if next_word_idx == 0:\n",
    "                continue\n",
    "            \n",
    "            # Add to sequence\n",
    "            input_seq.append(next_word_idx)\n",
    "            \n",
    "            # Stop if we generate end of sequence\n",
    "            if next_word_idx == word2idx.get('<EOS>', -1):\n",
    "                break\n",
    "    \n",
    "    # Convert indices back to words\n",
    "    generated_words = [idx2word[idx] for idx in input_seq]\n",
    "    \n",
    "    # Join words (handle punctuation)\n",
    "    result = []\n",
    "    for word in generated_words:\n",
    "        if word in '.,!?;:':\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(' ' + word)\n",
    "    \n",
    "    return ''.join(result).strip()\n",
    "\n",
    "print(\"âœ“ Text generation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a72a2e",
   "metadata": {},
   "source": [
    "### **15. Testing the Model: Shakespeare-Style Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317c002",
   "metadata": {},
   "source": [
    "Let's generate some text with different seed phrases and temperatures to see what our model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc44f807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATED TEXT SAMPLES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SEED: 'To be or not to be'\n",
      "================================================================================\n",
      "\n",
      "Temperature = 0.5:\n",
      "--------------------------------------------------------------------------------\n",
      "to be or not to be talked on, and yet them in fellow. brutus: nay, but i have done a better thing. gloucester: when i have said, my lord, i am glad to receive. king richard iii: o, tut, thou troublest me; for\n",
      "\n",
      "Temperature = 1.0:\n",
      "--------------------------------------------------------------------------------\n",
      "to be or not to be talked withal too; but 'twas you known the course of bed. hortensio: petruchio, ay, or an old rusty love gremio? biondello: therefore, tell thee, they was almost like a child, as i was a, to find you a sound\n",
      "\n",
      "Temperature = 1.5:\n",
      "--------------------------------------------------------------------------------\n",
      "to be or not to be thought and go with the worst to take mine as a prisoner in the affection of, it doth court and every below a father without it! or, if thou give i darest; 'tis thine 'not sad: but look that she will love you in any\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SEED: 'The king said'\n",
      "================================================================================\n",
      "\n",
      "Temperature = 0.5:\n",
      "--------------------------------------------------------------------------------\n",
      "the king said his son was his brother slain; and, in the official marks invested, you anon will meet you with the people's voice, and you shall find your safety manifested. volumnia: i prithee, tell me, what is the matter? clown: i have\n",
      "\n",
      "Temperature = 1.0:\n",
      "--------------------------------------------------------------------------------\n",
      "the king said how he would came to come to sandal, he would have done thyself and my brother. first citizen: i have done the charge of your husband, hath made you melancholy a sword of my sorrow, to soothe the sword of it to be the day\n",
      "\n",
      "Temperature = 1.5:\n",
      "--------------------------------------------------------------------------------\n",
      "the king said was virtuous home. that power, i would some slain that would is not the king, then, even by as what i will before a stale. hath all name and born; his dead was full of day you must be saying himself should make you\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SEED: 'What is thy name'\n",
      "================================================================================\n",
      "\n",
      "Temperature = 0.5:\n",
      "--------------------------------------------------------------------------------\n",
      "what is thy name? why, then, i pray, you may not, sir, so shall you be in: but the time is here. hortensio: what is the matter? biondello: why, sir, i know you: and now, sir, that you\n",
      "\n",
      "Temperature = 1.0:\n",
      "--------------------------------------------------------------------------------\n",
      "what is thy name? york: you are not to say, as i true hastings shall be contented. gloucester: o, sweet, fair lord, i'll have no grace of to be due of this royal person. gloucester: if thou wilt outstrip life, and let me\n",
      "\n",
      "Temperature = 1.5:\n",
      "--------------------------------------------------------------------------------\n",
      "what is thy name should end thee dead and full of whose son doth meet both and virtuous, that perform which ta'en that would marcius hardly we at without cominius, he's to lay them down on my soldiers until nothing but to god be against him, so am in aside but\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SEED: 'Good morning'\n",
      "================================================================================\n",
      "\n",
      "Temperature = 0.5:\n",
      "--------------------------------------------------------------------------------\n",
      "good morning. king edward iv: but whither is your name? why, courage, my lord, what news? what is our walls, and harsh our own for the table. second citizen: i know it, and not have done. brutus: i know\n",
      "\n",
      "Temperature = 1.0:\n",
      "--------------------------------------------------------------------------------\n",
      "good morning had you stolen and to the fight, and to be absolved. nurse: what is your name? woman: his name is the prince's mine son, the county paris, at saint peter's church. perdita: if he be ready, sir; and pray\n",
      "\n",
      "Temperature = 1.5:\n",
      "--------------------------------------------------------------------------------\n",
      "good morning wagoner either, by time most i; therefore let me speak, like a chaos in pox o' these fish; and i am not a wish an tyrant or a quarrel: and i have seen a swan with merry some mercy; and which i devise so\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with different seed texts and temperatures\n",
    "seed_texts = [\n",
    "    \"To be or not to be\",\n",
    "    \"The king said\",\n",
    "    \"What is thy name\",\n",
    "    \"Good morning\"\n",
    "]\n",
    "\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATED TEXT SAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for seed in seed_texts:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SEED: '{seed}'\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"Temperature = {temp}:\")\n",
    "        print(\"-\" * 80)\n",
    "        generated = generate_text(model, seed, word2idx, idx2word, max_length=50, temperature=temp)\n",
    "        print(generated)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e2ff8",
   "metadata": {},
   "source": [
    "### **16. Evaluating Success: What the RNN Learned**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8870cde3",
   "metadata": {},
   "source": [
    "**What RNNs Do Well: Local Structure** âœ…\n",
    "\n",
    "Looking at the generated text, you should notice that the RNN has learned:\n",
    "\n",
    "1. **Grammar and Syntax**\n",
    "   - Proper sentence structure (subject-verb-object)\n",
    "   - Correct use of articles (\"a\", \"the\")\n",
    "   - Proper verb conjugation\n",
    "\n",
    "2. **Shakespearean Style**\n",
    "   - Archaic words like \"thou\", \"thy\", \"hath\"\n",
    "   - Poetic phrasing and rhythm\n",
    "   - Appropriate vocabulary for the time period\n",
    "\n",
    "3. **Punctuation**\n",
    "   - Commas, periods, and question marks in reasonable places\n",
    "   - Sentence boundaries\n",
    "\n",
    "4. **Short-Range Dependencies**\n",
    "   - Words that make sense together in nearby context\n",
    "   - Proper noun-pronoun agreement within a few words\n",
    "\n",
    "**Example of Success:**\n",
    "```\n",
    "\"To be or not to be, that is the question of my heart.\"\n",
    "```\n",
    "This shows proper grammar, style, and local coherence!\n",
    "\n",
    "**Why This Works:**\n",
    "RNNs maintain a hidden state that captures recent context, making them excellent at learning local patterns within ~10-20 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d46d2",
   "metadata": {},
   "source": [
    "### **17. The Critical Bottleneck: Long-Range Dependencies** âŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ac576",
   "metadata": {},
   "source": [
    "**The Experiment: Testing Long-Range Memory**\n",
    "\n",
    "Now let's conduct a critical experiment to expose the RNN's fundamental limitation. We'll create sequences where important information at the beginning must be remembered much later.\n",
    "\n",
    "**Example Structure:**\n",
    "```\n",
    "\"The king, who ruled for many years with wisdom and justice, \n",
    " and who was loved by all his subjects throughout the realm, \n",
    " eventually [VERB]\"\n",
    "```\n",
    "\n",
    "The subject \"king\" is at position 1, but the verb should agree with it at position 30+. Can the RNN remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test long-range dependency with increasingly long contexts\n",
    "long_range_seeds = [\n",
    "    \"The king\",\n",
    "    \"The king, who was wise,\",\n",
    "    \"The king, who was wise and just, and who ruled the land,\",\n",
    "    \"The king, who was wise and just, and who ruled the land with grace and honor, and who was loved by all,\",\n",
    "    \"The king, who was wise and just, and who ruled the land with grace and honor, and who was loved by all his people, and who fought many battles,\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING LONG-RANGE DEPENDENCIES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAs the seed gets longer, watch how the model struggles to maintain coherence!\\n\")\n",
    "\n",
    "for i, seed in enumerate(long_range_seeds, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST {i}: Seed length = {len(seed.split())} words\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"SEED: {seed}\")\n",
    "    print(f\"\\nGENERATED:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    generated = generate_text(model, seed, word2idx, idx2word, max_length=30, temperature=1.0)\n",
    "    print(generated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0322f",
   "metadata": {},
   "source": [
    "**Analyzing the Failure**\n",
    "\n",
    "You should notice that as the seed gets longer, the generated text:\n",
    "- Loses connection to the original subject\n",
    "- Starts new topics unrelated to \"the king\"\n",
    "- May even contradict earlier information\n",
    "- Becomes less coherent overall\n",
    "\n",
    "**Why This Happens: The Sequential Bottleneck**\n",
    "\n",
    "RNNs process sequences **sequentially**, one token at a time:\n",
    "\n",
    "$$h_1 \\rightarrow h_2 \\rightarrow h_3 \\rightarrow \\cdots \\rightarrow h_{30}$$\n",
    "\n",
    "At each step, the hidden state must:\n",
    "1. Remember everything important from the past\n",
    "2. Incorporate the new word\n",
    "3. Pass updated information forward\n",
    "\n",
    "**The problem:** Even with LSTM/GRU gates, information from $h_1$ gets progressively **diluted** as it flows through 30+ time steps. The signal weakens, like a game of telephone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df8f26",
   "metadata": {},
   "source": [
    "### **18. Visualizing the Bottleneck Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a182a4b",
   "metadata": {},
   "source": [
    "Let's create a visual representation of how information flows (and degrades) through an RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize information flow in RNN\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Simulate information decay over time steps\n",
    "time_steps = np.arange(1, 51)\n",
    "information_retained = 100 * np.exp(-0.05 * time_steps)  # Exponential decay\n",
    "\n",
    "ax1.plot(time_steps, information_retained, linewidth=3, color='#E74C3C')\n",
    "ax1.fill_between(time_steps, 0, information_retained, alpha=0.3, color='#E74C3C')\n",
    "ax1.axhline(y=20, color='green', linestyle='--', linewidth=2, label='Minimum useful information')\n",
    "ax1.set_xlabel('Time Step (Token Position)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Information Retained (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('RNN Information Decay: The Bottleneck Problem', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_ylim([0, 105])\n",
    "\n",
    "# Annotate critical regions\n",
    "ax1.annotate('Strong Memory\\n(Recent tokens)', xy=(5, 90), fontsize=10, \n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "ax1.annotate('Fading Memory\\n(Moderate distance)', xy=(20, 40), fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "ax1.annotate('Forgotten!\\n(Long distance)', xy=(40, 10), fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "# Comparison: RNN vs Ideal\n",
    "ax2.plot(time_steps, information_retained, linewidth=3, color='#E74C3C', label='RNN (Sequential)', marker='o', markersize=4)\n",
    "ideal_information = np.ones_like(time_steps) * 95  # Constant information\n",
    "ax2.plot(time_steps, ideal_information, linewidth=3, color='#3498DB', label='Ideal (Transformers)', linestyle='--', marker='s', markersize=4)\n",
    "ax2.set_xlabel('Time Step (Token Position)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Information Retained (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('RNN vs Transformers: Information Access', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHT: The Bottleneck Problem\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“‰ RNN: Information decays as it passes through sequential time steps\")\n",
    "print(\"   â€¢ Token 1's information is diluted by the time we reach token 50\")\n",
    "print(\"   â€¢ The hidden state becomes a bottleneckâ€”it must compress everything!\")\n",
    "print(\"\\nðŸ“ˆ Transformers (Preview): Direct access to all tokens simultaneously\")\n",
    "print(\"   â€¢ Every token can 'attend' to every other token directly\")\n",
    "print(\"   â€¢ No information decay over distance!\")\n",
    "print(\"\\nðŸ’¡ This is why we need ATTENTION mechanisms â†’ Coming in Day 7!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567be1d",
   "metadata": {},
   "source": [
    "### **19. Summary: The Complete Journey**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075df860",
   "metadata": {},
   "source": [
    "ðŸŽ‰ **Congratulations!** You've completed the full RNN journey from data to generation to critical analysis.\n",
    "\n",
    "**What We've Accomplished:**\n",
    "\n",
    "**Part 1: The Assembly Line**\n",
    "- âœ… Built a complete data preprocessing pipeline\n",
    "- âœ… Implemented tokenization and vocabulary building\n",
    "- âœ… Created sliding windows for sequence learning\n",
    "- âœ… Handled variable-length sequences with padding\n",
    "\n",
    "**Part 2: The Engine**\n",
    "- âœ… Built and trained a GRU-based RNN\n",
    "- âœ… Understood truncated backpropagation\n",
    "- âœ… Applied gradient clipping for stability\n",
    "- âœ… Saved the trained model\n",
    "\n",
    "**Part 3: The Critique**\n",
    "- âœ… Generated Shakespeare-like text\n",
    "- âœ… Experimented with temperature sampling\n",
    "- âœ… Analyzed what RNNs learn well (local patterns)\n",
    "- âœ… **Discovered the critical bottleneck problem**\n",
    "\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "| What RNNs Excel At âœ… | What RNNs Struggle With âŒ |\n",
    "|----------------------|---------------------------|\n",
    "| Local grammar and syntax | Long-range dependencies |\n",
    "| Short-term context (5-20 tokens) | Context beyond 30+ tokens |\n",
    "| Style and tone | Maintaining coherence in long text |\n",
    "| Sequential patterns | Parallel information flow |\n",
    "\n",
    "\n",
    "**The Motivation for Transformers:**\n",
    "\n",
    "RNNs force all information through a sequential bottleneck (the hidden state). As sequences get longer, information from early tokens fades away. We need a mechanism that:\n",
    "\n",
    "1. âš¡ Allows **parallel processing** (faster training)\n",
    "2. ðŸ”— Provides **direct connections** between distant tokens\n",
    "3. ðŸŽ¯ Enables **selective attention** to relevant information\n",
    "\n",
    "**This is exactly what Transformers and Attention mechanisms provide!**\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
