{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173cb049",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 1: Setup and Data Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebbc96",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Import necessary libraries and load the Tiny Shakespeare dataset.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. Import the following libraries:\n",
    "   - `torch` and `torch.nn`\n",
    "   - `torch.utils.data` (Dataset, DataLoader)\n",
    "   - `numpy`, `matplotlib.pyplot`\n",
    "   - `urllib.request` for downloading data\n",
    "   - `time` for tracking training time\n",
    "\n",
    "2. Check if GPU is available and set the device\n",
    "\n",
    "3. Download the Tiny Shakespeare dataset from:\n",
    "   - URL: `'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'`\n",
    "   - Save it as `'tinyshakespeare.txt'`\n",
    "\n",
    "4. Load and print:\n",
    "   - Total number of characters\n",
    "   - First 500 characters\n",
    "\n",
    "### üí° Hints\n",
    "- Use `urllib.request.urlretrieve(url, filename)` to download\n",
    "- Use `open(filename, 'r', encoding='utf-8')` to read the file\n",
    "- Check device with: `'cuda' if torch.cuda.is_available() else 'cpu'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a71a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all necessary libraries\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check device (GPU or CPU)\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download the dataset\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f89d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the text and print statistics\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce4db0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 2: Character-Level Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702bb34",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Build vocabulary mappings for characters instead of words.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Get all unique characters** in the text:\n",
    "   - Use `sorted(set(text))` to get unique characters\n",
    "   - Create a list called `chars`\n",
    "\n",
    "2. **Build character mappings**:\n",
    "   - `char2idx`: Dictionary mapping each character to a unique index\n",
    "   - `idx2char`: Dictionary mapping each index back to its character\n",
    "   - Remember: dictionaries use `{key: value}` syntax\n",
    "\n",
    "3. **Calculate vocabulary size**:\n",
    "   - Store in variable `vocab_size`\n",
    "\n",
    "4. **Convert text to indices**:\n",
    "   - Create a list `data` containing the index for each character\n",
    "   - Use list comprehension: `[char2idx[ch] for ch in text]`\n",
    "\n",
    "5. **Print information**:\n",
    "   - Vocabulary size\n",
    "   - First 20 characters and their indices\n",
    "   - Sample of the character-to-index mapping\n",
    "\n",
    "### üí° Hints\n",
    "- For `char2idx`: `{ch: i for i, ch in enumerate(chars)}`\n",
    "- For `idx2char`: `{i: ch for i, ch in enumerate(chars)}`\n",
    "- No special tokens needed for character-level! (Unlike word-level with `<PAD>`, `<UNK>`, etc.)\n",
    "\n",
    "### ü§î Think About It\n",
    "How many unique characters do you expect? Compare with word-level vocabulary (thousands of words)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get all unique characters\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da90d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build char2idx and idx2char mappings\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898fa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert entire text to indices\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8da506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print vocabulary information\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452414f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 3: Creating the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45f1bc",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Build a PyTorch Dataset that creates character sequence pairs for training.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "Create a class `CharDataset` that inherits from `torch.utils.data.Dataset`:\n",
    "\n",
    "1. **`__init__` method**:\n",
    "   - Parameters: `data` (list of character indices), `seq_length` (how many characters to use as input)\n",
    "   - Store both parameters as instance variables\n",
    "   - Calculate `num_sequences` = total possible sequences you can create\n",
    "\n",
    "2. **`__len__` method**:\n",
    "   - Return the total number of sequences\n",
    "\n",
    "3. **`__getitem__` method**:\n",
    "   - Input: `idx` (which sequence to get)\n",
    "   - Extract a window of `seq_length + 1` characters starting at position `idx`\n",
    "   - Split into:\n",
    "     - `input_seq`: First `seq_length` characters (as torch.long tensor)\n",
    "     - `target`: The LAST character only (as torch.long tensor)\n",
    "   - Return both\n",
    "\n",
    "### üí° Hints\n",
    "- For slicing: `data[idx:idx + seq_length + 1]`\n",
    "- Split: `input_seq = sequence[:-1]`, `target = sequence[-1]`\n",
    "- Convert to tensor: `torch.tensor(..., dtype=torch.long)`\n",
    "\n",
    "### ü§î Think About It\n",
    "**Example:** If text is \"HELLO\" and seq_length=3:\n",
    "- Sample 0: Input=\"HEL\", Target=\"L\"\n",
    "- Sample 1: Input=\"ELL\", Target=\"O\"\n",
    "\n",
    "After creating the class, instantiate it with `seq_length=100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create CharDataset class\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d25cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create dataset instance with seq_length=100\n",
    "# Print dataset size and show a few examples\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc7a55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 4: Creating the DataLoader**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42217ff0",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Set up a DataLoader to batch and shuffle the data efficiently.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. Create a `DataLoader` with:\n",
    "   - Your dataset from Section 3\n",
    "   - `batch_size = 64`\n",
    "   - `shuffle = True`\n",
    "   - `drop_last = True` (drops incomplete batches)\n",
    "\n",
    "2. Test the DataLoader:\n",
    "   - Get one batch using `iter()` and `next()`\n",
    "   - Print the shapes of inputs and targets\n",
    "   - Convert a few examples back to characters to verify correctness\n",
    "\n",
    "### üí° Hints\n",
    "- `dataloader = DataLoader(dataset, batch_size=..., shuffle=..., drop_last=...)`\n",
    "- Get batch: `batch_iter = iter(dataloader)`, `inputs, targets = next(batch_iter)`\n",
    "- Convert back: `''.join([idx2char[idx.item()] for idx in input_seq])`\n",
    "\n",
    "### ü§î Expected Shapes\n",
    "- Inputs: `(batch_size, seq_length)` ‚Üí `(64, 100)`\n",
    "- Targets: `(batch_size,)` ‚Üí `(64,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create DataLoader\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the DataLoader - get one batch and examine it\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153dc91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 5: Building the RNN Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21a584",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Create a character-level RNN for next character prediction.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "Build a class `CharRNN` that inherits from `nn.Module`:\n",
    "\n",
    "1. **`__init__` method** - Initialize layers:\n",
    "   - Parameters: `vocab_size`, `embedding_dim`, `hidden_dim`, `num_layers`\n",
    "   - Create:\n",
    "     - `self.embedding`: `nn.Embedding(vocab_size, embedding_dim)`\n",
    "     - `self.lstm`: `nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)`\n",
    "     - `self.dropout`: `nn.Dropout(0.2)`\n",
    "     - `self.fc`: `nn.Linear(hidden_dim, vocab_size)`\n",
    "   - Store `hidden_dim` and `num_layers` as instance variables\n",
    "\n",
    "2. **`forward` method**:\n",
    "   - Input: `x` (input sequence), `hidden` (previous hidden state, optional)\n",
    "   - Steps:\n",
    "     1. Embed the input: `embedded = self.embedding(x)`\n",
    "     2. Pass through LSTM: `lstm_out, hidden = self.lstm(embedded, hidden)`\n",
    "     3. Take ONLY the last time step: `last_output = lstm_out[:, -1, :]`\n",
    "     4. Apply dropout: `last_output = self.dropout(last_output)`\n",
    "     5. Map to vocabulary: `output = self.fc(last_output)`\n",
    "   - Return: `output, hidden`\n",
    "\n",
    "3. **`init_hidden` method**:\n",
    "   - Input: `batch_size`\n",
    "   - Create two zero tensors for LSTM:\n",
    "     - Shape: `(num_layers, batch_size, hidden_dim)`\n",
    "     - Return as tuple: `(h0, c0)`\n",
    "   - Move to device: `.to(device)`\n",
    "\n",
    "### üí° Hints\n",
    "- LSTM returns `(output, (h_n, c_n))` unlike GRU which returns `(output, h_n)`\n",
    "- Use `batch_first=True` so input shape is `(batch, seq, features)`\n",
    "- The last time step: `[:, -1, :]` means \"all batches, last time step, all features\"\n",
    "\n",
    "### üé® Recommended Hyperparameters\n",
    "- `embedding_dim = 128`\n",
    "- `hidden_dim = 256`\n",
    "- `num_layers = 2`\n",
    "\n",
    "After creating the class, instantiate the model and print its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create CharRNN class\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate the model and print its structure\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ee174",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 6: Training Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e48004",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Set up the loss function and optimizer for training.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Define the loss function**:\n",
    "   - Use `nn.CrossEntropyLoss()`\n",
    "   - This is perfect for classification (predicting which character is next)\n",
    "\n",
    "2. **Define the optimizer**:\n",
    "   - Use `torch.optim.Adam(model.parameters(), lr=0.002)`\n",
    "   - Adam is a good default optimizer\n",
    "\n",
    "3. **Print the setup**:\n",
    "   - Confirm loss function and optimizer\n",
    "   - Print learning rate\n",
    "   - Count total trainable parameters\n",
    "\n",
    "### üí° Hints\n",
    "- Count parameters: `sum(p.numel() for p in model.parameters() if p.requires_grad)`\n",
    "- Format with commas: `f\"{count:,}\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define loss function and optimizer\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660baf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print training setup information\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879368e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 7: The Training Loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0c508",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Train your character-level RNN model.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "Create a training function that:\n",
    "\n",
    "1. **Function signature**: `train_model(model, dataloader, criterion, optimizer, num_epochs)`\n",
    "\n",
    "2. **Training loop structure**:\n",
    "   ```\n",
    "   for each epoch:\n",
    "       for each batch in dataloader:\n",
    "           1. Move inputs and targets to device\n",
    "           2. Initialize hidden state\n",
    "           3. Zero gradients: optimizer.zero_grad()\n",
    "           4. Forward pass: outputs, hidden = model(inputs, hidden)\n",
    "           5. Calculate loss: loss = criterion(outputs, targets)\n",
    "           6. Backward pass: loss.backward()\n",
    "           7. Clip gradients: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "           8. Update weights: optimizer.step()\n",
    "           9. Track loss\n",
    "   ```\n",
    "\n",
    "3. **Tracking and printing**:\n",
    "   - Store average loss per epoch in a list\n",
    "   - Print progress every 200 batches\n",
    "   - Print epoch summary (avg loss, time taken)\n",
    "   - Return the loss history\n",
    "\n",
    "4. **Train for 5-10 epochs**\n",
    "\n",
    "### üí° Hints\n",
    "- Set model to training mode: `model.train()`\n",
    "- Track time: `start_time = time.time()`, then `elapsed = time.time() - start_time`\n",
    "- Average loss: `total_loss / len(dataloader)`\n",
    "- Gradient clipping prevents exploding gradients (important for RNNs!)\n",
    "\n",
    "### ‚ö†Ô∏è Important\n",
    "- Training might take 5-15 minutes depending on your hardware\n",
    "- Loss should decrease over epochs\n",
    "- Don't worry if it's slow - character-level models process more sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the training function\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bade879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a4ae5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 8: Visualizing Training Progress**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34534c",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Plot the training loss to see how your model improved.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. Create a line plot showing:\n",
    "   - X-axis: Epoch number\n",
    "   - Y-axis: Average loss\n",
    "   - Add markers, grid, labels, and title\n",
    "\n",
    "2. Print:\n",
    "   - Initial loss (first epoch)\n",
    "   - Final loss (last epoch)\n",
    "   - Total improvement\n",
    "\n",
    "### üí° Hints\n",
    "- Use `plt.plot(epochs, losses, marker='o')`\n",
    "- Add grid: `plt.grid(True, alpha=0.3)`\n",
    "- Show plot: `plt.show()`\n",
    "\n",
    "### ü§î What to Look For\n",
    "- Loss should decrease over time\n",
    "- Curve should start to flatten (model converging)\n",
    "- If loss is still decreasing steeply, you could train longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training loss\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd451e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 9: Text Generation Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965c784",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Create a function that generates text character by character.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "Create a function `generate_text(model, char2idx, idx2char, seed_text, length, temperature)`:\n",
    "\n",
    "1. **Preparation**:\n",
    "   - Set model to eval mode: `model.eval()`\n",
    "   - Convert seed_text to indices\n",
    "   - Initialize hidden state\n",
    "\n",
    "2. **Generation loop** (repeat `length` times):\n",
    "   ```\n",
    "   with torch.no_grad():\n",
    "       for _ in range(length):\n",
    "           1. Convert current sequence to tensor\n",
    "           2. Forward pass through model\n",
    "           3. Get logits for last character\n",
    "           4. Apply temperature: logits = logits / temperature\n",
    "           5. Convert to probabilities: probs = torch.softmax(logits, dim=0)\n",
    "           6. Sample next character: next_idx = torch.multinomial(probs, 1).item()\n",
    "           7. Append to sequence\n",
    "   ```\n",
    "\n",
    "3. **Return**:\n",
    "   - Convert indices back to characters\n",
    "   - Join into string\n",
    "   - Return the generated text\n",
    "\n",
    "### üí° Hints\n",
    "- Temperature controls randomness:\n",
    "  - `temperature < 1.0`: More predictable (conservative)\n",
    "  - `temperature = 1.0`: Balanced\n",
    "  - `temperature > 1.0`: More random (creative)\n",
    "- Use `torch.no_grad()` to disable gradient computation (faster)\n",
    "- Convert character: `char2idx.get(ch, 0)` (use 0 if character not found)\n",
    "\n",
    "### üé® Function Parameters\n",
    "- `model`: Your trained model\n",
    "- `char2idx`: Character to index mapping\n",
    "- `idx2char`: Index to character mapping\n",
    "- `seed_text`: Starting text (e.g., \"The king \")\n",
    "- `length`: How many characters to generate\n",
    "- `temperature`: Controls randomness (default 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3862b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create text generation function\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc316bb8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 10: Testing Your Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6150da7",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Generate text with different seeds and temperatures to evaluate your model.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. **Test with different seed texts**:\n",
    "   - \"The king \"\n",
    "   - \"To be or not to be\"\n",
    "   - \"What is thy name\"\n",
    "   - Your own creative seed!\n",
    "\n",
    "2. **Test with different temperatures**:\n",
    "   - 0.5 (conservative)\n",
    "   - 1.0 (balanced)\n",
    "   - 1.5 (creative)\n",
    "\n",
    "3. **Generate**:\n",
    "   - 200-500 characters per sample\n",
    "   - Print the results clearly formatted\n",
    "\n",
    "4. **Analyze**:\n",
    "   - Does it spell words correctly?\n",
    "   - Does it use proper grammar?\n",
    "   - Does it sound like Shakespeare?\n",
    "   - How does temperature affect quality?\n",
    "\n",
    "### üí° Hints\n",
    "- Use a loop to test multiple combinations\n",
    "- Format output nicely:\n",
    "  ```python\n",
    "  print(f\"\\nSeed: '{seed}'\")\n",
    "  print(f\"Temperature: {temp}\")\n",
    "  print(\"-\" * 80)\n",
    "  print(generated_text)\n",
    "  ```\n",
    "\n",
    "### ü§î What to Observe\n",
    "- **Low temperature**: Repetitive but more correct\n",
    "- **High temperature**: Creative but might make mistakes\n",
    "- Character-level models learn spelling naturally!\n",
    "- Compare with word-level model - what differences do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test text generation with different seeds and temperatures\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8b7be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Section 11: Saving Your Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a06e4",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Save your trained model so you can use it later without retraining.\n",
    "\n",
    "### üìù Your Tasks\n",
    "\n",
    "1. Create a checkpoint dictionary containing:\n",
    "   - `'model_state_dict'`: Model weights (`model.state_dict()`)\n",
    "   - `'optimizer_state_dict'`: Optimizer state\n",
    "   - `'vocab_size'`: Size of character vocabulary\n",
    "   - `'embedding_dim'`: Embedding dimension used\n",
    "   - `'hidden_dim'`: Hidden dimension used\n",
    "   - `'num_layers'`: Number of LSTM layers\n",
    "   - `'char2idx'`: Character to index mapping\n",
    "   - `'idx2char'`: Index to character mapping\n",
    "   - `'loss_history'`: Training loss history\n",
    "\n",
    "2. Save using:\n",
    "   - `torch.save(checkpoint, 'char_rnn_model.pth')`\n",
    "\n",
    "3. Print confirmation:\n",
    "   - File saved location\n",
    "   - What's included in the checkpoint\n",
    "\n",
    "### üí° Hints\n",
    "- This saves everything needed to recreate and use your model\n",
    "- You can load it later with: `checkpoint = torch.load('char_rnn_model.pth')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the model checkpoint\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
