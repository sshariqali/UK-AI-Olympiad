{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Homework\n",
    "\n",
    "Welcome to your homework on **Gradient Descent with PyTorch**!\n",
    "\n",
    "In this notebook, you will practice the concepts from today's lesson:\n",
    "- Linear regression\n",
    "- Loss functions (MSE)\n",
    "- Manual gradient computation\n",
    "- PyTorch Autograd\n",
    "- Multiple linear regression\n",
    "\n",
    "**Instructions:**\n",
    "- Fill in the code cells marked with `# YOUR CODE HERE`\n",
    "- Do NOT change any other code\n",
    "- Run all cells to check your answers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Conceptual Questions\n",
    "\n",
    "Answer the following questions by assigning the correct option letter (as a string) to each variable.\n",
    "\n",
    "**1a)** In the equation $y = wx + b$, what does $w$ represent?\n",
    "- A: The input feature\n",
    "- B: The weight (how strongly the input affects the output)\n",
    "- C: The predicted output\n",
    "- D: The loss value\n",
    "\n",
    "**1b)** What does the Mean Squared Error (MSE) loss function measure?\n",
    "- A: The sum of all predictions\n",
    "- B: The average of squared differences between predictions and actual values\n",
    "- C: The number of training examples\n",
    "- D: The learning rate\n",
    "\n",
    "**1c)** If the gradient $\\frac{\\partial \\text{Loss}}{\\partial w}$ is **positive**, what should we do to $w$?\n",
    "- A: Increase $w$\n",
    "- B: Decrease $w$\n",
    "- C: Keep $w$ the same\n",
    "- D: Set $w$ to zero\n",
    "\n",
    "**1d)** What does `requires_grad=True` do in PyTorch?\n",
    "- A: It makes the tensor immutable\n",
    "- B: It tells PyTorch to track operations on this tensor for automatic gradient computation\n",
    "- C: It normalizes the tensor values\n",
    "- D: It converts the tensor to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "answer_1a = \"__\"  # Replace __ with A, B, C, or D\n",
    "answer_1b = \"__\"\n",
    "answer_1c = \"__\"\n",
    "answer_1d = \"__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Compute MSE Loss Manually\n",
    "\n",
    "Given the following predictions and actual values, compute the MSE loss **by hand** (using PyTorch operations).\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_{\\text{predicted}} - y_{\\text{actual}})^2$$\n",
    "\n",
    "| Predicted | Actual |\n",
    "|-----------|--------|\n",
    "| 50        | 48     |\n",
    "| 60        | 63     |\n",
    "| 70        | 68     |\n",
    "| 80        | 82     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.tensor([50, 60, 70, 80], dtype=torch.float32)\n",
    "y_actual = torch.tensor([48, 63, 68, 82], dtype=torch.float32)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Compute the MSE loss using the formula above\n",
    "mse_loss = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Manual Gradient Computation\n",
    "\n",
    "Given a simple model $y = wx + b$ with **one data point** $x = 3$, $y_{\\text{actual}} = 10$, and current parameters $w = 2$, $b = 1$:\n",
    "\n",
    "1. Compute the prediction $\\hat{y}$\n",
    "2. Compute the MSE loss (with just one data point: $\\text{Loss} = (\\hat{y} - y)^2$)\n",
    "3. Compute the gradient $\\frac{\\partial \\text{Loss}}{\\partial w} = 2(\\hat{y} - y) \\cdot x$\n",
    "4. Compute the gradient $\\frac{\\partial \\text{Loss}}{\\partial b} = 2(\\hat{y} - y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = 3.0\n",
    "y_actual_val = 10.0\n",
    "w_val = 2.0\n",
    "b_val = 1.0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Step 1: Compute prediction\n",
    "y_hat = ...\n",
    "\n",
    "# Step 2: Compute loss\n",
    "loss_val = ...\n",
    "\n",
    "# Step 3: Compute dL/dw\n",
    "dw = ...\n",
    "\n",
    "# Step 4: Compute dL/db\n",
    "db = ...\n",
    "\n",
    "print(f\"Prediction: {y_hat}\")\n",
    "print(f\"Loss: {loss_val}\")\n",
    "print(f\"dL/dw: {dw}\")\n",
    "print(f\"dL/db: {db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Linear Regression with Manual Gradients\n",
    "\n",
    "A gym trainer wants to predict how many **calories** a person burns based on the number of **minutes exercised**.\n",
    "\n",
    "| Minutes Exercised | Calories Burned |\n",
    "|-------------------|-----------------|\n",
    "| 10                | 120             |\n",
    "| 20                | 220             |\n",
    "| 30                | 340             |\n",
    "| 40                | 430             |\n",
    "| 50                | 550             |\n",
    "\n",
    "Build a linear model $y = wx + b$ using **manual gradient descent** (no autograd).\n",
    "\n",
    "Use:\n",
    "- `learning_rate = 0.0001`\n",
    "- `epochs = 5000`\n",
    "- `torch.manual_seed(0)` before initializing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data ---\n",
    "x = torch.tensor([10, 20, 30, 40, 50], dtype=torch.float32)\n",
    "y = torch.tensor([120, 220, 340, 430, 550], dtype=torch.float32)\n",
    "\n",
    "# --- Initialize parameters ---\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "w = ...  # Initialize w using torch.randn(1)\n",
    "b = ...  # Initialize b using torch.randn(1)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 5000\n",
    "n = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement the gradient descent loop\n",
    "# For each epoch:\n",
    "#   1. Compute predictions: y_pred = x * w + b\n",
    "#   2. Compute MSE loss: loss = mean((y_pred - y)^2)\n",
    "#   3. Compute error: error = y_pred - y\n",
    "#   4. Compute gradient for w: dw = (2/n) * sum(x * error)\n",
    "#   5. Compute gradient for b: db = (2/n) * sum(error)\n",
    "#   6. Update w and b\n",
    "#   7. Print loss every 500 epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pass  # Replace this with your implementation\n",
    "\n",
    "print(f\"\\nFinal weight: {w.item():.4f}\")\n",
    "print(f\"Final bias: {b.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Linear Regression with Autograd\n",
    "\n",
    "Now solve the **same problem** from Question 4, but using **PyTorch Autograd** instead of computing gradients manually.\n",
    "\n",
    "Remember the key differences:\n",
    "- Use `requires_grad=True` when creating `w` and `b`\n",
    "- Call `loss.backward()` to compute gradients automatically\n",
    "- Update parameters inside `torch.no_grad()`\n",
    "- Zero out gradients with `.grad.zero_()` after each update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data (same as Q4) ---\n",
    "x = torch.tensor([10, 20, 30, 40, 50], dtype=torch.float32)\n",
    "y = torch.tensor([120, 220, 340, 430, 550], dtype=torch.float32)\n",
    "\n",
    "# --- Initialize parameters ---\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "w_auto = ...  # Initialize with torch.randn(1) and requires_grad=True\n",
    "b_auto = ...  # Initialize with torch.randn(1) and requires_grad=True\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 5000\n",
    "n = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement gradient descent using autograd\n",
    "# For each epoch:\n",
    "#   1. Forward pass: y_pred = x * w_auto + b_auto\n",
    "#   2. Compute MSE loss\n",
    "#   3. Call loss.backward()\n",
    "#   4. Update parameters inside torch.no_grad() block\n",
    "#   5. Zero out gradients\n",
    "#   6. Print loss every 500 epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pass  # Replace this with your implementation\n",
    "\n",
    "print(f\"\\nFinal weight (autograd): {w_auto.item():.4f}\")\n",
    "print(f\"Final bias (autograd): {b_auto.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 6: Multiple Linear Regression\n",
    "\n",
    "A real estate agent wants to predict **house prices** (in $1000s) based on two features:\n",
    "- **Size** (in hundreds of sq ft)\n",
    "- **Number of bedrooms**\n",
    "\n",
    "| Size (100s sqft) | Bedrooms | Price ($1000s) |\n",
    "|------------------|----------|----------------|\n",
    "| 8                | 2        | 250            |\n",
    "| 12               | 3        | 340            |\n",
    "| 15               | 3        | 395            |\n",
    "| 18               | 4        | 450            |\n",
    "| 22               | 4        | 520            |\n",
    "| 25               | 5        | 580            |\n",
    "\n",
    "Build a multiple linear regression model:\n",
    "$$\\text{Price} = w_1 \\cdot \\text{Size} + w_2 \\cdot \\text{Bedrooms} + b$$\n",
    "\n",
    "Use Autograd with:\n",
    "- `learning_rate = 0.0001`\n",
    "- `epochs = 10000`\n",
    "- `torch.manual_seed(42)` before initializing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data ---\n",
    "# x shape: (6, 2) -> 6 houses, 2 features each\n",
    "x = torch.tensor([[8, 2],\n",
    "                   [12, 3],\n",
    "                   [15, 3],\n",
    "                   [18, 4],\n",
    "                   [22, 4],\n",
    "                   [25, 5]], dtype=torch.float32)\n",
    "\n",
    "y = torch.tensor([250, 340, 395, 450, 520, 580], dtype=torch.float32)\n",
    "\n",
    "# --- Initialize parameters ---\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "w = ...  # Shape (2, 1) with requires_grad=True\n",
    "b = ...  # Shape (1,) with requires_grad=True\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 10000\n",
    "n = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement gradient descent using autograd for multiple linear regression\n",
    "# Remember:\n",
    "#   - Use matrix multiplication (@) for the forward pass: y_pred = x @ w + b\n",
    "#   - Use .squeeze() on y_pred before computing loss\n",
    "#   - Print loss every 1000 epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pass  # Replace this with your implementation\n",
    "\n",
    "print(f\"\\nFinal weights: w1={w[0].item():.4f}, w2={w[1].item():.4f}\")\n",
    "print(f\"Final bias: {b.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 7: Learning Rate Experiment\n",
    "\n",
    "The **learning rate** is a critical hyperparameter. Let's see what happens when it's too large or too small.\n",
    "\n",
    "Using the simple dataset below, run gradient descent with **three different learning rates** and observe the final loss after 1000 epochs.\n",
    "\n",
    "Fill in the training loop, then answer the question at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data ---\n",
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "y = torch.tensor([3, 5, 7, 9, 11], dtype=torch.float32)  # y = 2x + 1\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.5]\n",
    "epochs = 1000\n",
    "n = len(x)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    torch.manual_seed(42)\n",
    "    w = torch.randn(1, dtype=torch.float32, requires_grad=True)\n",
    "    b = torch.randn(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Run gradient descent for the given number of epochs using autograd\n",
    "    final_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "    print(f\"LR = {lr}: Final Loss = {final_loss:.4f}, w = {w.item():.4f}, b = {b.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Based on your results above, which learning rate gave the best result and why?\n",
    "\n",
    "- A: `0.001` - Small learning rate, slow convergence, might not reach minimum\n",
    "- B: `0.01` - Good balance, converges well to the minimum\n",
    "- C: `0.5` - Large learning rate, might overshoot and diverge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "best_lr_answer = \"__\"  # Replace __ with A, B, or C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Make a Prediction\n",
    "\n",
    "Using the model you trained in **Question 4** (Calories burned), predict:\n",
    "\n",
    "1. How many calories would someone burn in **35 minutes** of exercise?\n",
    "2. How many calories would someone burn in **60 minutes** of exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-setup the trained model from Q4 (run Q4 first if needed)\n",
    "# If you completed Q4, w and b should still be available\n",
    "\n",
    "# YOUR CODE HERE\n",
    "x_new = torch.tensor([35, 60], dtype=torch.float32)\n",
    "predictions = ...  # Use the model equation: y = w * x + b\n",
    "\n",
    "print(f\"Predicted calories for 35 minutes: {predictions[0].item():.1f}\")\n",
    "print(f\"Predicted calories for 60 minutes: {predictions[1].item():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this homework you practiced:\n",
    "\n",
    "1. **Core concepts** of linear regression, loss functions, and gradients\n",
    "2. **Manual MSE computation** from predictions and actual values\n",
    "3. **Manual gradient computation** using the derivative formulas\n",
    "4. **Full gradient descent loop** with manually computed gradients\n",
    "5. **Autograd-based gradient descent** using `loss.backward()`\n",
    "6. **Multiple linear regression** with matrix multiplication\n",
    "7. **Learning rate effects** on model training\n",
    "\n",
    "Great work! These are the building blocks for everything in deep learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
