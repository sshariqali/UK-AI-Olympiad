{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "onLsjVhM3I8R",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1762712677256,
     "user": {
      "displayName": "Shariq Ali",
      "userId": "00441094809974358112"
     },
     "user_tz": 0
    },
    "id": "onLsjVhM3I8R",
    "outputId": "d54ec921-a8a6-4019-e661-d7bbfaffa08c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "const firstCell = document.querySelector('.cell.code_cell');\n",
       "if (firstCell) {\n",
       "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
       "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
       "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
       "\n",
       "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
       "       alt=\"Algopath Coding Academy Logo\"\n",
       "       width=\"400\"\n",
       "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
       "\n",
       "  <p style=\"font-size:16px; margin:0;\">\n",
       "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
       "  </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead08101",
   "metadata": {
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1762712733375,
     "user": {
      "displayName": "Shariq Ali",
      "userId": "00441094809974358112"
     },
     "user_tz": 0
    },
    "id": "ead08101"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f5e98",
   "metadata": {
    "id": "082f5e98"
   },
   "source": [
    "## **1. Predicting Test Scores**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083b46f",
   "metadata": {
    "id": "d083b46f"
   },
   "source": [
    "Imagine you're a teacher who wants to predict student test scores based on **Hours studied**\n",
    "\n",
    "You collect data from previous students and notice a pattern: students who study more tend to score higher. But you want to **quantify** this relationship!\n",
    "\n",
    "Like for example: $$TestScore = w * HoursStudied + b$$\n",
    "\n",
    "Let's say that the data you collected was:\n",
    "\n",
    "| Hours Studied | Test Score |\n",
    "|---------------|------------|\n",
    "| 2             | 75         |\n",
    "| 4             | 80         |\n",
    "| 6             | 90         |\n",
    "| 8             | 94         |\n",
    "| 10            | 98         |\n",
    "\n",
    "**Our task**:\n",
    "- Find the values of $w$\n",
    "- Build a model that learns the relationship between study time and test scores, then use it to predict scores for new students!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df5c06",
   "metadata": {
    "id": "41df5c06"
   },
   "source": [
    "## **2. Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375e157",
   "metadata": {
    "id": "4375e157"
   },
   "source": [
    "The model we are about to build is called **Linear Regression**\n",
    "\n",
    "- **Linear** because we are trying to model a straight-line (linear) relationship between the input variable and output variable.\n",
    "- **Regression** because we are predicting a continuous numerical value, not a category or label.\n",
    "\n",
    "Linear regression is one of the simplest and most widely used machine learning models.\n",
    "\n",
    "Let‚Äôs rewrite our earlier equation in a general form:\n",
    "\n",
    "$$y = w x + b$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y$ -> the predicted output (Test Score)\n",
    "- $x$ -> Hours Studied\n",
    "- $w$ -> Weight (coefficient) that measure how strongly the input affects the output\n",
    "- $b$ -> Bias (intercept), which adjusts the baseline prediction\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/sshariqali/mnist_pretrained_model/blob/main/gd_v1.png?raw=true\" \n",
    "       style=\"width: 1000px; clip-path: inset(0 50% 0 0); margin-right: -500px;\" \n",
    "       alt=\"Linear Relationship\">\n",
    "  <p><i>Linear Relationship between X and Y</i></p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5f8c3",
   "metadata": {
    "id": "e6f5f8c3"
   },
   "source": [
    "## **3. Problem Framing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e83221",
   "metadata": {
    "id": "d8e83221"
   },
   "source": [
    "Now we need to find the **best values** for our weights ($w$) and bias ($b$) that minimize the difference between our predictions and actual values.\n",
    "\n",
    "The **best values** will give us what's called the **Line of Best Fit** that would have the **minimum average difference** between the actual values and predictions.\n",
    "\n",
    "But how do we know what **best** means?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0773774",
   "metadata": {
    "id": "f0773774"
   },
   "source": [
    "## **4. The Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f368f4",
   "metadata": {
    "id": "c2f368f4"
   },
   "source": [
    "We measure how wrong our predictions are using a **loss function**. The most common one for regression is **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\text{MSE (loss)} = \\frac{1}{n}\\sum_{i=1}^{n}(y_{\\text{predicted}} - y_{\\text{actual}})^2$$\n",
    "\n",
    "- This calculates the average of **squared differences** between predictions and actual values\n",
    "- Squaring ensures errors are always positive and penalizes larger errors more\n",
    "\n",
    "**Our goal**: Minimize this loss! But how?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627559b8",
   "metadata": {
    "id": "627559b8"
   },
   "source": [
    "## **5. Enter Calculus: The Power of Derivatives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be8583",
   "metadata": {
    "id": "35be8583"
   },
   "source": [
    "To minimize the loss, we need to know **which direction** to adjust our parameters ($w$, $b$) and **by how much**. This is where **derivatives** (also called **gradients**) come in!\n",
    "\n",
    "- A derivative tells us the **rate of change** of a function\n",
    "- In our case: *\"If I change $w_1$ by a tiny amount, how much does the loss change?\"*\n",
    "- Mathematically: $\\frac{\\partial \\text{Loss}}{\\partial w}$ means \"the partial derivative of loss with respect to $w$\"\n",
    "\n",
    "Recall that the **Partial Derivative** $\\frac{\\partial \\text{Loss}}{\\partial w}$ is computed Mathematicaly:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "= \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) x_{i}\n",
    "}\n",
    "$$\n",
    "\n",
    "**Sign of a derivative:**\n",
    "1. **Positive derivative** ‚Üí Loss increases as parameter increases ‚Üí We should **decrease** the parameter\n",
    "2. **Negative derivative** ‚Üí Loss decreases as parameter increases ‚Üí We should **increase** the parameter\n",
    "3. **Zero derivative** ‚Üí We're at a minimum (or maximum)!\n",
    "\n",
    "**The optimization rule:**\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial \\text{Loss}}{\\partial w}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the **learning rate** (how big a step we take)\n",
    "- We **subtract** the derivative to move in the direction that reduces loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680aa6dd",
   "metadata": {
    "id": "680aa6dd"
   },
   "source": [
    "## **6. Gradient Descent!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f38da8",
   "metadata": {
    "id": "e1f38da8"
   },
   "source": [
    "When we **repeat** this process of:\n",
    "1. Using derivatives to find which direction reduces loss\n",
    "2. Taking a small step (learning rate) in that direction\n",
    "3. Updating our parameters\n",
    "\n",
    "Over and over until we reach the minimum loss ‚Äî this becomes the **Gradient Descent Algorithm**!\n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "1. **Initialize**: Start with random values for $w$, and $b$\n",
    "\n",
    "2. **Forward Pass**: Calculate predictions using current parameters\n",
    "   $$\\hat{y} = w x + b$$\n",
    "\n",
    "3. **Calculate Loss**: Measure how wrong our predictions are using MSE\n",
    "   $$\\text{Loss} = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "4. **Compute Gradients**: Calculate the derivatives (how much each parameter affects the loss)\n",
    "   $$\\frac{\\partial \\text{Loss}}{\\partial w}, \\quad \\frac{\\partial \\text{Loss}}{\\partial b}$$\n",
    "\n",
    "5. **Update Parameters**: Take a step in the direction that reduces loss\n",
    "   $$w = w - \\alpha \\frac{\\partial \\text{Loss}}{\\partial w}$$\n",
    "   $$b = b - \\alpha \\frac{\\partial \\text{Loss}}{\\partial b}$$\n",
    "\n",
    "6. **Repeat**: Go back to step 2 and repeat until the loss stops decreasing (convergence)\n",
    "\n",
    "This iterative process gradually moves our parameters toward the optimal values that minimize the loss function!\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/631731_P7z2BKhd0R-9uyn9ThDasA.webp\" width=\"700\"/>\n",
    "  <p><i>Gradient Descent Curve</i></p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed1689",
   "metadata": {
    "id": "eaed1689"
   },
   "source": [
    "## **7. PyTorch Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11cff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d4418105d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed5b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features (x):\n",
      "tensor([ 2.,  4.,  6.,  8., 10.])\n",
      "torch.Size([5])\n",
      "\n",
      "Target values (y):\n",
      "tensor([75., 80., 90., 94., 98.])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Prepare the data as PyTorch tensors ---\n",
    "\n",
    "x = torch.tensor([2, 4, 6, 8, 10], dtype = torch.float32)\n",
    "y = torch.tensor([75, 80, 90, 94, 98], dtype = torch.float32)\n",
    "\n",
    "print(\"Input features (x):\")\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(\"\\nTarget values (y):\")\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa5e2cf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1762713651571,
     "user": {
      "displayName": "Shariq Ali",
      "userId": "00441094809974358112"
     },
     "user_tz": 0
    },
    "id": "aa5e2cf0",
    "outputId": "b4611c34-1d15-4804-e7b3-2a03cfa318dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights:\n",
      "tensor([0.3367])\n",
      "Weights Shape: torch.Size([1])\n",
      "\n",
      "Initial bias: 0.1288\n",
      "Bias Shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Initialize parameters randomly ---\n",
    "\n",
    "torch.manual_seed(42)\n",
    "w = torch.randn(1, dtype = torch.float32)  # 1 weight (w)\n",
    "b = torch.randn(1, dtype = torch.float32)  # 1 bias\n",
    "\n",
    "print(f\"Initial weights:\\n{w}\")\n",
    "print(f\"Weights Shape: {w.shape}\")\n",
    "print(f\"\\nInitial bias: {b.item():.4f}\")\n",
    "print(f\"Bias Shape: {b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41223261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Setup Hyperparameters ---\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 2000 # Number of iterations for gradient descent\n",
    "n = len(x)  # Number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ccaa236",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6280,
     "status": "ok",
     "timestamp": 1762713663770,
     "user": {
      "displayName": "Shariq Ali",
      "userId": "00441094809974358112"
     },
     "user_tz": 0
    },
    "id": "9ccaa236",
    "outputId": "0ac7a2f4-63ba-4ca0-caab-af9fce77cc80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 412.0172\n",
      "Epoch [200/2000], Loss: 202.6555\n",
      "Epoch [300/2000], Loss: 100.2602\n",
      "Epoch [400/2000], Loss: 50.1801\n",
      "Epoch [500/2000], Loss: 25.6868\n",
      "Epoch [600/2000], Loss: 13.7074\n",
      "Epoch [700/2000], Loss: 7.8486\n",
      "Epoch [800/2000], Loss: 4.9831\n",
      "Epoch [900/2000], Loss: 3.5816\n",
      "Epoch [1000/2000], Loss: 2.8962\n",
      "Epoch [1100/2000], Loss: 2.5609\n",
      "Epoch [1200/2000], Loss: 2.3970\n",
      "Epoch [1300/2000], Loss: 2.3168\n",
      "Epoch [1400/2000], Loss: 2.2775\n",
      "Epoch [1500/2000], Loss: 2.2584\n",
      "Epoch [1600/2000], Loss: 2.2490\n",
      "Epoch [1700/2000], Loss: 2.2444\n",
      "Epoch [1800/2000], Loss: 2.2421\n",
      "Epoch [1900/2000], Loss: 2.2411\n",
      "Epoch [2000/2000], Loss: 2.2405\n",
      "\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Implement Gradient Descent with Manual Gradients ---\n",
    "\n",
    "# Store loss history for visualization\n",
    "loss_history = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Forward pass: compute predictions\n",
    "    y_pred = x * w + b\n",
    "\n",
    "    # Compute loss (MSE)\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # --- Manually compute gradients ---\n",
    "    # Remember: dL/dw = (2/n) * X^T @ (y_pred - y)\n",
    "    #           dL/db = (2/n) * sum(y_pred - y)\n",
    "\n",
    "    error = y_pred - y\n",
    "\n",
    "    # Gradient for weights\n",
    "    dw = (2 / n) * torch.sum(x * error)\n",
    "\n",
    "    # Gradient for bias\n",
    "    db = (2 / n) * torch.sum(error)\n",
    "\n",
    "    # Update parameters using gradient descent\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weight: 3.007255792617798\n",
      "Trained bias: 69.3470\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Check final parameters ---\n",
    "\n",
    "print(f\"Trained weight: {w.item()}\")\n",
    "print(f\"Trained bias: {b.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da000344",
   "metadata": {},
   "source": [
    "Now that we have trained our model and found the optimal values for **weight ($w$)** and **bias ($b$)**, we can use them to predict the test score for any number of hours studied!\n",
    "\n",
    "Our final model equation is:\n",
    "\n",
    "$$\\text{Predicted Score} = w \\times \\text{Hours Studied} + b$$\n",
    "\n",
    "For example, if a student studies for **5 hours**, we simply plug that value into our equation:\n",
    "\n",
    "$$\\text{Predicted Score} = (3.01 \\times 5) + 69.35 \\approx 84.40$$\n",
    "\n",
    "This allows us to estimate performance for new students based on the patterns the model learned from the previous data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1d7c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([84.3833, 90.3978, 96.4123])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 6: Use the trained model to make predictions ---\n",
    "\n",
    "x_new = torch.tensor([5, 7, 9], dtype = torch.float32)\n",
    "\n",
    "new_predictions =  x_new * w + b\n",
    "\n",
    "new_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757543a",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/sshariqali/mnist_pretrained_model/blob/main/gd_3.png?raw=true\" width=\"700\"/>\n",
    "  <p><i>Final Predictions</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49d7db",
   "metadata": {
    "id": "7f49d7db"
   },
   "source": [
    "## **8. PyTorch (Auto Grad)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e64ea",
   "metadata": {
    "id": "874e64ea"
   },
   "source": [
    "In the previous section, we manually calculated gradients using the formulas we derived. But imagine if we had:\n",
    "- **Hundreds of parameters** instead of just 3\n",
    "- **Complex neural networks** with multiple layers\n",
    "- **Different activation functions** and architectures\n",
    "\n",
    "Manually computing gradients would become extremely tedious and error-prone! üò∞\n",
    "\n",
    "This is where **PyTorch's Autograd** (Automatic Differentiation) comes to the rescue! üéâ\n",
    "\n",
    "**Autograd** automatically computes gradients for us by:\n",
    "1. Tracking all operations on tensors that have `requires_grad=True`\n",
    "2. Building a computational graph in the background\n",
    "3. Using the chain rule to compute gradients automatically when we call `.backward()`\n",
    "\n",
    "Let's implement the **exact same** linear regression model, but this time letting PyTorch handle all the gradient calculations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e9eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights:\n",
      "tensor([0.3367], requires_grad=True)\n",
      "Weights Shape: torch.Size([1])\n",
      "\n",
      "Initial bias: 0.1288\n",
      "Bias Shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Initialize parameters with requires_grad = True ---\n",
    "\n",
    "torch.manual_seed(42)\n",
    "w_auto = torch.randn(1, dtype = torch.float32, requires_grad = True)  # 1 weight (w)\n",
    "b_auto = torch.randn(1, dtype = torch.float32, requires_grad = True)  # 1 bias\n",
    "\n",
    "print(f\"Initial weights:\\n{w_auto}\")\n",
    "print(f\"Weights Shape: {w_auto.shape}\")\n",
    "print(f\"\\nInitial bias: {b_auto.item():.4f}\")\n",
    "print(f\"Bias Shape: {b_auto.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68e5b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Setup Hyperparameters ---\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 2000 # Number of iterations for gradient descent\n",
    "n = len(x)  # Number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc66c8f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14370,
     "status": "ok",
     "timestamp": 1762713880207,
     "user": {
      "displayName": "Shariq Ali",
      "userId": "00441094809974358112"
     },
     "user_tz": 0
    },
    "id": "bc66c8f3",
    "outputId": "1ff0d838-2c75-4a23-c238-1c2cb1e961c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 412.0172\n",
      "Epoch [200/2000], Loss: 202.6555\n",
      "Epoch [300/2000], Loss: 100.2602\n",
      "Epoch [400/2000], Loss: 50.1801\n",
      "Epoch [500/2000], Loss: 25.6868\n",
      "Epoch [600/2000], Loss: 13.7074\n",
      "Epoch [700/2000], Loss: 7.8486\n",
      "Epoch [800/2000], Loss: 4.9831\n",
      "Epoch [900/2000], Loss: 3.5816\n",
      "Epoch [1000/2000], Loss: 2.8962\n",
      "Epoch [1100/2000], Loss: 2.5609\n",
      "Epoch [1200/2000], Loss: 2.3970\n",
      "Epoch [1300/2000], Loss: 2.3168\n",
      "Epoch [1400/2000], Loss: 2.2776\n",
      "Epoch [1500/2000], Loss: 2.2584\n",
      "Epoch [1600/2000], Loss: 2.2490\n",
      "Epoch [1700/2000], Loss: 2.2444\n",
      "Epoch [1800/2000], Loss: 2.2422\n",
      "Epoch [1900/2000], Loss: 2.2411\n",
      "Epoch [2000/2000], Loss: 2.2405\n",
      "\n",
      "‚úÖ Training complete with Autograd!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Gradient Descent with Autograd ---\n",
    "\n",
    "loss_history_auto = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = x * w_auto + b_auto\n",
    "\n",
    "    # Compute loss\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "    loss_history_auto.append(loss.item())\n",
    "\n",
    "    # Compute gradients automatically!\n",
    "    loss.backward()  # This computes dL/dw and dL/db for us!\n",
    "\n",
    "    # Update parameters (we need to disable gradient tracking for this update)\n",
    "    with torch.no_grad():\n",
    "        w_auto -= learning_rate * w_auto.grad\n",
    "        b_auto -= learning_rate * b_auto.grad\n",
    "\n",
    "        # Zero out gradients for next iteration\n",
    "        w_auto.grad.zero_()\n",
    "        b_auto.grad.zero_()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete with Autograd!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fff15",
   "metadata": {},
   "source": [
    "**Key Autograd Concepts**\n",
    "\n",
    "*   **`requires_grad=True`**: This signals PyTorch to start building the \"Forward Pass\" graph so it knows how to apply the Chain Rule later during backpropagation.\n",
    "\n",
    "*   **`torch.no_grad()`**: When updating the weights ($w = w - \\alpha \\cdot \\text{grad}$), you don't want PyTorch to track that specific subtraction as part of the gradient calculation. It is a manual update step that shouldn't be part of the computational graph.\n",
    "\n",
    "*   **In-place Updates**: We use `w_auto -= learning_rate * w_auto.grad` instead of `w_auto = w_auto - learning_rate * w_auto.grad` to modify the tensor data directly without creating a new tensor that would break the gradient tracking.\n",
    "\n",
    "*   **`.zero_()`**: PyTorch accumulates gradients (adds them together) every time you call `.backward()`. If you don't zero them out, the current gradient will be added to the previous one, leading to incorrect updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd74b797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weight: 3.007255792617798\n",
      "Trained bias: 69.3470\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Check final parameters ---\n",
    "\n",
    "print(f\"Trained weight: {w_auto.item()}\")\n",
    "print(f\"Trained bias: {b_auto.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1f41e",
   "metadata": {},
   "source": [
    "## **9. Multiple Linear Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4325c",
   "metadata": {},
   "source": [
    "Suppose our dataset now has an additional column `Hours Slept` which will also be used in modelling `Test Score`\n",
    "\n",
    "| Hours Studied | Hours Slept | Test Score |\n",
    "|---------------|-------------|------------|\n",
    "| 2             | 6           | 75         |\n",
    "| 4             | 7           | 80         |\n",
    "| 6             | 8           | 90         |\n",
    "| 8             | 7.5         | 94         |\n",
    "| 10            | 8           | 98         |\n",
    "\n",
    "New Equation:\n",
    "\n",
    "$$TestScore = w_1 * HoursStudied + w_2 * HoursSlept + b$$\n",
    "$$y = w_1x_1 + w_2x_2 + b$$\n",
    "\n",
    "**Your task**:\n",
    "- Find the values of $w_1$ and $w_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e092a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features (x):\n",
      "tensor([[ 2.0000,  4.0000],\n",
      "        [ 6.0000,  8.0000],\n",
      "        [10.0000,  6.0000],\n",
      "        [ 7.0000,  8.0000],\n",
      "        [ 7.5000,  8.0000]])\n",
      "torch.Size([5, 2])\n",
      "\n",
      "Target values (y):\n",
      "tensor([75., 80., 90., 94., 98.])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Prepare the data as PyTorch tensors ---\n",
    "\n",
    "x = torch.tensor([[2, 4, 6, 8, 10], [6, 7, 8, 7.5, 8]], dtype = torch.float32).reshape(5 , 2)\n",
    "y = torch.tensor([75, 80, 90, 94, 98], dtype = torch.float32)\n",
    "\n",
    "print(\"Input features (x):\")\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(\"\\nTarget values (y):\")\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ab58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights:\n",
      "tensor([[0.3367],\n",
      "        [0.1288]], requires_grad=True)\n",
      "Weights Shape: torch.Size([2, 1])\n",
      "\n",
      "Initial bias: 0.2345\n",
      "Bias Shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Initialize parameters with requires_grad = True ---\n",
    "\n",
    "torch.manual_seed(42)\n",
    "w_auto = torch.randn(2, 1, dtype = torch.float32, requires_grad = True)  # 2 weights (w1, w2)\n",
    "b_auto = torch.randn(1, dtype = torch.float32, requires_grad = True)  # 1 bias\n",
    "\n",
    "print(f\"Initial weights:\\n{w_auto}\")\n",
    "print(f\"Weights Shape: {w_auto.shape}\")\n",
    "print(f\"\\nInitial bias: {b_auto.item():.4f}\")\n",
    "print(f\"Bias Shape: {b_auto.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02d3ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Setup Hyperparameters ---\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 5000 # Number of iterations for gradient descent\n",
    "n = len(x)  # Number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22517d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/5000], Loss: 186.2492\n",
      "Epoch [200/5000], Loss: 156.7641\n",
      "Epoch [300/5000], Loss: 132.7553\n",
      "Epoch [400/5000], Loss: 113.2028\n",
      "Epoch [500/5000], Loss: 97.2796\n",
      "Epoch [600/5000], Loss: 84.3120\n",
      "Epoch [700/5000], Loss: 73.7513\n",
      "Epoch [800/5000], Loss: 65.1508\n",
      "Epoch [900/5000], Loss: 58.1467\n",
      "Epoch [1000/5000], Loss: 52.4427\n",
      "Epoch [1100/5000], Loss: 47.7974\n",
      "Epoch [1200/5000], Loss: 44.0143\n",
      "Epoch [1300/5000], Loss: 40.9335\n",
      "Epoch [1400/5000], Loss: 38.4245\n",
      "Epoch [1500/5000], Loss: 36.3812\n",
      "Epoch [1600/5000], Loss: 34.7171\n",
      "Epoch [1700/5000], Loss: 33.3619\n",
      "Epoch [1800/5000], Loss: 32.2583\n",
      "Epoch [1900/5000], Loss: 31.3596\n",
      "Epoch [2000/5000], Loss: 30.6277\n",
      "Epoch [2100/5000], Loss: 30.0316\n",
      "Epoch [2200/5000], Loss: 29.5461\n",
      "Epoch [2300/5000], Loss: 29.1508\n",
      "Epoch [2400/5000], Loss: 28.8288\n",
      "Epoch [2500/5000], Loss: 28.5666\n",
      "Epoch [2600/5000], Loss: 28.3530\n",
      "Epoch [2700/5000], Loss: 28.1791\n",
      "Epoch [2800/5000], Loss: 28.0375\n",
      "Epoch [2900/5000], Loss: 27.9222\n",
      "Epoch [3000/5000], Loss: 27.8283\n",
      "Epoch [3100/5000], Loss: 27.7518\n",
      "Epoch [3200/5000], Loss: 27.6895\n",
      "Epoch [3300/5000], Loss: 27.6387\n",
      "Epoch [3400/5000], Loss: 27.5974\n",
      "Epoch [3500/5000], Loss: 27.5638\n",
      "Epoch [3600/5000], Loss: 27.5364\n",
      "Epoch [3700/5000], Loss: 27.5141\n",
      "Epoch [3800/5000], Loss: 27.4959\n",
      "Epoch [3900/5000], Loss: 27.4811\n",
      "Epoch [4000/5000], Loss: 27.4691\n",
      "Epoch [4100/5000], Loss: 27.4592\n",
      "Epoch [4200/5000], Loss: 27.4512\n",
      "Epoch [4300/5000], Loss: 27.4447\n",
      "Epoch [4400/5000], Loss: 27.4394\n",
      "Epoch [4500/5000], Loss: 27.4351\n",
      "Epoch [4600/5000], Loss: 27.4316\n",
      "Epoch [4700/5000], Loss: 27.4287\n",
      "Epoch [4800/5000], Loss: 27.4264\n",
      "Epoch [4900/5000], Loss: 27.4245\n",
      "Epoch [5000/5000], Loss: 27.4230\n",
      "\n",
      "‚úÖ Training complete with Autograd!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Gradient Descent with Autograd ---\n",
    "\n",
    "loss_history_auto = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = x @ w_auto + b_auto # Matrix multiplication (5x2 @ 2x1 = 5x1)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = torch.mean((y_pred.squeeze() - y) ** 2)\n",
    "    loss_history_auto.append(loss.item())\n",
    "\n",
    "    # Compute gradients automatically!\n",
    "    loss.backward()  # This computes dL/dw and dL/db for us!\n",
    "\n",
    "    # Update parameters (we need to disable gradient tracking for this update)\n",
    "    with torch.no_grad():\n",
    "        w_auto -= learning_rate * w_auto.grad\n",
    "        b_auto -= learning_rate * b_auto.grad\n",
    "\n",
    "        # Zero out gradients for next iteration\n",
    "        w_auto.grad.zero_()\n",
    "        b_auto.grad.zero_()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete with Autograd!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4753bcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: 1.8086, 1.9646\n",
      "Trained bias: 62.2669\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Check final parameters ---\n",
    "\n",
    "print(f\"Trained weights: {w_auto[0].item():.4f}, {w_auto[1].item():.4f}\")\n",
    "print(f\"Trained bias: {b_auto.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62441275",
   "metadata": {},
   "source": [
    "## **10. Reading Material**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8591ed",
   "metadata": {},
   "source": [
    "### Partial Derivative of Loss - Mathematical Derivation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d946dc",
   "metadata": {},
   "source": [
    "To minimize the loss, we need to know **which direction** to adjust our parameters ($w_1$, $w_2$, $b$) and **by how much**. This is where **derivatives** (also called **gradients**) come in!\n",
    "\n",
    "- A derivative tells us the **rate of change** of a function\n",
    "- In our case: *\"If I change $w_1$ by a tiny amount, how much does the loss change?\"*\n",
    "- Mathematically: $\\frac{\\partial \\text{Loss}}{\\partial w_1}$ means \"the partial derivative of loss with respect to $w_1$\"\n",
    "\n",
    "Now lets see how this **Partial Derivative** $\\frac{\\partial \\text{Loss}}{\\partial w_1}$ is computed Mathematicaly:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "where the predicted value is given by the linear model:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = w_1 x_{1i} + w_2 x_{2i} + w_0\n",
    "$$\n",
    "\n",
    "where $i$ means a student\n",
    "\n",
    "---\n",
    "\n",
    "1Ô∏è‚É£ **Substitute the prediction formula**\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} ((w_1 x_{1i} + w_2 x_{2i} + w_0) - y_i)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "2Ô∏è‚É£ **Take the partial derivative of Loss with respect to $w_1$**\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_1}\n",
    "= \\frac{\\partial}{\\partial w_1}\n",
    "\\left( \\frac{1}{n} \\sum_{i=1}^{n} ((w_1 x_{1i} + w_2 x_{2i} + w_0) - y_i)^2 \\right)\n",
    "$$\n",
    "\n",
    "Bring the constant $\\frac{1}{n}$ outside:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_1}\n",
    "= \\frac{1}{n} \\sum_{i=1}^{n}\n",
    "\\frac{\\partial}{\\partial w_1}\n",
    "((w_1 x_{1i} + w_2 x_{2i} + w_0 - y_i)^2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "3Ô∏è‚É£ **Apply the chain rule**\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_1} ((w_1 x_{1i} + w_2 x_{2i} + w_0 - y_i)^2)\n",
    "= 2(w_1 x_{1i} + w_2 x_{2i} + w_0 - y_i) \\cdot \\frac{\\partial}{\\partial w_1}(w_1 x_{1i} + w_2 x_{2i} + w_0 - y_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_1}(w_1 x_{1i} + w_2 x_{2i} + w_0 - y_i) = x_{1i}\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_1} ((w_1 x_{1i} + w_2 x_{2i} + w_0 - y_i)^2)\n",
    "= 2 x_{1i} (w_1 x_{1i} + w_2 x_{2i} + w_0 - y_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "4Ô∏è‚É£ **Substitute back**\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_1}\n",
    "= \\frac{2}{n} \\sum_{i=1}^{n} ((w_1 x_{1i} + w_2 x_{2i} + w_0) - y_i) x_{1i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "5Ô∏è‚É£ **Simplify**\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_1}\n",
    "= \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) x_{1i}\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Why derivatives help us minimize loss:**\n",
    "1. **Positive derivative** ‚Üí Loss increases as parameter increases ‚Üí We should **decrease** the parameter\n",
    "2. **Negative derivative** ‚Üí Loss decreases as parameter increases ‚Üí We should **increase** the parameter\n",
    "3. **Zero derivative** ‚Üí We're at a minimum (or maximum)!\n",
    "\n",
    "**The optimization rule:**\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial \\text{Loss}}{\\partial w}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the **learning rate** (how big a step we take)\n",
    "- We **subtract** the derivative to move in the direction that reduces loss\n",
    "\n",
    "Think of it like hiking down a mountain in fog:\n",
    "- The derivative tells you which way is downhill (steepest descent)\n",
    "- The learning rate determines how big your steps are\n",
    "- You keep taking steps downhill until you reach the bottom (minimum loss)!\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/631731_P7z2BKhd0R-9uyn9ThDasA.webp\" width=\"700\"/>\n",
    "  <p><i>Gradient Descent Curve</i></p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b499be",
   "metadata": {
    "id": "69b499be"
   },
   "source": [
    "---\n",
    "\n",
    "### **Conclusion: What We Learned**\n",
    "\n",
    "Congratulations! You've just built a complete linear regression model from scratch using PyTorch! üéâ\n",
    "\n",
    "**Key Concepts Covered:**\n",
    "\n",
    "1. **Linear Regression Basics**\n",
    "   - Modeling relationships between inputs and outputs\n",
    "   - Understanding weights, bias, and predictions\n",
    "\n",
    "2. **Loss Functions**\n",
    "   - Mean Squared Error (MSE) to measure prediction quality\n",
    "   - The goal: minimize loss to improve our model\n",
    "\n",
    "3. **Calculus & Gradients**\n",
    "   - Derivatives tell us how to improve parameters\n",
    "   - Chain rule for computing gradients\n",
    "   - Partial derivatives for multi-variable optimization\n",
    "\n",
    "4. **Gradient Descent Algorithm**\n",
    "   - Iteratively updating parameters to minimize loss\n",
    "   - Learning rate controls step size\n",
    "   - Forward pass ‚Üí compute loss ‚Üí compute gradients ‚Üí update parameters\n",
    "\n",
    "5. **Manual Implementation**\n",
    "   - Coded gradient formulas ourselves\n",
    "   - Understood the math behind the optimization\n",
    "\n",
    "6. **PyTorch Autograd**\n",
    "   - Automatic differentiation eliminates manual gradient coding\n",
    "   - `.backward()` does the calculus for us\n",
    "   - Same results with less code and no errors!\n",
    "\n",
    "**What's Next?**\n",
    "\n",
    "This simple linear regression is the foundation for:\n",
    "- **Deep Neural Networks** (stack multiple layers)\n",
    "- **Convolutional Neural Networks** (for images)\n",
    "- **Recurrent Neural Networks** (for sequences)\n",
    "- **Transformers** (for language models like GPT)\n",
    "\n",
    "The principles remain the same:\n",
    "1. Define a model\n",
    "2. Define a loss function\n",
    "3. Use gradients to optimize\n",
    "4. Let autograd handle the calculus!\n",
    "\n",
    "---\n",
    "\n",
    "**You now understand the core of deep learning!** üöÄ\n",
    "\n",
    "Every complex neural network is just this same process scaled up with more layers, parameters, and data. The mathematics of gradient descent and backpropagation powers everything from ChatGPT to self-driving cars!\n",
    "\n",
    "Keep exploring, and happy learning! üìö‚ú®"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "082f5e98",
    "41df5c06",
    "68ff469c",
    "e6f5f8c3",
    "f0773774",
    "627559b8",
    "680aa6dd",
    "eaed1689",
    "7f49d7db",
    "e2726bf0"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
