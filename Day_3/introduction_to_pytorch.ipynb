{"cells":[{"cell_type":"code","source":["# @title\n","from IPython.display import display, HTML\n","\n","display(HTML(\"\"\"\n","<script>\n","const firstCell = document.querySelector('.cell.code_cell');\n","if (firstCell) {\n","  firstCell.querySelector('.input').style.pointerEvents = 'none';\n","  firstCell.querySelector('.input').style.opacity = '0.5';\n","}\n","</script>\n","\"\"\"))\n","\n","html = \"\"\"\n","<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n","  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n","\n","  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n","       alt=\"Algopath Coding Academy Logo\"\n","       width=\"400\"\n","       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n","\n","  <p style=\"font-size:16px; margin:0;\">\n","    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n","  </p>\n","</div>\n","\"\"\"\n","\n","display(HTML(html))"],"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":412},"id":"0lAI4wJsxyUI","executionInfo":{"status":"ok","timestamp":1762107289572,"user_tz":0,"elapsed":16,"user":{"displayName":"Shariq Ali","userId":"00441094809974358112"}},"outputId":"46faec05-737f-4121-d4f1-ac1985f00332"},"id":"0lAI4wJsxyUI","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<script>\n","const firstCell = document.querySelector('.cell.code_cell');\n","if (firstCell) {\n","  firstCell.querySelector('.input').style.pointerEvents = 'none';\n","  firstCell.querySelector('.input').style.opacity = '0.5';\n","}\n","</script>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n","  <h1 style=\"margin:0;\">üëã Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n","\n","  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n","       alt=\"Algopath Coding Academy Logo\"\n","       width=\"400\"\n","       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n","\n","  <p style=\"font-size:16px; margin:0;\">\n","    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n","  </p>\n","</div>\n"]},"metadata":{}}]},{"cell_type":"markdown","id":"457f92fa","metadata":{"id":"457f92fa"},"source":["# PyTorch Tensors Tutorial\n","\n","\n","**Table of Contents:**\n","1. [Introduction to PyTorch](#1)\n","2. [Creating Tensors](#2)\n","3. [Tensor Attributes and Properties](#3)\n","4. [Tensor Indexing Slicing and Filtering](#4)\n","5. [Tensor Operations](#5)\n","6. [Tensor Manipulation](#6)\n","7. [GPU Interaction](#7)\n","8. [NumPy vs PyTorch Comparison](#8)\n","\n","---"]},{"cell_type":"markdown","id":"23bd53d1","metadata":{"id":"23bd53d1"},"source":["<a name='1'></a>\n","## **1. Introduction to PyTorch Tensors**"]},{"cell_type":"markdown","source":["### What is PyTorch?\n","PyTorch is an open-source deep learning library developed by Facebook's AI Research lab. It provides:\n","- Tensor computation with strong GPU acceleration\n","- Automatic differentiation for building neural networks\n","- A flexible and intuitive API for research and production\n","\n","### What is a Tensor?\n","A **tensor** is the core data structure in PyTorch. Think of it as:\n","- Similar to a NumPy `ndarray` but with additional capabilities\n","- Can run on GPUs for accelerated computing\n","- Supports automatic differentiation (autograd) for neural network training\n","- A multi-dimensional array that can represent scalars, vectors, matrices, and higher-dimensional data"],"metadata":{"id":"8m3FnYCs4iDO"},"id":"8m3FnYCs4iDO"},{"cell_type":"markdown","id":"df007a87","metadata":{"id":"df007a87"},"source":["### Importing PyTorch"]},{"cell_type":"code","execution_count":1,"id":"eb692616","metadata":{"id":"eb692616","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762202366615,"user_tz":0,"elapsed":10634,"user":{"displayName":"Shariq Ali","userId":"00441094809974358112"}},"outputId":"c79502e5-e38e-4d29-bd3d-ee1233ca0abc"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.8.0+cu126\n"]}],"source":["import torch\n","import numpy as np\n","\n","print(f\"PyTorch version: {torch.__version__}\")"]},{"cell_type":"markdown","id":"d54a611a","metadata":{"id":"d54a611a"},"source":["### Setting up your Device\n","Check if CUDA (GPU) is available and set the device accordingly:"]},{"cell_type":"code","execution_count":2,"id":"de0b3891","metadata":{"id":"de0b3891","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762202394435,"user_tz":0,"elapsed":43,"user":{"displayName":"Shariq Ali","userId":"00441094809974358112"}},"outputId":"21c3bad7-ee1c-45cd-c44c-7335390c87a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","GPU not available, using CPU\n"]}],"source":["# Check if CUDA is available\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"Using device: {device}\")\n","\n","if device == 'cuda':\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"GPU not available, using CPU\")"]},{"cell_type":"markdown","id":"67c184a2","metadata":{"id":"67c184a2"},"source":["---\n","<a name='2'></a>\n","## **2. Creating Tensors**"]},{"cell_type":"markdown","source":["### From Existing Data"],"metadata":{"id":"DscBqY7M4llk"},"id":"DscBqY7M4llk"},{"cell_type":"code","execution_count":null,"id":"3c299076","metadata":{"id":"3c299076"},"outputs":[],"source":["# Using torch.tensor() - from Python lists or tuples\n","tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n","print(\"From list:\", tensor_from_list)\n","\n","tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","print(\"\\n2D tensor from nested list:\")\n","print(tensor_2d)\n","\n","# Using torch.from_numpy() - from NumPy array (shares memory!)\n","np_array = np.array([1.0, 2.0, 3.0, 4.0])\n","tensor_from_numpy = torch.from_numpy(np_array)\n","print(\"\\nFrom NumPy array:\", tensor_from_numpy)\n","print(\"Note: This tensor shares memory with the NumPy array\")"]},{"cell_type":"markdown","id":"fcb3b29a","metadata":{"id":"fcb3b29a"},"source":["### Creating New Tensors"]},{"cell_type":"code","execution_count":null,"id":"5a5b053c","metadata":{"id":"5a5b053c"},"outputs":[],"source":["# torch.zeros() and torch.ones()\n","zeros_tensor = torch.zeros(3, 4)\n","print(\"Zeros tensor (3x4):\")\n","print(zeros_tensor)\n","\n","ones_tensor = torch.ones(2, 3, 5)\n","print(\"\\nOnes tensor (2x3x5):\")\n","print(ones_tensor)\n","\n","# torch.rand() - uniform distribution [0, 1)\n","rand_uniform = torch.rand(3, 3)\n","print(\"\\nRandom uniform [0, 1):\")\n","print(rand_uniform)\n","\n","# torch.randn() - standard normal distribution (mean=0, std=1)\n","rand_normal = torch.randn(3, 3)\n","print(\"\\nRandom normal (mean=0, std=1):\")\n","print(rand_normal)\n","\n","# torch.arange() - sequence of values\n","arange_tensor = torch.arange(0, 10, 2)\n","print(\"\\nArange (0 to 10, step 2):\", arange_tensor)\n","\n","# torch.linspace() - linearly spaced values\n","linspace_tensor = torch.linspace(0, 1, 5)\n","print(\"\\nLinspace (0 to 1, 5 values):\", linspace_tensor)"]},{"cell_type":"markdown","id":"6f402251","metadata":{"id":"6f402251"},"source":["### Creating Tensors Based on Other Tensors"]},{"cell_type":"code","execution_count":null,"id":"e9cb6212","metadata":{"id":"e9cb6212"},"outputs":[],"source":["# Create a reference tensor\n","reference_tensor = torch.rand(2, 3)\n","print(\"Reference tensor:\")\n","print(reference_tensor)\n","\n","# torch.zeros_like() and torch.ones_like()\n","zeros_like = torch.zeros_like(reference_tensor)\n","print(\"\\nZeros like reference:\")\n","print(zeros_like)\n","\n","ones_like = torch.ones_like(reference_tensor)\n","print(\"\\nOnes like reference:\")\n","print(ones_like)\n","\n","# torch.rand_like()\n","rand_like = torch.rand_like(reference_tensor)\n","print(\"\\nRandom like reference:\")\n","print(rand_like)"]},{"cell_type":"markdown","id":"dd5daebc","metadata":{"id":"dd5daebc"},"source":["---\n","<a name='3'></a>\n","## **3. Tensor Attributes and Properties**"]},{"cell_type":"code","execution_count":null,"id":"71e79efa","metadata":{"id":"71e79efa"},"outputs":[],"source":["# Create a sample tensor\n","sample_tensor = torch.randn(3, 4, 5)\n","\n","# Data type\n","print(f\"Data type (dtype): {sample_tensor.dtype}\")\n","\n","# Shape\n","print(f\"Shape: {sample_tensor.shape}\")\n","print(f\"Size (same as shape): {sample_tensor.size()}\")\n","\n","# Device\n","print(f\"Device: {sample_tensor.device}\")\n","\n","# Number of dimensions\n","print(f\"Number of dimensions (ndim): {sample_tensor.ndim}\")\n","\n","# Total number of elements\n","print(f\"Number of elements (numel): {sample_tensor.numel()}\")\n","\n","# Create tensors with specific dtypes\n","int_tensor = torch.tensor([1, 2, 3], dtype=torch.long)\n","float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n","print(f\"\\nInteger tensor dtype: {int_tensor.dtype}\")\n","print(f\"Float tensor dtype: {float_tensor.dtype}\")"]},{"cell_type":"markdown","id":"b8d156fe","metadata":{"id":"b8d156fe"},"source":["---\n","<a name='4'></a>\n","## **4. Tensor Indexing, Slicing, and Filtering**"]},{"cell_type":"code","execution_count":null,"id":"d03b2828","metadata":{"id":"d03b2828"},"outputs":[],"source":["# Create a sample tensor for indexing\n","tensor = torch.arange(24).reshape(4, 6)\n","print(\"Original tensor:\")\n","print(tensor)\n","\n","# Standard indexing\n","print(\"\\nElement at [0, 0]:\", tensor[0, 0])\n","print(\"Element at [2, 3]:\", tensor[2, 3])\n","\n","# Accessing rows and columns\n","print(\"\\nFirst row:\", tensor[0])\n","print(\"Second column:\", tensor[:, 1])\n","\n","# Slicing\n","print(\"\\nFirst 2 rows:\")\n","print(tensor[0:2])\n","\n","print(\"\\nRows 1-3, columns 2-4:\")\n","print(tensor[1:3, 2:4])\n","\n","print(\"\\nEvery other row:\")\n","print(tensor[::2])\n","\n","# Boolean/Masked indexing\n","mask = tensor > 10\n","print(\"\\nBoolean mask (elements > 10):\")\n","print(mask)\n","print(\"\\nFiltered values (> 10):\")\n","print(tensor[mask])\n","\n","# torch.where() - conditional selection\n","result = torch.where(tensor > 10, tensor, torch.tensor(0))\n","print(\"\\nUsing torch.where (replace values <= 10 with 0):\")\n","print(result)"]},{"cell_type":"markdown","id":"0b1e3e8a","metadata":{"id":"0b1e3e8a"},"source":["---\n","<a name='5'></a>\n","## **5. Tensor Operations**"]},{"cell_type":"markdown","source":["### Element-wise Arithmetic"],"metadata":{"id":"3wIYTj8B4pAQ"},"id":"3wIYTj8B4pAQ"},{"cell_type":"code","execution_count":null,"id":"16b17d2d","metadata":{"id":"16b17d2d"},"outputs":[],"source":["# Create sample tensors\n","a = torch.tensor([1.0, 2.0, 3.0])\n","b = torch.tensor([4.0, 5.0, 6.0])\n","\n","# Addition\n","print(\"Addition (a + b):\", a + b)\n","print(\"Addition (torch.add):\", torch.add(a, b))\n","\n","# Subtraction\n","print(\"\\nSubtraction (a - b):\", a - b)\n","\n","# Multiplication (element-wise)\n","print(\"\\nMultiplication (a * b):\", a * b)\n","\n","# Division\n","print(\"\\nDivision (a / b):\", a / b)\n","\n","# Power\n","print(\"\\nPower (a ** 2):\", a ** 2)\n","\n","# Scalar operations\n","print(\"\\nScalar multiplication (a * 3):\", a * 3)\n","print(\"Scalar addition (a + 10):\", a + 10)"]},{"cell_type":"markdown","id":"a40ed4af","metadata":{"id":"a40ed4af"},"source":["### In-place Operations\n","Operations ending with `_` modify the tensor in-place:"]},{"cell_type":"code","execution_count":null,"id":"ed7b9c1b","metadata":{"id":"ed7b9c1b"},"outputs":[],"source":["# In-place operations (methods ending with _)\n","x = torch.tensor([1.0, 2.0, 3.0])\n","print(\"Original x:\", x)\n","\n","# In-place addition\n","x.add_(5)\n","print(\"After x.add_(5):\", x)\n","\n","# In-place multiplication\n","x.mul_(2)\n","print(\"After x.mul_(2):\", x)\n","\n","# Note: In-place operations save memory but modify the original tensor\n","# Regular operations create new tensors"]},{"cell_type":"markdown","id":"99df9b58","metadata":{"id":"99df9b58"},"source":["### Matrix Operations"]},{"cell_type":"code","execution_count":null,"id":"a5f19db3","metadata":{"id":"a5f19db3"},"outputs":[],"source":["# Matrix multiplication\n","mat1 = torch.tensor([[1, 2], [3, 4]])\n","mat2 = torch.tensor([[5, 6], [7, 8]])\n","\n","print(\"Matrix 1:\")\n","print(mat1)\n","print(\"\\nMatrix 2:\")\n","print(mat2)\n","\n","# Matrix multiplication using torch.matmul()\n","result1 = torch.matmul(mat1, mat2)\n","print(\"\\nMatrix multiplication (torch.matmul):\")\n","print(result1)\n","\n","# Matrix multiplication using @ operator\n","result2 = mat1 @ mat2\n","print(\"\\nMatrix multiplication (@ operator):\")\n","print(result2)\n","\n","# Transpose\n","mat = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","print(\"\\nOriginal matrix:\")\n","print(mat)\n","print(\"Shape:\", mat.shape)\n","\n","print(\"\\nTransposed (using .T):\")\n","print(mat.T)\n","print(\"Shape:\", mat.T.shape)\n","\n","print(\"\\nTransposed (using .transpose()):\")\n","print(mat.transpose(0, 1))\n","print(\"Shape:\", mat.transpose(0, 1).shape)"]},{"cell_type":"markdown","id":"c8b8bdfb","metadata":{"id":"c8b8bdfb"},"source":["### Reduction Operations"]},{"cell_type":"code","execution_count":null,"id":"9756830f","metadata":{"id":"9756830f"},"outputs":[],"source":["# Create a sample tensor\n","tensor = torch.tensor([[1.0, 2.0, 3.0],\n","                       [4.0, 5.0, 6.0],\n","                       [7.0, 8.0, 9.0]])\n","print(\"Original tensor:\")\n","print(tensor)\n","\n","# Sum\n","print(\"\\nSum of all elements:\", torch.sum(tensor))\n","print(\"Sum along dimension 0 (columns):\", torch.sum(tensor, dim=0))\n","print(\"Sum along dimension 1 (rows):\", torch.sum(tensor, dim=1))\n","\n","# Mean\n","print(\"\\nMean of all elements:\", torch.mean(tensor))\n","print(\"Mean along dimension 0:\", torch.mean(tensor, dim=0))\n","\n","# Standard deviation\n","print(\"\\nStandard deviation:\", torch.std(tensor))\n","\n","# Max and Min\n","print(\"\\nMax value:\", torch.max(tensor))\n","print(\"Min value:\", torch.min(tensor))\n","\n","# Max/Min along a dimension\n","print(\"\\nMax along dimension 0:\", torch.max(tensor, dim=0))\n","print(\"Max along dimension 1:\", torch.max(tensor, dim=1))\n","\n","# Argmax (index of maximum value)\n","print(\"\\nArgmax (overall):\", torch.argmax(tensor))\n","print(\"Argmax along dimension 0:\", torch.argmax(tensor, dim=0))\n","print(\"Argmax along dimension 1:\", torch.argmax(tensor, dim=1))"]},{"cell_type":"markdown","id":"b5c6aa4e","metadata":{"id":"b5c6aa4e"},"source":["### Broadcasting\n","Broadcasting allows operations between tensors of different shapes:"]},{"cell_type":"code","execution_count":null,"id":"87871d74","metadata":{"id":"87871d74"},"outputs":[],"source":["# Broadcasting examples\n","a = torch.tensor([[1, 2, 3],\n","                  [4, 5, 6]])\n","b = torch.tensor([10, 20, 30])\n","\n","print(\"Tensor a (2x3):\")\n","print(a)\n","print(\"\\nTensor b (3,):\")\n","print(b)\n","\n","# Broadcasting: b is automatically expanded to match a's shape\n","result = a + b\n","print(\"\\nBroadcasted addition (a + b):\")\n","print(result)\n","\n","# Broadcasting with column vector\n","c = torch.tensor([[100], [200]])\n","print(\"\\nTensor c (2x1):\")\n","print(c)\n","\n","result2 = a + c\n","print(\"\\nBroadcasted addition (a + c):\")\n","print(result2)\n","\n","# Scalar broadcasting\n","result3 = a * 10\n","print(\"\\nScalar broadcasting (a * 10):\")\n","print(result3)"]},{"cell_type":"markdown","id":"c71802da","metadata":{"id":"c71802da"},"source":["---\n","<a name='6'></a>\n","## **6. Tensor Manipulation (Reshaping)**"]},{"cell_type":"markdown","source":["### Reshaping Tensors"],"metadata":{"id":"vYcGjw7n4rbe"},"id":"vYcGjw7n4rbe"},{"cell_type":"code","execution_count":null,"id":"2e94dee0","metadata":{"id":"2e94dee0"},"outputs":[],"source":["# Create a sample tensor\n","tensor = torch.arange(12)\n","print(\"Original tensor:\", tensor)\n","print(\"Shape:\", tensor.shape)\n","\n","# torch.reshape() - can return a copy or a view\n","reshaped1 = torch.reshape(tensor, (3, 4))\n","print(\"\\nReshaped to (3, 4) using torch.reshape():\")\n","print(reshaped1)\n","\n","reshaped2 = torch.reshape(tensor, (2, 6))\n","print(\"\\nReshaped to (2, 6):\")\n","print(reshaped2)\n","\n","# .view() - returns a view (must be contiguous)\n","viewed = tensor.view(4, 3)\n","print(\"\\nReshaped to (4, 3) using .view():\")\n","print(viewed)\n","\n","# Using -1 to infer dimension\n","auto_reshape = tensor.view(3, -1)\n","print(\"\\nReshaped to (3, -1) - auto-infer second dimension:\")\n","print(auto_reshape)\n","print(\"Shape:\", auto_reshape.shape)"]},{"cell_type":"markdown","id":"014b728a","metadata":{"id":"014b728a"},"source":["### Changing Dimensions"]},{"cell_type":"code","execution_count":null,"id":"db24b86a","metadata":{"id":"db24b86a"},"outputs":[],"source":["# torch.squeeze() - remove dimensions of size 1\n","tensor_with_ones = torch.randn(1, 3, 1, 4)\n","print(\"Original shape:\", tensor_with_ones.shape)\n","\n","squeezed = torch.squeeze(tensor_with_ones)\n","print(\"After squeeze:\", squeezed.shape)\n","\n","# Squeeze specific dimension\n","squeezed_dim = torch.squeeze(tensor_with_ones, dim=0)\n","print(\"After squeeze(dim=0):\", squeezed_dim.shape)\n","\n","# torch.unsqueeze() - add a dimension of size 1\n","tensor = torch.randn(3, 4)\n","print(\"\\nOriginal shape:\", tensor.shape)\n","\n","unsqueezed_0 = torch.unsqueeze(tensor, dim=0)\n","print(\"After unsqueeze(dim=0):\", unsqueezed_0.shape)\n","\n","unsqueezed_1 = torch.unsqueeze(tensor, dim=1)\n","print(\"After unsqueeze(dim=1):\", unsqueezed_1.shape)\n","\n","unsqueezed_2 = torch.unsqueeze(tensor, dim=2)\n","print(\"After unsqueeze(dim=2):\", unsqueezed_2.shape)"]},{"cell_type":"markdown","id":"2f653c44","metadata":{"id":"2f653c44"},"source":["### Combining Tensors"]},{"cell_type":"code","execution_count":null,"id":"3ec8534d","metadata":{"id":"3ec8534d"},"outputs":[],"source":["# torch.cat() - concatenate along an existing dimension\n","a = torch.tensor([[1, 2], [3, 4]])\n","b = torch.tensor([[5, 6], [7, 8]])\n","\n","print(\"Tensor a:\")\n","print(a)\n","print(\"\\nTensor b:\")\n","print(b)\n","\n","# Concatenate along dimension 0 (rows)\n","cat_dim0 = torch.cat([a, b], dim=0)\n","print(\"\\nConcatenated along dim=0:\")\n","print(cat_dim0)\n","print(\"Shape:\", cat_dim0.shape)\n","\n","# Concatenate along dimension 1 (columns)\n","cat_dim1 = torch.cat([a, b], dim=1)\n","print(\"\\nConcatenated along dim=1:\")\n","print(cat_dim1)\n","print(\"Shape:\", cat_dim1.shape)\n","\n","# torch.stack() - stack along a new dimension\n","stacked_dim0 = torch.stack([a, b], dim=0)\n","print(\"\\nStacked along dim=0 (new dimension):\")\n","print(stacked_dim0)\n","print(\"Shape:\", stacked_dim0.shape)\n","\n","stacked_dim1 = torch.stack([a, b], dim=1)\n","print(\"\\nStacked along dim=1 (new dimension):\")\n","print(stacked_dim1)\n","print(\"Shape:\", stacked_dim1.shape)"]},{"cell_type":"markdown","id":"95bf25e7","metadata":{"id":"95bf25e7"},"source":["### Splitting Tensors"]},{"cell_type":"code","execution_count":null,"id":"caf7c01d","metadata":{"id":"caf7c01d"},"outputs":[],"source":["# Create a tensor to split\n","tensor = torch.arange(12).reshape(3, 4)\n","print(\"Original tensor:\")\n","print(tensor)\n","\n","# torch.split() - split into chunks of a given size\n","splits = torch.split(tensor, 2, dim=0)\n","print(\"\\nSplit into chunks of size 2 along dim=0:\")\n","for i, split in enumerate(splits):\n","    print(f\"Chunk {i}:\")\n","    print(split)\n","\n","# torch.chunk() - split into a specific number of chunks\n","chunks = torch.chunk(tensor, 2, dim=1)\n","print(\"\\nSplit into 2 chunks along dim=1:\")\n","for i, chunk in enumerate(chunks):\n","    print(f\"Chunk {i}:\")\n","    print(chunk)"]},{"cell_type":"markdown","id":"cf3e228d","metadata":{"id":"cf3e228d"},"source":["### Reordering Dimensions"]},{"cell_type":"code","execution_count":null,"id":"4f0dea29","metadata":{"id":"4f0dea29"},"outputs":[],"source":["# tensor.permute() - reorder dimensions\n","tensor = torch.randn(2, 3, 4)\n","print(\"Original shape:\", tensor.shape)\n","print(\"Original dimensions: (batch, height, width)\")\n","\n","# Permute dimensions\n","permuted = tensor.permute(2, 0, 1)\n","print(\"\\nPermuted shape:\", permuted.shape)\n","print(\"Permuted dimensions: (width, batch, height)\")\n","\n","# Example with image data (common use case)\n","# Original: (batch_size, height, width, channels) -> (batch_size, channels, height, width)\n","image_tensor = torch.randn(32, 224, 224, 3)  # 32 images, 224x224, RGB\n","print(\"\\nOriginal image tensor (NHWC):\", image_tensor.shape)\n","\n","image_permuted = image_tensor.permute(0, 3, 1, 2)  # Convert to NCHW\n","print(\"Permuted image tensor (NCHW):\", image_permuted.shape)"]},{"cell_type":"markdown","id":"f94b10c4","metadata":{"id":"f94b10c4"},"source":["---\n","<a name='7'></a>\n","## **7. NumPy & GPU Interaction**"]},{"cell_type":"markdown","source":["### NumPy Bridge"],"metadata":{"id":"3UPIkt254ukG"},"id":"3UPIkt254ukG"},{"cell_type":"code","execution_count":null,"id":"0916393d","metadata":{"id":"0916393d"},"outputs":[],"source":["# CPU Tensors and NumPy arrays share memory\n","np_array = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n","print(\"Original NumPy array:\", np_array)\n","\n","# Convert NumPy to Tensor (shares memory)\n","tensor_from_np = torch.from_numpy(np_array)\n","print(\"Tensor from NumPy:\", tensor_from_np)\n","\n","# Modify the tensor\n","tensor_from_np[0] = 100\n","print(\"\\nAfter modifying tensor:\")\n","print(\"Tensor:\", tensor_from_np)\n","print(\"NumPy array (also changed!):\", np_array)\n","\n","# Convert Tensor to NumPy (only works on CPU tensors)\n","torch_tensor = torch.tensor([10.0, 20.0, 30.0, 40.0])\n","np_from_tensor = torch_tensor.numpy()\n","print(\"\\nTensor:\", torch_tensor)\n","print(\"NumPy from tensor:\", np_from_tensor)\n","\n","# Modify NumPy array\n","np_from_tensor[0] = 999\n","print(\"\\nAfter modifying NumPy array:\")\n","print(\"NumPy array:\", np_from_tensor)\n","print(\"Tensor (also changed!):\", torch_tensor)\n","\n","print(\"\\n‚ö†Ô∏è Note: CPU Tensors and NumPy arrays share the same memory location!\")"]},{"cell_type":"markdown","id":"c54a68a3","metadata":{"id":"c54a68a3"},"source":["### Moving Tensors Between CPU and GPU"]},{"cell_type":"code","execution_count":null,"id":"c3e29655","metadata":{"id":"c3e29655"},"outputs":[],"source":["# Create a CPU tensor\n","cpu_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\n","print(\"CPU Tensor:\", cpu_tensor)\n","print(\"Device:\", cpu_tensor.device)\n","\n","# Move to GPU (if available)\n","if torch.cuda.is_available():\n","    # Method 1: Using .to(device)\n","    gpu_tensor = cpu_tensor.to(device)\n","    print(\"\\nGPU Tensor (using .to()):\", gpu_tensor)\n","    print(\"Device:\", gpu_tensor.device)\n","\n","    # Method 2: Using .cuda()\n","    gpu_tensor2 = cpu_tensor.cuda()\n","    print(\"\\nGPU Tensor (using .cuda()):\", gpu_tensor2)\n","    print(\"Device:\", gpu_tensor2.device)\n","\n","    # Move back to CPU\n","    back_to_cpu = gpu_tensor.cpu()\n","    print(\"\\nBack to CPU:\", back_to_cpu)\n","    print(\"Device:\", back_to_cpu.device)\n","\n","    # Convert GPU tensor to NumPy (requires moving to CPU first)\n","    # gpu_tensor.numpy()  # This would raise an error!\n","    np_array = gpu_tensor.cpu().numpy()\n","    print(\"\\nNumPy array from GPU tensor:\", np_array)\n","else:\n","    print(\"\\n‚ö†Ô∏è CUDA not available. Tensor remains on CPU.\")\n","    print(\"To use GPU features, ensure you have:\")\n","    print(\"1. A CUDA-capable GPU\")\n","    print(\"2. CUDA toolkit installed\")\n","    print(\"3. PyTorch with CUDA support installed\")"]},{"cell_type":"markdown","id":"85008f93","metadata":{"id":"85008f93"},"source":["---\n","<a name='8'></a>\n","## **8. NumPy vs PyTorch: Comparison and Benefits**"]},{"cell_type":"markdown","source":["### Key Similarities"],"metadata":{"id":"k7vZyjFz409f"},"id":"k7vZyjFz409f"},{"cell_type":"code","execution_count":null,"id":"35d5723f","metadata":{"id":"35d5723f"},"outputs":[],"source":["# Similar operations in NumPy and PyTorch\n","print(\"=\" * 60)\n","print(\"NUMPY vs PYTORCH - Similar Syntax\")\n","print(\"=\" * 60)\n","\n","# Creating arrays/tensors\n","np_arr = np.array([1, 2, 3, 4, 5])\n","torch_tensor = torch.tensor([1, 2, 3, 4, 5])\n","\n","print(\"\\nNumPy array:\", np_arr)\n","print(\"PyTorch tensor:\", torch_tensor)\n","\n","# Zeros\n","np_zeros = np.zeros((3, 3))\n","torch_zeros = torch.zeros(3, 3)\n","print(\"\\nNumPy zeros:\\n\", np_zeros)\n","print(\"PyTorch zeros:\\n\", torch_zeros)\n","\n","# Random values\n","np_rand = np.random.randn(2, 3)\n","torch_rand = torch.randn(2, 3)\n","print(\"\\nNumPy random:\\n\", np_rand)\n","print(\"PyTorch random:\\n\", torch_rand)\n","\n","# Reshaping\n","np_reshaped = np_arr.reshape(5, 1)\n","torch_reshaped = torch_tensor.reshape(5, 1)\n","print(\"\\nNumPy reshaped:\\n\", np_reshaped)\n","print(\"PyTorch reshaped:\\n\", torch_reshaped)\n","\n","# Mathematical operations\n","print(\"\\nNumPy mean:\", np_arr.mean())\n","print(\"PyTorch mean:\", torch_tensor.float().mean())\n","\n","print(\"\\nNumPy sum:\", np_arr.sum())\n","print(\"PyTorch sum:\", torch_tensor.sum())"]},{"cell_type":"markdown","id":"42486fd3","metadata":{"id":"42486fd3"},"source":["### Why PyTorch for Machine Learning? Key Benefits\n","\n","PyTorch offers several advantages over NumPy for machine learning tasks:\n","\n","#### 1. **GPU Acceleration**\n","- PyTorch tensors can seamlessly move between CPU and GPU\n","- Massive speedup for large-scale computations (10-100x faster)\n","- Essential for training deep neural networks\n","\n","#### 2. **Automatic Differentiation (Autograd)**\n","- Automatically computes gradients for backpropagation\n","- Critical for training neural networks\n","- No need to manually derive and implement gradient calculations\n","\n","#### 3. **Built for Deep Learning**\n","- Rich ecosystem of neural network layers, optimizers, and loss functions\n","- Easy model building with `torch.nn` module\n","- Pre-trained models and transfer learning support\n","\n","#### 4. **Dynamic Computation Graphs**\n","- Graphs are built on-the-fly, allowing for flexible architectures\n","- Easier debugging compared to static graphs\n","- Supports variable-length inputs and conditional logic\n","\n","#### 5. **Production Ready**\n","- TorchScript for model deployment\n","- ONNX support for interoperability\n","- Mobile deployment with PyTorch Mobile\n","\n","#### 6. **Strong Community and Ecosystem**\n","- Extensive libraries (torchvision, torchaudio, etc.)\n","- Active research community\n","- Excellent documentation and tutorials"]},{"cell_type":"markdown","id":"eebc4d4c","metadata":{"id":"eebc4d4c"},"source":["### Demonstration: GPU Speedup"]},{"cell_type":"code","execution_count":null,"id":"c4fa7898","metadata":{"id":"c4fa7898"},"outputs":[],"source":["import time\n","\n","# Large matrix multiplication comparison\n","size = 5000\n","\n","# NumPy (CPU only)\n","np_a = np.random.randn(size, size)\n","np_b = np.random.randn(size, size)\n","\n","start = time.time()\n","np_result = np.dot(np_a, np_b)\n","np_time = time.time() - start\n","\n","print(f\"NumPy (CPU) time: {np_time:.4f} seconds\")\n","\n","# PyTorch CPU\n","torch_a_cpu = torch.randn(size, size)\n","torch_b_cpu = torch.randn(size, size)\n","\n","start = time.time()\n","torch_result_cpu = torch.matmul(torch_a_cpu, torch_b_cpu)\n","torch_cpu_time = time.time() - start\n","\n","print(f\"PyTorch (CPU) time: {torch_cpu_time:.4f} seconds\")\n","\n","# PyTorch GPU (if available)\n","if torch.cuda.is_available():\n","    torch_a_gpu = torch.randn(size, size).to(device)\n","    torch_b_gpu = torch.randn(size, size).to(device)\n","\n","    # Warm up GPU\n","    _ = torch.matmul(torch_a_gpu, torch_b_gpu)\n","    torch.cuda.synchronize()\n","\n","    start = time.time()\n","    torch_result_gpu = torch.matmul(torch_a_gpu, torch_b_gpu)\n","    torch.cuda.synchronize()\n","    torch_gpu_time = time.time() - start\n","\n","    print(f\"PyTorch (GPU) time: {torch_gpu_time:.4f} seconds\")\n","    print(f\"\\nüöÄ GPU Speedup: {torch_cpu_time / torch_gpu_time:.2f}x faster than CPU\")\n","else:\n","    print(\"\\n‚ö†Ô∏è GPU not available for speed comparison\")"]},{"cell_type":"markdown","id":"eadd5d9e","metadata":{"id":"eadd5d9e"},"source":["### Demonstration: Automatic Differentiation (Autograd)\n","This is PyTorch's killer feature for machine learning!"]},{"cell_type":"code","execution_count":null,"id":"8b663998","metadata":{"id":"8b663998"},"outputs":[],"source":["# Automatic differentiation example\n","# Enable gradient tracking with requires_grad=True\n","x = torch.tensor(2.0, requires_grad=True)\n","y = torch.tensor(3.0, requires_grad=True)\n","\n","# Define a computation\n","z = x**2 + y**3\n","print(f\"x = {x.item()}\")\n","print(f\"y = {y.item()}\")\n","print(f\"z = x¬≤ + y¬≥ = {z.item()}\")\n","\n","# Compute gradients automatically\n","z.backward()\n","\n","# Access gradients\n","print(f\"\\n‚àÇz/‚àÇx = 2x = {x.grad.item()}\")  # Should be 2*x = 4\n","print(f\"‚àÇz/‚àÇy = 3y¬≤ = {y.grad.item()}\")  # Should be 3*y¬≤ = 27\n","\n","print(\"\\n‚ú® PyTorch automatically computed these gradients!\")\n","print(\"This is essential for training neural networks with backpropagation.\")\n","\n","# NumPy cannot do this - you'd have to compute gradients manually\n","print(\"\\n‚ö†Ô∏è NumPy doesn't have automatic differentiation.\")\n","print(\"You would need to manually derive and implement gradient calculations.\")"]},{"cell_type":"markdown","id":"5a190a45","metadata":{"id":"5a190a45"},"source":["### Summary Comparison Table\n","\n","| Feature | NumPy | PyTorch |\n","|---------|-------|---------|\n","| **Data Structure** | ndarray | Tensor |\n","| **GPU Support** | ‚ùå No | ‚úÖ Yes |\n","| **Automatic Differentiation** | ‚ùå No | ‚úÖ Yes (Autograd) |\n","| **Deep Learning** | ‚ùå Not built for it | ‚úÖ Purpose-built |\n","| **Speed (CPU)** | Very fast | Very fast |\n","| **Speed (GPU)** | N/A | 10-100x faster |\n","| **Use Case** | General numerical computing | Machine Learning & DL |\n","| **Syntax** | Similar | Similar to NumPy |\n","| **Ecosystem** | SciPy, scikit-learn | torchvision, torchaudio |\n","| **Learning Curve** | Moderate | Easy if you know NumPy |\n","\n","### When to Use What?\n","\n","**Use NumPy when:**\n","- Doing general numerical computations\n","- Working with small to medium datasets\n","- Not training neural networks\n","- CPU processing is sufficient\n","\n","**Use PyTorch when:**\n","- Building and training neural networks\n","- Need GPU acceleration\n","- Require automatic differentiation\n","- Working on deep learning projects\n","- Need production deployment of ML models"]},{"cell_type":"markdown","id":"b2cb46cf","metadata":{"id":"b2cb46cf"},"source":["## Conclusion\n","\n","In this tutorial, you've learned:\n","\n","1. ‚úÖ What PyTorch is and why it's essential for deep learning\n","2. ‚úÖ How to create tensors in various ways\n","3. ‚úÖ Tensor attributes and properties\n","4. ‚úÖ Indexing, slicing, and filtering tensors\n","5. ‚úÖ Essential tensor operations (arithmetic, matrix ops, reductions)\n","6. ‚úÖ Tensor manipulation (reshaping, combining, splitting)\n","7. ‚úÖ NumPy integration and GPU acceleration\n","8. ‚úÖ Key differences between NumPy and PyTorch\n","9. ‚úÖ Why PyTorch is superior for machine learning\n","\n","### Next Steps\n","\n","- Explore PyTorch's autograd in depth\n","- Learn about `torch.nn` for building neural networks\n","- Practice with real datasets using `torch.utils.data`\n","- Implement your first neural network!\n","\n","**Happy Learning! üöÄ**"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"collapsed_sections":["23bd53d1","67c184a2","dd5daebc","b8d156fe","0b1e3e8a","c71802da","f94b10c4","85008f93"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}