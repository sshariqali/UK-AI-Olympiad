{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22338f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "const firstCell = document.querySelector('.cell.code_cell');\n",
    "if (firstCell) {\n",
    "  firstCell.querySelector('.input').style.pointerEvents = 'none';\n",
    "  firstCell.querySelector('.input').style.opacity = '0.5';\n",
    "}\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "html = \"\"\"\n",
    "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:12px; padding:8px;\">\n",
    "  <h1 style=\"margin:0;\">ğŸ‘‹ Welcome to <span style=\"color:#1E88E5;\">Algopath Coding Academy</span>!</h1>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/sshariqali/mnist_pretrained_model/main/algopath_logo.jpg\"\n",
    "       alt=\"Algopath Coding Academy Logo\"\n",
    "       width=\"400\"\n",
    "       style=\"border-radius:15px; box-shadow:0 4px 12px rgba(0,0,0,0.2); max-width:100%; height:auto;\" />\n",
    "\n",
    "  <p style=\"font-size:16px; margin:0;\">\n",
    "    <em>Empowering young minds to think creatively, code intelligently, and build the future with AI.</em>\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a06953",
   "metadata": {},
   "source": [
    "## Day 8 - Part 7: Scaling Up to a Real GPT! ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "### **The Complete Journey**\n",
    "\n",
    "**Day 7:**\n",
    "- Part 1-2: Bigram Language Model (simple baseline)\n",
    "- Part 3: Understanding Attention (Q, K, V concept)\n",
    "\n",
    "**Day 8:**\n",
    "- Part 3: Single-Head Attention\n",
    "- Part 4: Multi-Head Attention\n",
    "- Part 5: Complete Transformer Block (+ FFN)\n",
    "- Part 6: Add & Norm (LayerNorm + Residuals)\n",
    "- **Part 7: SCALING UP!** â† ğŸ“ You are here!\n",
    "\n",
    "---\n",
    "\n",
    "### **Agenda for this Notebook**\n",
    "\n",
    "| Section | Topic | Description |\n",
    "|:-------:|-------|-------------|\n",
    "| 1 | **Setup** | Data pipeline (unchanged) |\n",
    "| 2 | **ğŸ†• Scaled Hyperparameters** | Real GPT-like settings |\n",
    "| 3 | **Components with Dropout** | Regularization for large models |\n",
    "| 4 | **Stacked Blocks** | Multiple Transformer blocks |\n",
    "| 5 | **Pre-trained Model** | Load saved weights |\n",
    "| 6 | **Generation** | Generate real Shakespeare! |\n",
    "\n",
    "---\n",
    "\n",
    "### **What We're Scaling**\n",
    "\n",
    "| Parameter | Before | Now | Real GPT-3 |\n",
    "|-----------|--------|-----|------------|\n",
    "| `n_embed` | 32 | **384** | 12,288 |\n",
    "| `block_size` | 8 | **256** | 2,048 |\n",
    "| `n_head` | 4 | **6** | 96 |\n",
    "| `n_layer` | 1 | **6** | 96 |\n",
    "| `dropout` | 0.0 | **0.2** | 0.1 |\n",
    "\n",
    "**New Additions:**\n",
    "- **Dropout**: Randomly zeros activations during training (prevents overfitting)\n",
    "- **Multiple Blocks**: Stack 6 Transformer blocks for deeper processing\n",
    "- **Final LayerNorm**: Normalize before the final prediction head\n",
    "\n",
    "This is a **real language model** that can generate coherent Shakespeare! ğŸ­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77802b55",
   "metadata": {
    "id": "77802b55"
   },
   "source": [
    "---\n",
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301e45f7",
   "metadata": {
    "id": "301e45f7"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2213216",
   "metadata": {
    "id": "a2213216"
   },
   "source": [
    "---\n",
    "## Section 2: Scaled Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e96afbc",
   "metadata": {},
   "source": [
    "**Notice the dramatically different values!**\n",
    "\n",
    "| Parameter | Old Value | New Value | Why? |\n",
    "|-----------|-----------|-----------|------|\n",
    "| `block_size` | 8 | **256** | Longer context = better understanding |\n",
    "| `n_embed` | 32 | **384** | More dimensions = richer representations |\n",
    "| `n_head` | 4 | **6** | More heads = more attention patterns |\n",
    "| `n_layer` | 1 | **6** | More blocks = deeper processing |\n",
    "| `dropout` | 0.0 | **0.2** | Regularization for large models |\n",
    "| `learning_rate` | 1e-3 | **3e-4** | Smaller LR for stable training |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed3dddd",
   "metadata": {
    "id": "4ed3dddd"
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # how many independent sequences to process in parallel\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embed = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af594b",
   "metadata": {
    "id": "98af594b"
   },
   "source": [
    "---\n",
    "## Section 3: Data Pipeline (Same as Before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb260d1",
   "metadata": {
    "id": "bfb260d1"
   },
   "outputs": [],
   "source": [
    "# Load the tiny shakespeare dataset\n",
    "dataset = \"tiny_shakespeare.txt\"\n",
    "\n",
    "# Load the dataset into a string\n",
    "with open(dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c999dda",
   "metadata": {
    "id": "8c999dda"
   },
   "source": [
    "Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f242695",
   "metadata": {
    "id": "0f242695"
   },
   "outputs": [],
   "source": [
    "stoi = { ch: i for i, ch in enumerate(chars)}\n",
    "itos = { i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189f198",
   "metadata": {
    "id": "3189f198"
   },
   "source": [
    "Data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93cee0fd",
   "metadata": {
    "id": "93cee0fd"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8056b32",
   "metadata": {
    "id": "f8056b32"
   },
   "source": [
    "Batch loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba2c0ad",
   "metadata": {
    "id": "aba2c0ad"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of input-target pairs\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1a940",
   "metadata": {
    "id": "8ae1a940"
   },
   "source": [
    "Loss estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b75acd4",
   "metadata": {
    "id": "3b75acd4"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3da25",
   "metadata": {
    "id": "ffe3da25"
   },
   "source": [
    "---\n",
    "## Section 4: Model Components with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17781d",
   "metadata": {},
   "source": [
    "**ğŸ†• Dropout Added Throughout!**\n",
    "\n",
    "Dropout randomly \"drops\" (zeros out) neurons during training:\n",
    "- Prevents the model from relying on any single neuron\n",
    "- Acts as regularization, reducing overfitting\n",
    "- Only active during training (disabled at inference)\n",
    "\n",
    "**Where we add dropout:**\n",
    "1. After attention weights (before multiplying with V)\n",
    "2. After the projection in Multi-Head Attention\n",
    "3. After the FFN\n",
    "\n",
    "```python\n",
    "self.dropout = torch.nn.Dropout(dropout)  # dropout = 0.2 means 20% dropped\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3821eff2",
   "metadata": {
    "id": "3821eff2"
   },
   "outputs": [],
   "source": [
    "class Head(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af42471",
   "metadata": {
    "id": "6af42471"
   },
   "source": [
    "Multi-Head Attention (with dropout):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e0c40e",
   "metadata": {
    "id": "71e0c40e"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = torch.nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdf38e",
   "metadata": {
    "id": "73bdf38e"
   },
   "source": [
    "Feed-Forward Network (with dropout):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "781babea",
   "metadata": {
    "id": "781babea"
   },
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, n_embed * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_embed * 4, n_embed),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab086854",
   "metadata": {
    "id": "ab086854"
   },
   "source": [
    "Complete Block (same as Part 6, but using components with dropout):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f59e79",
   "metadata": {
    "id": "e1f59e79"
   },
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.ln2 = torch.nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f380cee",
   "metadata": {
    "id": "4f380cee"
   },
   "source": [
    "---\n",
    "## Section 5: Complete Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcf900",
   "metadata": {},
   "source": [
    "**Key changes for scaling:**\n",
    "\n",
    "1. **Multiple Blocks**: Instead of 1 block, we stack `n_layer = 6` blocks!\n",
    "```python\n",
    "self.blocks = nn.Sequential(*[Block(...) for _ in range(n_layer)])\n",
    "```\n",
    "\n",
    "2. **Final LayerNorm**: Before the LM head, one more normalization\n",
    "```python\n",
    "self.ln_f = nn.LayerNorm(n_embed)\n",
    "```\n",
    "\n",
    "**Architecture:**\n",
    "**IMAGE HERE**\n",
    "\n",
    "This is essentially a **mini-GPT**! Same architecture, just smaller scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20ddb9",
   "metadata": {
    "id": "8a20ddb9"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = torch.nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = torch.nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = torch.nn.LayerNorm(n_embed)\n",
    "        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, y = None):\n",
    "        # idx is (B, T) tensor of indices.\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        # Loss\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = torch.functional.F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) tensor of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim = -1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841eb8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7841eb8e",
    "outputId": "775d112a-e835-4b75-b12e-dd22cdc65720"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('model_weights.pth', map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd89db",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Loading Pre-trained Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec339a",
   "metadata": {},
   "source": [
    "Training this model from scratch takes a while (~15-30 minutes on GPU). \n",
    "\n",
    "We've pre-trained the model and saved the weights. Let's load them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dcfa6",
   "metadata": {
    "id": "fa7dcfa6"
   },
   "source": [
    "---\n",
    "## Section 7: Training (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f309a",
   "metadata": {},
   "source": [
    "**âš ï¸ Training is commented out** because we loaded pre-trained weights.\n",
    "\n",
    "If you want to train from scratch:\n",
    "1. Comment out the `model.load_state_dict(...)` line above\n",
    "2. Uncomment the training loop below\n",
    "3. Run for ~5000 iterations (takes 15-30 min on GPU)\n",
    "\n",
    "**Training Tips:**\n",
    "- Lower learning rate (3e-4) is crucial for stable training\n",
    "- Watch for val_loss to decrease along with train_loss\n",
    "- If val_loss increases while train_loss decreases â†’ overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083a5aba",
   "metadata": {
    "id": "083a5aba"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f11cfa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f11cfa1",
    "outputId": "e4dba04d-cca9-4b50-fa1a-3aef444dcf42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2846, val loss 4.2820\n",
      "step 500: train loss 2.1101, val loss 2.1662\n",
      "step 1000: train loss 1.7105, val loss 1.8663\n",
      "step 1500: train loss 1.5289, val loss 1.7154\n",
      "step 2000: train loss 1.4320, val loss 1.6319\n",
      "step 2500: train loss 1.3596, val loss 1.5877\n",
      "step 3000: train loss 1.3164, val loss 1.5455\n",
      "step 3500: train loss 1.2737, val loss 1.5340\n",
      "step 4000: train loss 1.2385, val loss 1.5005\n",
      "step 4500: train loss 1.2085, val loss 1.4999\n"
     ]
    }
   ],
   "source": [
    "# for i in range(max_iters):\n",
    "\n",
    "#     if i % eval_interval == 0:\n",
    "#         losses = estimate_loss()\n",
    "#         print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "#     # sample a batch of data\n",
    "#     xb, yb = get_batch('train')\n",
    "\n",
    "#     # evaluate the loss\n",
    "#     logits, loss = model(xb, yb)\n",
    "\n",
    "#     # perform backpropagation\n",
    "#     loss.backward()\n",
    "\n",
    "#     # update the weights\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # zero the gradients\n",
    "#     optimizer.zero_grad(set_to_none = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c484f3b",
   "metadata": {},
   "source": [
    "Save model weights (for next time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "-UJCOjgLEmOl",
   "metadata": {
    "id": "-UJCOjgLEmOl"
   },
   "outputs": [],
   "source": [
    "# Save the model's weights\n",
    "\n",
    "# torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082eef5c",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Text Generation - The Finale!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c3040",
   "metadata": {},
   "source": [
    "**The moment of truth!** Let's generate 1000 characters of Shakespeare-like text.\n",
    "\n",
    "Our model has learned:\n",
    "- Character patterns and spelling\n",
    "- Word boundaries and common words\n",
    "- Simple grammatical structures\n",
    "- Even some character names and dialogue patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e48318cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e48318cd",
    "outputId": "8a3fbe48-64f9-4995-cdf5-5c6b42ffc52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Nurse:\n",
      "Seeing with that we should some be done,\n",
      "I'll not deliver such backed with her back:\n",
      "As way, away, what fall out of my noble mood,\n",
      "And bloody well make my father hand late:\n",
      "Give me thee them them on that we love down upon,\n",
      "If courage preventive her years in she rooten\n",
      "Against my life to that more would do piece,\n",
      "So thought that ever now poor great to\n",
      "The womb cruptiffs are great live, I may post\n",
      "To fight do I of.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Saddle men, gentle mine mischarate\n",
      "His honourable carelering nature,\n",
      "By though thy beam of a cells of permised:\n",
      "And madam, thou shadows thy witchford, my countryman!\n",
      "\n",
      "DUKE OF YORK:\n",
      "My fortune kiss's weapons in my stead?\n",
      "Canst thou shalt entreated: therefore, fear those thought\n",
      "May sucking to low her eye out as any barb'd;\n",
      "Which did cowards she for my lamb despire\n",
      "By the right creature, helpess upon the name,\n",
      "That teyrs of his father's nose fellows tongues.\n",
      "\n",
      "GREEN:\n",
      "Be that will say this be be thou shalt companied\n",
      "By this imposition, for God, know her\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype = torch.long).to(device) # Since idx 0 is a new line character\n",
    "out = model.generate(context, max_new_tokens = 1000)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024362a",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ‰ Congratulations! You've Built a GPT!\n",
    "\n",
    "### ğŸ“Š What You've Accomplished\n",
    "\n",
    "Over Day 7 and Day 8, you've built a complete **GPT-style language model** from scratch!\n",
    "\n",
    "**Architecture Summary:**\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         TRANSFORMER                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Input Token IDs (B, T)                                          â”‚\n",
    "â”‚       â†“                                                          â”‚\n",
    "â”‚  Token Embedding (vocab_size â†’ n_embed)                          â”‚\n",
    "â”‚       +                                                          â”‚\n",
    "â”‚  Position Embedding (block_size â†’ n_embed)                       â”‚\n",
    "â”‚       â†“                                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\n",
    "â”‚  â”‚         TRANSFORMER BLOCK Ã— 6          â”‚                       â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                       â”‚\n",
    "â”‚  â”‚  â”‚ LayerNorm â†’ Multi-Head Attention â”‚  â”‚                       â”‚\n",
    "â”‚  â”‚  â”‚         (+ Residual)             â”‚  â”‚                       â”‚\n",
    "â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                       â”‚\n",
    "â”‚  â”‚  â”‚ LayerNorm â†’ Feed-Forward Network â”‚  â”‚                       â”‚\n",
    "â”‚  â”‚  â”‚         (+ Residual)             â”‚  â”‚                       â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚\n",
    "â”‚       â†“                                                          â”‚\n",
    "â”‚  Final LayerNorm                                                 â”‚\n",
    "â”‚       â†“                                                          â”‚\n",
    "â”‚  Language Model Head (n_embed â†’ vocab_size)                      â”‚\n",
    "â”‚       â†“                                                          â”‚\n",
    "â”‚  Output Logits (B, T, vocab_size)                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”‘ Key Concepts Mastered\n",
    "\n",
    "| Concept | What You Learned |\n",
    "|---------|-----------------|\n",
    "| **Tokenization** | Convert text â†” numbers |\n",
    "| **Embeddings** | Dense vector representations |\n",
    "| **Self-Attention** | Tokens communicate via Q-K-V |\n",
    "| **Multi-Head** | Multiple attention patterns in parallel |\n",
    "| **Feed-Forward** | Per-token non-linear processing |\n",
    "| **Layer Norm** | Stabilize activations |\n",
    "| **Residuals** | Enable deep networks |\n",
    "| **Dropout** | Prevent overfitting |\n",
    "| **Causal Masking** | No peeking at the future! |\n",
    "\n",
    "### ğŸš€ From Here to ChatGPT\n",
    "\n",
    "Your model vs. GPT-4:\n",
    "\n",
    "| Aspect | Your Model | GPT-4 |\n",
    "|--------|-----------|-------|\n",
    "| Parameters | ~10 million | ~1.8 trillion |\n",
    "| Context | 256 tokens | 128,000 tokens |\n",
    "| Training Data | 1MB Shakespeare | Trillions of tokens |\n",
    "| Training Time | 30 minutes | Months on thousands of GPUs |\n",
    "| Architecture | **IDENTICAL!** | **IDENTICAL!** |\n",
    "\n",
    "**The fundamental architecture is the SAME!** GPT-4 just has:\n",
    "- More layers, more heads, more dimensions\n",
    "- Better training data and techniques\n",
    "- RLHF (Reinforcement Learning from Human Feedback)\n",
    "- Instruction tuning\n",
    "\n",
    "### ğŸ“ What's Next?\n",
    "\n",
    "Now that you understand Transformers, explore:\n",
    "- **BERT**: Bidirectional (sees full context, not just past)\n",
    "- **Vision Transformers (ViT)**: Attention for images\n",
    "- **Multimodal Models**: Combining text, images, audio\n",
    "- **Fine-tuning**: Adapting pre-trained models to your tasks\n",
    "- **RLHF**: Making models helpful and safe\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ™ Thank You!\n",
    "\n",
    "You've completed the Day 7-8 journey into the heart of modern AI!\n",
    "\n",
    "**Remember:** Every ChatGPT, Claude, Gemini, and LLaMA is built on these exact same principles you just learned. You now understand the architecture that's transforming the world! ğŸŒ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
