{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24f447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4db04",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1f2418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a36140",
   "metadata": {},
   "source": [
    "Language Modelling Principal (Token Communication):\n",
    "\n",
    "Token in 5th Location for example should not be able to communicate with token in 6th, 7th or 8th token as they are future tokens. \n",
    "\n",
    "It should only be able to communicate with tokens before it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d131b90",
   "metadata": {},
   "source": [
    "We can do this using Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a1668e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "c=\n",
      "tensor([[ 5.,  7.],\n",
      "        [ 7.,  7.],\n",
      "        [12., 10.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.tril(torch.ones(3, 3)) # torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3,2), dtype = torch.float)\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e453dcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 1\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True) # normalize the rows\n",
    "out_1 = wei @ x # (T,T) @ (B,T,C) ---> (B,T,T) @ (B,T,C) ---> (B,T,C)\n",
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0921fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = torch.softmax(wei, dim=-1)\n",
    "out_2 = wei @ x\n",
    "out_2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
