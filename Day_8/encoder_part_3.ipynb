{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2586571",
   "metadata": {},
   "source": [
    "### **1. Recap: What We've Built So Far**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d9d29",
   "metadata": {},
   "source": [
    "**Our Encoder Layer Building Progress:**\n",
    "\n",
    "| Component | Status | Purpose |\n",
    "|-----------|--------|--------|\n",
    "| Multi-Head Attention | âœ… Day 7 | Relate words to each other |\n",
    "| Positional Encoding | âœ… Step 1 | Add position information |\n",
    "| Feed-Forward Network | âœ… Step 2 | Add non-linear transformations |\n",
    "| **Layer Normalization** | ğŸ”§ **Today** | **Stabilize training** |\n",
    "| **Residual Connections** | ğŸ”§ **Today** | **Enable gradient flow** |\n",
    "| **Complete Encoder Layer** | ğŸ”§ **Today** | **Assemble everything** |\n",
    "\n",
    "**Today's Goal:** Combine all the pieces to build the **Complete Encoder Layer** â€“ the fundamental building block of the Transformer Encoder!\n",
    "\n",
    "Let's assemble everything! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f63659",
   "metadata": {},
   "source": [
    "### **2. The Encoder Layer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62300ad7",
   "metadata": {},
   "source": [
    "Before we dive into the new components, let's see the complete picture:\n",
    "\n",
    "```\n",
    "                    Input (batch, seq_len, d_model)\n",
    "                              |\n",
    "                              â–¼\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚   Multi-Head Attention  â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              |\n",
    "                              â–¼\n",
    "                    Dropout + Residual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                              |                   |\n",
    "                              â–¼                   |\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       |\n",
    "                 â”‚    Layer Normalization  â”‚â—„â”€â”€â”€â”€â”€â”˜\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              |\n",
    "                              â–¼\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚   Feed-Forward Network  â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              |\n",
    "                              â–¼\n",
    "                    Dropout + Residual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                              |                   |\n",
    "                              â–¼                   |\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       |\n",
    "                 â”‚    Layer Normalization  â”‚â—„â”€â”€â”€â”€â”€â”˜\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              |\n",
    "                              â–¼\n",
    "                   Output (batch, seq_len, d_model)\n",
    "```\n",
    "\n",
    "**Key Formula (Post-Norm, Original Transformer):**\n",
    "\n",
    "$$\\text{sublayer\\_output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "Where `Sublayer` is either Multi-Head Attention or Feed-Forward Network.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"300\"/>\n",
    "  <p><i>The Encoder Layer: Attention + FFN with Residuals and Layer Norm</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55980b7e",
   "metadata": {},
   "source": [
    "### **3. Residual Connections (Skip Connections)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa83959",
   "metadata": {},
   "source": [
    "#### **The Problem: Vanishing Gradients in Deep Networks**\n",
    "\n",
    "As neural networks get deeper, they face a critical problem: **vanishing gradients**.\n",
    "\n",
    "During backpropagation, gradients are multiplied through each layer:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_1} = \\frac{\\partial L}{\\partial x_n} \\cdot \\frac{\\partial x_n}{\\partial x_{n-1}} \\cdot \\ldots \\cdot \\frac{\\partial x_2}{\\partial x_1}$$\n",
    "\n",
    "If each gradient term is less than 1, the product becomes **exponentially small**:\n",
    "\n",
    "$$0.9^{10} = 0.35 \\quad \\text{(10 layers)}$$\n",
    "$$0.9^{50} = 0.005 \\quad \\text{(50 layers)}$$\n",
    "$$0.9^{100} \\approx 0.00003 \\quad \\text{(100 layers)}$$\n",
    "\n",
    "**Result:** Early layers barely learn because their gradients are too small!\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mMQ3k3L_IftXtIUJBpE6rA.png\" width=\"500\"/>\n",
    "  <p><i>Vanishing gradients make deep networks hard to train</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6b6c0",
   "metadata": {},
   "source": [
    "#### **The Solution: Residual Connections**\n",
    "\n",
    "Residual connections (introduced in ResNet) provide a **shortcut** for gradients:\n",
    "\n",
    "$$\\text{output} = x + F(x)$$\n",
    "\n",
    "Where:\n",
    "- $x$ = input (passed through unchanged)\n",
    "- $F(x)$ = transformation applied by the sublayer\n",
    "- $+$ = element-wise addition\n",
    "\n",
    "**Why This Helps:**\n",
    "\n",
    "1. **Direct Gradient Path**: Gradients can flow directly through the addition\n",
    "2. **Identity Shortcut**: Even if $F(x) \\approx 0$, output â‰ˆ input (no information lost)\n",
    "3. **Easier Optimization**: Network only needs to learn the \"residual\" $F(x)$\n",
    "\n",
    "```\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                         â”‚\n",
    "        â”‚    Skip Connection      â”‚\n",
    "        â”‚         (x)             â”‚\n",
    "        â”‚                         â”‚\n",
    "   x â”€â”€â”€â”¼â”€â”€â”€â–º [Sublayer F] â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–º x + F(x)\n",
    "        â”‚         â†“               â”‚\n",
    "        â”‚        F(x)             â”‚\n",
    "        â”‚                         â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Intuition:** Instead of learning the full transformation, the network learns what to **add** to the input. This is often easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Demonstrate the gradient flow benefit of residual connections\n",
    "print(\"=\" * 60)\n",
    "print(\"GRADIENT FLOW: With vs Without Residual Connections\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate gradient flow through layers\n",
    "num_layers = 20\n",
    "gradient_factor = 0.8  # Each layer reduces gradient by this factor\n",
    "\n",
    "# Without residual connections\n",
    "gradient_no_residual = 1.0\n",
    "gradients_no_res = [gradient_no_residual]\n",
    "for _ in range(num_layers):\n",
    "    gradient_no_residual *= gradient_factor\n",
    "    gradients_no_res.append(gradient_no_residual)\n",
    "\n",
    "# With residual connections (gradient can flow through skip connection)\n",
    "# Simplified model: gradient = gradient_through_skip + gradient_through_layer\n",
    "gradient_residual = 1.0\n",
    "gradients_res = [gradient_residual]\n",
    "for _ in range(num_layers):\n",
    "    # Part through skip (preserved) + part through layer (reduced)\n",
    "    gradient_residual = 0.5 * gradient_residual + 0.5 * (gradient_residual * gradient_factor)\n",
    "    gradients_res.append(gradient_residual)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gradients_no_res, 'r-o', label='Without Residual', markersize=4)\n",
    "plt.plot(gradients_res, 'g-o', label='With Residual', markersize=4)\n",
    "plt.xlabel('Layer (from output to input)', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Gradient Flow Through Deep Networks', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAfter {num_layers} layers:\")\n",
    "print(f\"  Without residual: gradient = {gradients_no_res[-1]:.6f}\")\n",
    "print(f\"  With residual:    gradient = {gradients_res[-1]:.6f}\")\n",
    "print(f\"\\nâœ… Residual connections preserve {gradients_res[-1]/gradients_no_res[-1]:.1f}x more gradient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cbcf7",
   "metadata": {},
   "source": [
    "### **4. Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892f83d",
   "metadata": {},
   "source": [
    "#### **Why Normalize?**\n",
    "\n",
    "Neural network activations can have wildly different scales across layers and training steps. This causes:\n",
    "\n",
    "1. **Unstable Training**: Large activations â†’ large gradients â†’ exploding updates\n",
    "2. **Slow Convergence**: Network spends time learning to rescale values\n",
    "3. **Sensitivity to Initialization**: Bad initial weights cause training to fail\n",
    "\n",
    "**Normalization** forces activations to have consistent statistics (mean â‰ˆ 0, std â‰ˆ 1), which:\n",
    "- Stabilizes training\n",
    "- Allows higher learning rates\n",
    "- Reduces sensitivity to initialization\n",
    "\n",
    "#### **Batch Norm vs Layer Norm**\n",
    "\n",
    "| Aspect | Batch Normalization | Layer Normalization |\n",
    "|--------|---------------------|---------------------|\n",
    "| Normalizes across | Batch dimension | Feature dimension |\n",
    "| Depends on | Other samples in batch | Only current sample |\n",
    "| Works with small batches? | âŒ No (noisy statistics) | âœ… Yes |\n",
    "| Works with variable seq lengths? | âŒ Problematic | âœ… Yes |\n",
    "| Used in | CNNs (ResNet, etc.) | Transformers, RNNs |\n",
    "\n",
    "**For Transformers, Layer Normalization is the clear choice!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574e44f",
   "metadata": {},
   "source": [
    "#### **Layer Normalization Formula**\n",
    "\n",
    "For an input vector $x$ of dimension $d$:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$ (mean across features)\n",
    "- $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$ (variance across features)\n",
    "- $\\gamma$ = learnable scale parameter (initialized to 1)\n",
    "- $\\beta$ = learnable shift parameter (initialized to 0)\n",
    "- $\\epsilon$ = small constant for numerical stability (e.g., 1e-6)\n",
    "\n",
    "**Visual Comparison:**\n",
    "\n",
    "```\n",
    "Batch Normalization:          Layer Normalization:\n",
    "                              \n",
    "  Batch                         Batch\n",
    "    â†“                             â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ â–  â–  â–  â–  â”‚ â† Normalize       â”‚ â–  â–¡ â–¡ â–¡ â”‚\n",
    "â”‚ â–  â–  â–  â–  â”‚   across batch    â”‚ â–  â–¡ â–¡ â–¡ â”‚ â† Normalize\n",
    "â”‚ â–  â–  â–  â–  â”‚   (vertically)    â”‚ â–  â–¡ â–¡ â–¡ â”‚   across features\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   (horizontally)\n",
    "  Features                      Features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the difference between Batch Norm and Layer Norm\n",
    "\n",
    "# Create sample data: (batch_size=4, seq_len=3, d_model=5)\n",
    "batch_size, seq_len, d_model = 4, 3, 5\n",
    "x = torch.randn(batch_size, seq_len, d_model) * 3 + 2  # Mean â‰ˆ 2, Std â‰ˆ 3\n",
    "\n",
    "print(\"Original Data Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {x.shape} (batch={batch_size}, seq={seq_len}, features={d_model})\")\n",
    "print(f\"Mean: {x.mean().item():.3f}\")\n",
    "print(f\"Std:  {x.std().item():.3f}\")\n",
    "print()\n",
    "\n",
    "# Layer Normalization (normalize across last dimension = features)\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "x_ln = layer_norm(x)\n",
    "\n",
    "print(\"After Layer Normalization\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {x_ln.shape}\")\n",
    "\n",
    "# Check statistics for each position in each batch\n",
    "print(\"\\nPer-position statistics (should be meanâ‰ˆ0, stdâ‰ˆ1):\")\n",
    "for b in range(min(2, batch_size)):  # Show first 2 batches\n",
    "    for s in range(seq_len):\n",
    "        mean = x_ln[b, s, :].mean().item()\n",
    "        std = x_ln[b, s, :].std(unbiased=False).item()\n",
    "        print(f\"  Batch {b}, Position {s}: mean={mean:+.4f}, std={std:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Each position is normalized independently!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Layer Normalization effect\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Sample one position from the batch\n",
    "sample_before = x[0, 0, :].detach().numpy()\n",
    "sample_after = x_ln[0, 0, :].detach().numpy()\n",
    "\n",
    "# Plot 1: Before normalization\n",
    "axes[0].bar(range(d_model), sample_before, color='coral', edgecolor='black')\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0].axhline(y=sample_before.mean(), color='blue', linestyle='--', label=f'Mean: {sample_before.mean():.2f}')\n",
    "axes[0].set_xlabel('Feature Dimension')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].set_title('Before Layer Normalization')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(-5, 8)\n",
    "\n",
    "# Plot 2: After normalization\n",
    "axes[1].bar(range(d_model), sample_after, color='lightgreen', edgecolor='black')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].axhline(y=sample_after.mean(), color='blue', linestyle='--', label=f'Mean: {sample_after.mean():.2f}')\n",
    "axes[1].set_xlabel('Feature Dimension')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('After Layer Normalization')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(-5, 8)\n",
    "\n",
    "# Plot 3: Distribution comparison\n",
    "axes[2].hist(x.flatten().detach().numpy(), bins=30, alpha=0.5, label='Before', color='coral')\n",
    "axes[2].hist(x_ln.flatten().detach().numpy(), bins=30, alpha=0.5, label='After', color='lightgreen')\n",
    "axes[2].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[2].set_xlabel('Value')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Distribution of All Values')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Layer Normalization: Centering and Scaling', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"â€¢ Values are centered around 0 (mean â‰ˆ 0)\")\n",
    "print(\"â€¢ Values are scaled to unit variance (std â‰ˆ 1)\")\n",
    "print(\"â€¢ This makes training much more stable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4d172",
   "metadata": {},
   "source": [
    "### **5. Pre-Norm vs Post-Norm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2c748",
   "metadata": {},
   "source": [
    "There are two common ways to apply Layer Normalization in the Encoder:\n",
    "\n",
    "#### **Post-Norm (Original Transformer)**\n",
    "\n",
    "$$\\text{output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "```\n",
    "x â”€â”€â–º Sublayer â”€â”€â–º + â”€â”€â–º LayerNorm â”€â”€â–º output\n",
    "       â”‚          â–²\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (residual)\n",
    "```\n",
    "\n",
    "#### **Pre-Norm (GPT-2, Modern Transformers)**\n",
    "\n",
    "$$\\text{output} = x + \\text{Sublayer}(\\text{LayerNorm}(x))$$\n",
    "\n",
    "```\n",
    "x â”€â”€â–º LayerNorm â”€â”€â–º Sublayer â”€â”€â–º + â”€â”€â–º output\n",
    "â”‚                               â–²\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (residual)\n",
    "```\n",
    "\n",
    "#### **Comparison**\n",
    "\n",
    "| Aspect | Post-Norm | Pre-Norm |\n",
    "|--------|-----------|----------|\n",
    "| Used in | Original Transformer, BERT | GPT-2, GPT-3, LLaMA |\n",
    "| Training Stability | Can be unstable | More stable |\n",
    "| Gradient Flow | Gradients pass through LayerNorm | Direct gradient path |\n",
    "| Learning Rate | Needs warmup | More robust |\n",
    "| Final LayerNorm | Not needed | Often added at the end |\n",
    "\n",
    "**For this tutorial, we'll use Post-Norm** (original Transformer) but mention Pre-Norm for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14907fac",
   "metadata": {},
   "source": [
    "### **6. Building the Components**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b5f51",
   "metadata": {},
   "source": [
    "Before we assemble the complete Encoder Layer, let's bring in the components we built in previous steps:\n",
    "\n",
    "1. **Multi-Head Attention** (from Day 7)\n",
    "2. **Positional Encoding** (from Step 1)  \n",
    "3. **Position-wise Feed-Forward Network** (from Step 2)\n",
    "\n",
    "Let's implement them here so we have everything in one place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 1: Multi-Head Attention (from Day 7)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "    \n",
    "    Splits the input into multiple heads, applies scaled dot-product attention\n",
    "    to each head, and concatenates the results.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension (e.g., 512)\n",
    "        num_heads: Number of attention heads (e.g., 8)\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention.\n",
    "        \n",
    "        Args:\n",
    "            Q: Queries (batch, num_heads, seq_len, d_k)\n",
    "            K: Keys (batch, num_heads, seq_len, d_k)\n",
    "            V: Values (batch, num_heads, seq_len, d_k)\n",
    "            mask: Optional mask (batch, 1, 1, seq_len) or (batch, 1, seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch, num_heads, seq_len, d_k)\n",
    "            attention_weights: Attention weights (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of Multi-Head Attention.\n",
    "        \n",
    "        Args:\n",
    "            query: Query tensor (batch, seq_len, d_model)\n",
    "            key: Key tensor (batch, seq_len, d_model)\n",
    "            value: Value tensor (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch, seq_len, d_model)\n",
    "            attention_weights: Attention weights (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Reshape for multi-head: (batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads: (batch, num_heads, seq_len, d_k) -> (batch, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"âœ… MultiHeadAttention class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef08a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 2: Positional Encoding (from Step 1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding using sine and cosine functions.\n",
    "    \n",
    "    Adds positional information to input embeddings so the model\n",
    "    can distinguish between different positions in the sequence.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Embedding dimension (must match input embedding size)\n",
    "        max_seq_len: Maximum sequence length to support\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        # Position indices: [0, 1, 2, ..., max_seq_len-1]\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Division term: 10000^(2i/d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sine to even indices, cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension: (1, max_seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of same shape with positional encoding added\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"âœ… PositionalEncoding class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e91cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 3: Position-wise Feed-Forward Network (from Step 2)\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    A simple two-layer MLP applied independently to each position.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (d_model) â†’ Linear â†’ Activation â†’ Dropout â†’ Linear â†’ Output (d_model)\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension (e.g., 512)\n",
    "        d_ff: Feed-forward hidden dimension (typically 4 * d_model = 2048)\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "        activation: Activation function ('relu' or 'gelu')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, activation='relu'):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the FFN.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "print(\"âœ… PositionWiseFeedForward class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3178029",
   "metadata": {},
   "source": [
    "### **7. The Complete Encoder Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91612e9",
   "metadata": {},
   "source": [
    "Now we have all the pieces! Let's assemble them into the complete **Encoder Layer**.\n",
    "\n",
    "**The Encoder Layer consists of:**\n",
    "\n",
    "1. **Multi-Head Self-Attention** sublayer\n",
    "2. **Residual Connection + Layer Normalization** (after attention)\n",
    "3. **Position-wise Feed-Forward Network** sublayer\n",
    "4. **Residual Connection + Layer Normalization** (after FFN)\n",
    "\n",
    "**Architecture (Post-Norm):**\n",
    "\n",
    "```\n",
    "Input x\n",
    "    â”‚\n",
    "    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     Multi-Head Attention        â”‚\n",
    "â”‚     (Self-Attention: Q=K=V=x)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â–¼\n",
    "  Dropout\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼              â”‚ (residual)\n",
    "   Add â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â–¼\n",
    "  LayerNorm\n",
    "    â”‚\n",
    "    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Position-wise Feed-Forward    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â–¼\n",
    "  Dropout\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼              â”‚ (residual)\n",
    "   Add â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â–¼\n",
    "  LayerNorm\n",
    "    â”‚\n",
    "    â–¼\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Encoder Layer of the Transformer.\n",
    "    \n",
    "    Consists of:\n",
    "    1. Multi-Head Self-Attention with residual connection and layer norm\n",
    "    2. Position-wise Feed-Forward Network with residual connection and layer norm\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension (e.g., 512)\n",
    "        num_heads: Number of attention heads (e.g., 8)\n",
    "        d_ff: Feed-forward hidden dimension (e.g., 2048)\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "        activation: Activation function for FFN ('relu' or 'gelu')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, activation='relu'):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout, activation)\n",
    "        \n",
    "        # Layer Normalization (one for each sublayer)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout (for residual connections)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Encoder Layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Encoded output of shape (batch_size, seq_len, d_model)\n",
    "            attention_weights: Self-attention weights (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # ============================================\n",
    "        # Sublayer 1: Multi-Head Self-Attention\n",
    "        # ============================================\n",
    "        \n",
    "        # Self-attention (Q = K = V = x)\n",
    "        attn_output, attention_weights = self.self_attention(x, x, x, mask)\n",
    "        \n",
    "        # Dropout\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        \n",
    "        # Residual connection + Layer Normalization (Post-Norm)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # ============================================\n",
    "        # Sublayer 2: Position-wise Feed-Forward\n",
    "        # ============================================\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # Dropout\n",
    "        ff_output = self.dropout2(ff_output)\n",
    "        \n",
    "        # Residual connection + Layer Normalization (Post-Norm)\n",
    "        output = self.norm2(x + ff_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"âœ… EncoderLayer class defined!\")\n",
    "print()\n",
    "print(\"Architecture summary:\")\n",
    "print(\"  Input â†’ Self-Attention â†’ Add & Norm â†’ FFN â†’ Add & Norm â†’ Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd0f35",
   "metadata": {},
   "source": [
    "### **8. Testing the Encoder Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Encoder Layer with the original Transformer configuration\n",
    "\n",
    "# Configuration (from \"Attention Is All You Need\" paper)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "# Create the encoder layer\n",
    "encoder_layer = EncoderLayer(\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "print(\"Encoder Layer Configuration:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  num_heads: {num_heads}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"  dropout: {dropout}\")\n",
    "print()\n",
    "\n",
    "# Create sample input\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"  â†’ (batch_size={batch_size}, seq_len={seq_len}, d_model={d_model})\")\n",
    "\n",
    "# Forward pass\n",
    "encoder_layer.eval()\n",
    "with torch.no_grad():\n",
    "    output, attention_weights = encoder_layer(x)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"  â†’ (batch_size={batch_size}, seq_len={seq_len}, d_model={d_model})\")\n",
    "print(f\"\\nAttention weights shape: {attention_weights.shape}\")\n",
    "print(f\"  â†’ (batch_size={batch_size}, num_heads={num_heads}, seq_len={seq_len}, seq_len={seq_len})\")\n",
    "print()\n",
    "print(\"âœ… Encoder Layer works correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2091591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention weights\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for head in range(num_heads):\n",
    "    ax = axes[head // 4, head % 4]\n",
    "    \n",
    "    # Get attention weights for first batch, this head\n",
    "    attn = attention_weights[0, head].detach().numpy()\n",
    "    \n",
    "    im = ax.imshow(attn, cmap='Blues', vmin=0, vmax=0.5)\n",
    "    ax.set_title(f'Head {head + 1}', fontsize=11)\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle('Self-Attention Weights Across All 8 Heads\\n(Encoder Layer)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"â€¢ Each head learns different attention patterns\")\n",
    "print(\"â€¢ Some heads may focus on nearby positions (local)\")\n",
    "print(\"â€¢ Some heads may attend to specific patterns (global)\")\n",
    "print(\"â€¢ Together, they capture diverse relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9c238",
   "metadata": {},
   "source": [
    "### **9. Parameter Count Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6607055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the parameter distribution in the Encoder Layer\n",
    "\n",
    "def count_parameters(module):\n",
    "    \"\"\"Count trainable parameters in a module.\"\"\"\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "def analyze_encoder_layer(encoder_layer):\n",
    "    \"\"\"Detailed breakdown of encoder layer parameters.\"\"\"\n",
    "    \n",
    "    # Count parameters for each component\n",
    "    attn_params = count_parameters(encoder_layer.self_attention)\n",
    "    ffn_params = count_parameters(encoder_layer.feed_forward)\n",
    "    norm1_params = count_parameters(encoder_layer.norm1)\n",
    "    norm2_params = count_parameters(encoder_layer.norm2)\n",
    "    total_params = count_parameters(encoder_layer)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENCODER LAYER PARAMETER BREAKDOWN\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(f\"{'Component':<30} {'Parameters':>12} {'Percentage':>12}\")\n",
    "    print(\"-\" * 54)\n",
    "    print(f\"{'Multi-Head Attention':<30} {attn_params:>12,} {100*attn_params/total_params:>11.1f}%\")\n",
    "    print(f\"{'Feed-Forward Network':<30} {ffn_params:>12,} {100*ffn_params/total_params:>11.1f}%\")\n",
    "    print(f\"{'Layer Norm 1':<30} {norm1_params:>12,} {100*norm1_params/total_params:>11.1f}%\")\n",
    "    print(f\"{'Layer Norm 2':<30} {norm2_params:>12,} {100*norm2_params/total_params:>11.1f}%\")\n",
    "    print(\"-\" * 54)\n",
    "    print(f\"{'TOTAL':<30} {total_params:>12,} {'100.0%':>12}\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'attention': attn_params,\n",
    "        'ffn': ffn_params,\n",
    "        'norm': norm1_params + norm2_params,\n",
    "        'total': total_params\n",
    "    }\n",
    "\n",
    "params = analyze_encoder_layer(encoder_layer)\n",
    "\n",
    "# Visualize as pie chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "labels = ['Multi-Head Attention', 'Feed-Forward Network', 'Layer Normalization']\n",
    "sizes = [params['attention'], params['ffn'], params['norm']]\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "explode = (0, 0.05, 0)\n",
    "\n",
    "ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "       shadow=True, startangle=90)\n",
    "ax.set_title('Encoder Layer Parameter Distribution', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Key Insight:\")\n",
    "print(f\"   The FFN contains {100*params['ffn']/params['total']:.1f}% of the parameters!\")\n",
    "print(\"   This is because d_ff = 4 Ã— d_model makes it very large.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d9b19",
   "metadata": {},
   "source": [
    "### **10. Testing with Masking (Padding Mask)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9298ed",
   "metadata": {},
   "source": [
    "In real applications, sequences often have different lengths and are padded to the same length. We need to **mask** the padding tokens so the model ignores them.\n",
    "\n",
    "**Padding Mask:**\n",
    "- 1 = real token (attend to it)\n",
    "- 0 = padding token (ignore it)\n",
    "\n",
    "```\n",
    "Sequence: [\"Hello\", \"world\", \"<PAD>\", \"<PAD>\"]\n",
    "Mask:     [   1,       1,        0,        0  ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b819c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate padding masks\n",
    "\n",
    "def create_padding_mask(seq_len, pad_positions):\n",
    "    \"\"\"\n",
    "    Create a padding mask for attention.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Total sequence length\n",
    "        pad_positions: List of positions that are padding (to be masked)\n",
    "    \n",
    "    Returns:\n",
    "        mask: Tensor of shape (1, 1, 1, seq_len) for broadcasting\n",
    "    \"\"\"\n",
    "    mask = torch.ones(1, 1, 1, seq_len)\n",
    "    for pos in pad_positions:\n",
    "        mask[0, 0, 0, pos] = 0\n",
    "    return mask\n",
    "\n",
    "# Example: Sequence of length 10, with last 3 positions as padding\n",
    "seq_len = 10\n",
    "pad_positions = [7, 8, 9]  # Last 3 are padding\n",
    "\n",
    "mask = create_padding_mask(seq_len, pad_positions)\n",
    "print(\"Padding Mask:\")\n",
    "print(f\"  Shape: {mask.shape}\")\n",
    "print(f\"  Values: {mask.squeeze().numpy()}\")\n",
    "print(f\"  (1 = attend, 0 = ignore)\")\n",
    "print()\n",
    "\n",
    "# Test encoder layer with mask\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "encoder_layer.eval()\n",
    "with torch.no_grad():\n",
    "    output_masked, attn_weights_masked = encoder_layer(x, mask=mask)\n",
    "\n",
    "print(\"Encoder output with masking:\")\n",
    "print(f\"  Output shape: {output_masked.shape}\")\n",
    "\n",
    "# Visualize attention with masking\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Without mask (for comparison, re-run)\n",
    "with torch.no_grad():\n",
    "    _, attn_no_mask = encoder_layer(x, mask=None)\n",
    "\n",
    "# Head 0 without mask\n",
    "im1 = axes[0].imshow(attn_no_mask[0, 0].detach().numpy(), cmap='Blues', vmin=0, vmax=0.3)\n",
    "axes[0].set_title('Head 1: Without Mask', fontsize=12)\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Head 0 with mask\n",
    "im2 = axes[1].imshow(attn_weights_masked[0, 0].detach().numpy(), cmap='Blues', vmin=0, vmax=0.3)\n",
    "axes[1].set_title('Head 1: With Padding Mask\\n(Positions 7-9 masked)', fontsize=12)\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "axes[1].axvline(x=6.5, color='red', linestyle='--', linewidth=2, label='Mask boundary')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.suptitle('Effect of Padding Mask on Attention', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Notice how attention weights are ~0 for masked positions (7-9)!\")\n",
    "print(\"   The model correctly ignores padding tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94429e",
   "metadata": {},
   "source": [
    "### **11. Verifying Residual Connections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc534236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify that residual connections help preserve information\n",
    "\n",
    "# Create a simple test: if the sublayers output zeros, \n",
    "# the residual connection should preserve the input (after normalization)\n",
    "\n",
    "class ZeroAttention(nn.Module):\n",
    "    \"\"\"A mock attention that outputs zeros (for testing residual connections).\"\"\"\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        return torch.zeros_like(q), None\n",
    "\n",
    "class ZeroFFN(nn.Module):\n",
    "    \"\"\"A mock FFN that outputs zeros (for testing residual connections).\"\"\"\n",
    "    def forward(self, x):\n",
    "        return torch.zeros_like(x)\n",
    "\n",
    "# Create encoder layer with zero sublayers\n",
    "encoder_zero = EncoderLayer(d_model=64, num_heads=4, d_ff=256, dropout=0.0)\n",
    "encoder_zero.self_attention = ZeroAttention()\n",
    "encoder_zero.feed_forward = ZeroFFN()\n",
    "encoder_zero.eval()\n",
    "\n",
    "# Test input\n",
    "x_test = torch.randn(1, 5, 64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_zero, _ = encoder_zero(x_test)\n",
    "\n",
    "# Compare input and output\n",
    "# Due to layer normalization, values will be normalized but correlation should be high\n",
    "correlation = torch.corrcoef(torch.stack([x_test.flatten(), output_zero.flatten()]))[0, 1]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESIDUAL CONNECTION VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Test: When sublayers output zeros, residual preserves input\")\n",
    "print()\n",
    "print(f\"Input mean:  {x_test.mean().item():.4f}\")\n",
    "print(f\"Output mean: {output_zero.mean().item():.4f} (normalized to ~0)\")\n",
    "print()\n",
    "print(f\"Input std:   {x_test.std().item():.4f}\")\n",
    "print(f\"Output std:  {output_zero.std().item():.4f} (normalized to ~1)\")\n",
    "print()\n",
    "print(f\"Correlation: {correlation.item():.4f}\")\n",
    "print()\n",
    "print(\"âœ… High correlation shows residual connections preserve information!\")\n",
    "print(\"   Even with zero sublayer outputs, the input information flows through.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b673e61",
   "metadata": {},
   "source": [
    "### **12. Summary: The Complete Encoder Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802dabf",
   "metadata": {},
   "source": [
    "Excellent work! You've built the complete Encoder Layer! ğŸ‰\n",
    "\n",
    "**What We Built Today:**\n",
    "\n",
    "âœ… **Residual Connections**: Skip connections that enable gradient flow\n",
    "- Formula: $\\text{output} = x + \\text{Sublayer}(x)$\n",
    "- Prevents vanishing gradients in deep networks\n",
    "\n",
    "âœ… **Layer Normalization**: Stabilizes training by normalizing activations\n",
    "- Normalizes across the feature dimension (not batch)\n",
    "- Adds learnable scale ($\\gamma$) and shift ($\\beta$)\n",
    "\n",
    "âœ… **Complete Encoder Layer**: Assembled all components together\n",
    "```\n",
    "Input â†’ Self-Attention â†’ Add & Norm â†’ FFN â†’ Add & Norm â†’ Output\n",
    "```\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "| Component | Role |\n",
    "|-----------|------|\n",
    "| Multi-Head Attention | Capture relationships between positions |\n",
    "| Residual Connections | Enable gradient flow, preserve information |\n",
    "| Layer Normalization | Stabilize training, normalize activations |\n",
    "| Feed-Forward Network | Add non-linear transformation power |\n",
    "\n",
    "---\n",
    "\n",
    "**Our Progress:**\n",
    "\n",
    "| Component | Status |\n",
    "|-----------|--------|\n",
    "| Multi-Head Attention | âœ… Day 7 |\n",
    "| Positional Encoding | âœ… Step 1 |\n",
    "| Feed-Forward Network | âœ… Step 2 |\n",
    "| **Complete Encoder Layer** | âœ… **Step 3 (Today!)** |\n",
    "| Encoder Stack | â³ Step 4 (Next!) |\n",
    "\n",
    "**Next Up:** In Step 4, we'll stack multiple Encoder Layers to build the complete **Encoder Stack**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64cb8c",
   "metadata": {},
   "source": [
    "### **13. Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5defe9c1",
   "metadata": {},
   "source": [
    "Try these exercises to deepen your understanding!\n",
    "\n",
    "**Exercise 1:** Implement a **Pre-Norm** version of the Encoder Layer (normalize before sublayer, not after).\n",
    "\n",
    "**Exercise 2:** Add a parameter to toggle between Pre-Norm and Post-Norm variants.\n",
    "\n",
    "**Exercise 3:** What happens if you remove the residual connections? Run an experiment and compare gradient magnitudes.\n",
    "\n",
    "**Exercise 4:** Try using GELU activation instead of ReLU. Does it change the output distribution?\n",
    "\n",
    "**Exercise 5:** Implement dropout on the attention weights (inside the attention mechanism). How does this affect the attention patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdc023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your exercises!\n",
    "\n",
    "# Exercise 1: Pre-Norm Encoder Layer\n",
    "# Hint: Apply LayerNorm BEFORE the sublayer, not after\n",
    "# x = x + Sublayer(LayerNorm(x))\n",
    "\n",
    "class PreNormEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-Norm variant of the Encoder Layer.\n",
    "    Used in GPT-2, GPT-3, LLaMA, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # TODO: Implement Pre-Norm version\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # TODO: Implement forward pass\n",
    "        # Remember: LayerNorm comes BEFORE the sublayer!\n",
    "        pass\n",
    "\n",
    "# Your code here!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
