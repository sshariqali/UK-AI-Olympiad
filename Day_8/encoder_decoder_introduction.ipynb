{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271600b7",
   "metadata": {},
   "source": [
    "# **Day 8 (Part 1): Understanding the Encoder-Decoder Architecture**\n",
    "\n",
    "## Building the Engine: The Heart of the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbbdd5",
   "metadata": {},
   "source": [
    "### **1. Recap: Where We Left Off**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b71cd3c",
   "metadata": {},
   "source": [
    "On **Day 7**, we built the **core attention mechanism** â€“ the fundamental building block of Transformers:\n",
    "\n",
    "| What We Built | What It Does |\n",
    "|--------------|-------------|\n",
    "| **Scaled Dot-Product Attention** | Computes weighted sums based on relevance |\n",
    "| **Multi-Head Attention** | Multiple attention heads for diverse patterns |\n",
    "| **Padding Mask** | Ignores padding tokens |\n",
    "| **Causal Mask** | Prevents seeing future tokens |\n",
    "\n",
    "**Today's Mission:** Take these building blocks and assemble them into complete **Encoder** and **Decoder** layers â€“ the two engines that power the Transformer!\n",
    "\n",
    "Let's dive in! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0f0d9",
   "metadata": {},
   "source": [
    "### **2. The Big Picture: What Are Encoders and Decoders?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181670e",
   "metadata": {},
   "source": [
    "Before we dive into the code, let's understand the **purpose** and **roles** of Encoders and Decoders in the Transformer architecture.\n",
    "\n",
    "#### **The Translation Analogy ğŸŒ**\n",
    "\n",
    "Imagine you're a professional translator converting English to French:\n",
    "\n",
    "```\n",
    "English: \"The cat sat on the mat\"\n",
    "         â†“\n",
    "    [Understanding Phase - ENCODER]\n",
    "         â†“\n",
    "    Deep understanding of the meaning\n",
    "         â†“\n",
    "    [Generation Phase - DECODER]\n",
    "         â†“\n",
    "French: \"Le chat s'est assis sur le tapis\"\n",
    "```\n",
    "\n",
    "**Two Distinct Phases:**\n",
    "\n",
    "1. **Encoder (Understanding)**: Read and deeply understand the input\n",
    "   - \"What is the subject? The cat.\"\n",
    "   - \"What action? Sitting.\"\n",
    "   - \"Where? On the mat.\"\n",
    "   - Result: Rich representation of meaning\n",
    "\n",
    "2. **Decoder (Generation)**: Generate the output word by word\n",
    "   - Look at the understanding (from Encoder)\n",
    "   - Look at what you've already written\n",
    "   - Decide what word comes next\n",
    "\n",
    "**Key Insight:** The Encoder and Decoder work **together** but have **different jobs**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426df797",
   "metadata": {},
   "source": [
    "#### **Visual Representation**\n",
    "\n",
    "Here's how data flows through the complete Transformer:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" width=\"700\"/>\n",
    "  <p><i>The Encoder processes the input, the Decoder generates the output</i></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "| Component | Input | Output | Purpose |\n",
    "|-----------|-------|--------|--------|\n",
    "| **Encoder** | Source sequence (e.g., English) | Rich contextual representations | Understand the input deeply |\n",
    "| **Decoder** | Target sequence (e.g., French) + Encoder output | Next token predictions | Generate output sequence |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa277955",
   "metadata": {},
   "source": [
    "### **3. Why Do We Need Both? The Sequence-to-Sequence Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c01083",
   "metadata": {},
   "source": [
    "#### **The Challenge: Variable Length Input â†’ Variable Length Output**\n",
    "\n",
    "Many real-world tasks involve converting one sequence into another, where the lengths may differ:\n",
    "\n",
    "| Task | Input | Output |\n",
    "|------|-------|--------|\n",
    "| **Machine Translation** | \"How are you?\" (3 words) | \"Comment allez-vous?\" (3 words, but could differ) |\n",
    "| **Summarization** | 500-word article | 50-word summary |\n",
    "| **Question Answering** | \"What is the capital of France?\" | \"Paris\" |\n",
    "| **Code Generation** | \"Write a function to sort a list\" | 10 lines of Python code |\n",
    "| **Speech Recognition** | 5 seconds of audio | \"Hello, world\" |\n",
    "\n",
    "**The Problem:** We can't just use a single neural network layer because:\n",
    "- Input and output have **different lengths**\n",
    "- We need to **first understand** the full input before generating output\n",
    "- We need to **generate output sequentially** (word by word)\n",
    "\n",
    "**The Solution:** Encoder-Decoder architecture!\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/0*VrRTrruwf2BtW4t5.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a28f844",
   "metadata": {},
   "source": [
    "### **4. The Encoder: Understanding the Input**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412201c",
   "metadata": {},
   "source": [
    "#### **4.1 What Does the Encoder Do?**\n",
    "\n",
    "The Encoder's job is to **deeply understand the input sequence** and create rich representations that capture:\n",
    "\n",
    "- ğŸ“ **Word meanings** (semantic information)\n",
    "- ğŸ”— **Relationships** between words (syntactic information)\n",
    "- ğŸ¯ **Context** for each word (disambiguation)\n",
    "\n",
    "**Example: Understanding \"bank\"**\n",
    "\n",
    "The word \"bank\" can mean:\n",
    "- ğŸ¦ Financial institution: \"I deposited money in the bank\"\n",
    "- ğŸŒŠ River side: \"The boat floated near the bank\"\n",
    "\n",
    "The Encoder uses **self-attention** to look at surrounding words and disambiguate:\n",
    "\n",
    "```\n",
    "\"I deposited money in the bank\"\n",
    "                        â†‘\n",
    "                   \"bank\" attends to \"deposited\" and \"money\"\n",
    "                        â†’ Meaning: Financial institution! ğŸ¦\n",
    "\n",
    "\"The boat floated near the bank\"\n",
    "                          â†‘\n",
    "                     \"bank\" attends to \"boat\" and \"floated\"\n",
    "                          â†’ Meaning: River side! ğŸŒŠ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2425e5",
   "metadata": {},
   "source": [
    "#### **4.2 The Encoder Layer Architecture**\n",
    "\n",
    "Each Encoder Layer consists of **two sublayers**:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://kikaben.com/transformers-encoder-decoder/images/encoder-layer.png\" width=\"350\"/>\n",
    "  <p><i>A single Encoder Layer</i></p>\n",
    "</div>\n",
    "\n",
    "**Sublayer 1: Multi-Head Self-Attention**\n",
    "- Every word looks at every other word in the input\n",
    "- Creates contextualized representations\n",
    "- This is the attention mechanism we built on Day 7!\n",
    "\n",
    "**Sublayer 2: Position-wise Feed-Forward Network**\n",
    "- A simple two-layer neural network applied to each position independently\n",
    "- Adds non-linear transformation power\n",
    "- Same weights shared across all positions\n",
    "\n",
    "**The Secret Sauce: Add & Norm**\n",
    "\n",
    "Each sublayer is wrapped with:\n",
    "1. **Residual Connection** (Add): `output = sublayer(x) + x`\n",
    "2. **Layer Normalization** (Norm): Stabilizes training\n",
    "\n",
    "```\n",
    "x â†’ [Sublayer] â†’ + â†’ [LayerNorm] â†’ output\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         Residual Connection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa8728",
   "metadata": {},
   "source": [
    "#### **4.3 The Encoder Stack**\n",
    "\n",
    "The original Transformer uses **N = 6 identical Encoder layers** stacked on top of each other:\n",
    "\n",
    "```\n",
    "Input Embeddings + Positional Encoding\n",
    "               â†“\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ Encoder Layer 1 â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â†“\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ Encoder Layer 2 â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â†“\n",
    "              ...\n",
    "               â†“\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ Encoder Layer 6 â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â†“\n",
    "      Encoder Output (sent to Decoder)\n",
    "```\n",
    "\n",
    "**Why Multiple Layers?**\n",
    "\n",
    "Each layer builds more **abstract representations**:\n",
    "- Layer 1: Surface-level patterns (word similarity)\n",
    "- Layer 2-3: Syntactic patterns (grammar, phrases)\n",
    "- Layer 4-5: Semantic patterns (meaning, context)\n",
    "- Layer 6: High-level understanding (intent, sentiment)\n",
    "\n",
    "**Analogy:** Like reading a text multiple times â€“ each pass reveals deeper understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f324c",
   "metadata": {},
   "source": [
    "### **5. The Decoder: Generating the Output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf8a19",
   "metadata": {},
   "source": [
    "#### **5.1 What Does the Decoder Do?**\n",
    "\n",
    "The Decoder's job is to **generate the output sequence one token at a time**, using:\n",
    "\n",
    "1. ğŸ“– The **understanding from the Encoder** (what does the input mean?)\n",
    "2. âœï¸ The **tokens generated so far** (what have I already written?)\n",
    "\n",
    "**Example: Translating Step by Step**\n",
    "\n",
    "```\n",
    "English Input: \"The cat sat on the mat\"\n",
    "                    â†“ [Encoder]\n",
    "              Encoder Output (rich representation)\n",
    "                    â†“\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚   DECODER   â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Step 1: Input \"<START>\"           â†’ Output: \"Le\"\n",
    "Step 2: Input \"<START> Le\"        â†’ Output: \"chat\"\n",
    "Step 3: Input \"<START> Le chat\"   â†’ Output: \"s'est\"\n",
    "Step 4: ...continue until \"<END>\"\n",
    "```\n",
    "\n",
    "**Key Point:** The Decoder is **autoregressive** â€“ it uses its own previous outputs as inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a5f2b",
   "metadata": {},
   "source": [
    "#### **5.2 The Decoder Layer Architecture**\n",
    "\n",
    "Each Decoder Layer is more complex â€“ it has **three sublayers**:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://kikaben.com/transformers-encoder-decoder/images/decoder-layer.png\" width=\"350\"/>\n",
    "  <p><i>A single Decoder Layer</i></p>\n",
    "</div>\n",
    "\n",
    "**Sublayer 1: Masked Multi-Head Self-Attention**\n",
    "- Output tokens attend to **previous output tokens only**\n",
    "- Uses a **causal mask** to prevent seeing future tokens\n",
    "- Remember: We can't cheat by looking at future words!\n",
    "\n",
    "**Sublayer 2: Multi-Head Cross-Attention** â­ NEW!\n",
    "- Output tokens attend to **all input tokens** (Encoder output)\n",
    "- **Query** comes from Decoder (what am I generating?)\n",
    "- **Key & Value** come from Encoder (what's in the input?)\n",
    "- This is how the Decoder \"looks at\" the source!\n",
    "\n",
    "**Sublayer 3: Position-wise Feed-Forward Network**\n",
    "- Same as in the Encoder\n",
    "- Adds non-linear transformation power\n",
    "\n",
    "Each sublayer has Add & Norm (residual + layer normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cef49d",
   "metadata": {},
   "source": [
    "#### **5.3 Cross-Attention: The Bridge Between Encoder and Decoder**\n",
    "\n",
    "Cross-attention is the **critical connection** that lets the Decoder access the Encoder's understanding:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://vaclavkosar.com/images/visual-representation-cross-attention-2.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "```python\n",
    "# In Decoder's Cross-Attention:\n",
    "Query = decoder_output @ W_q    # \"What am I looking for?\" (from Decoder)\n",
    "Key   = encoder_output @ W_k    # \"What's available?\" (from Encoder)\n",
    "Value = encoder_output @ W_v    # \"What information?\" (from Encoder)\n",
    "\n",
    "# Q from Decoder, K & V from Encoder!\n",
    "cross_attention = Attention(Query, Key, Value)\n",
    "```\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "When generating the French word \"chat\" (cat):\n",
    "- Query: \"I'm about to generate a word â€“ what should it be?\"\n",
    "- Keys: \"Here are all the English words: The, cat, sat, on, the, mat\"\n",
    "- Values: \"Here's what each English word means\"\n",
    "- Result: High attention to \"cat\" â†’ Generate \"chat\"!\n",
    "\n",
    "| Generating | Attends Most To | Why |\n",
    "|------------|----------------|-----|\n",
    "| \"Le\" (The) | \"The\" | Article alignment |\n",
    "| \"chat\" (cat) | \"cat\" | Noun translation |\n",
    "| \"s'est assis\" (sat) | \"sat\" | Verb translation |\n",
    "| \"sur\" (on) | \"on\" | Preposition alignment |\n",
    "| \"le tapis\" (the mat) | \"the mat\" | Phrase translation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912eb6e",
   "metadata": {},
   "source": [
    "### **6. Putting It All Together: The Complete Flow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db2651",
   "metadata": {},
   "source": [
    "Let's trace how data flows through the entire Transformer for a translation task:\n",
    "\n",
    "```\n",
    "                    ENCODER               DECODER\n",
    "                    \n",
    "                 \"The cat sat\"     \"Le chat s'est assis\"\n",
    "                       â†“                     â†“\n",
    "                  [Embedding]           [Embedding]\n",
    "                       â†“                     â†“\n",
    "             [Positional Encoding]  [Positional Encoding]\n",
    "                       â†“                     â†“\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ Self-Attention  â”‚  â”‚  Masked Self-Attention  â”‚\n",
    "              â”‚ (Bidirectional) â”‚  â”‚ (Causal/Unidirectional) â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â†“                     â†“\n",
    "                  [Add & Norm]          [Add & Norm]\n",
    "                       â†“                     â†“\n",
    "                       â†“          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Cross-Attention     â”‚\n",
    "                       â”‚          â”‚(Decoder Q, Encoder K,V)â”‚\n",
    "                       â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚                     â†“\n",
    "                       â”‚                [Add & Norm]\n",
    "                       â†“                     â†“\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚Feed-Forwardâ”‚        â”‚Feed-Forwardâ”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â†“                     â†“\n",
    "                  [Add & Norm]          [Add & Norm]\n",
    "                       â†“                     â†“\n",
    "                 Encoder Output      [Linear + Softmax]\n",
    "                    (memory)                 â†“\n",
    "                                    Next Token Prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ead16",
   "metadata": {},
   "source": [
    "### **7. Key Components We Need to Build**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b00397",
   "metadata": {},
   "source": [
    "Before we can build the Encoder and Decoder layers, we need a few more components:\n",
    "\n",
    "#### **7.1 Positional Encoding**\n",
    "\n",
    "**The Problem:** Self-attention is **permutation invariant** â€“ it doesn't know word order!\n",
    "\n",
    "```python\n",
    "# These would give the SAME attention patterns without positional info:\n",
    "\"The cat sat on the mat\"\n",
    "\"mat the on sat cat The\"\n",
    "```\n",
    "\n",
    "**The Solution:** Add position information to embeddings using sine and cosine functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "**Why Sine/Cosine?**\n",
    "- Different frequencies for different dimensions\n",
    "- Can extrapolate to longer sequences\n",
    "- Relative positions can be learned\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://machinelearningmastery.com/wp-content/uploads/2022/01/PE3.png\" width=\"500\"/>\n",
    "  <p><i>Positional Encoding visualization</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcb036",
   "metadata": {},
   "source": [
    "#### **7.2 Position-wise Feed-Forward Network**\n",
    "\n",
    "A simple two-layer MLP applied to each position independently:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "**Dimensions:**\n",
    "- Input: $d_{model}$ (e.g., 512)\n",
    "- Hidden: $d_{ff}$ (usually 4 Ã— $d_{model}$ = 2048)\n",
    "- Output: $d_{model}$ (back to 512)\n",
    "\n",
    "```\n",
    "Input (512) â†’ Linear (512 â†’ 2048) â†’ ReLU â†’ Linear (2048 â†’ 512) â†’ Output (512)\n",
    "```\n",
    "\n",
    "**Why This Architecture?**\n",
    "- **Expand-then-contract**: Larger hidden layer allows for more expressive transformations\n",
    "- **Position-wise**: Same weights applied to every position (efficient)\n",
    "- **Non-linearity**: ReLU adds the ability to model complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47803d59",
   "metadata": {},
   "source": [
    "#### **7.3 Layer Normalization**\n",
    "\n",
    "Normalizes across the feature dimension for each sample:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean across features\n",
    "- $\\sigma^2$ = variance across features  \n",
    "- $\\gamma$, $\\beta$ = learned parameters\n",
    "\n",
    "**Why Layer Norm?**\n",
    "- Stabilizes training (prevents extreme values)\n",
    "- Works well with variable-length sequences\n",
    "- Enables training of very deep networks\n",
    "\n",
    "**PyTorch:** `nn.LayerNorm(d_model)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58304e03",
   "metadata": {},
   "source": [
    "#### **7.4 Residual Connections**\n",
    "\n",
    "A simple but powerful idea: **add the input to the output**:\n",
    "\n",
    "$$\\text{output} = \\text{sublayer}(x) + x$$\n",
    "\n",
    "**Why Residual Connections?**\n",
    "\n",
    "1. **Gradient Flow**: Provides a \"highway\" for gradients to flow backward\n",
    "2. **Identity Mapping**: Network can easily learn to do nothing if needed\n",
    "3. **Deep Networks**: Enables training of very deep models (6+ layers)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://production-media.paperswithcode.com/methods/resnet-e1548261477164_2_mD02h5A.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "**Without residuals:** Gradients vanish in deep networks â†’ training fails\n",
    "\n",
    "**With residuals:** Gradients flow freely â†’ deep networks train successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f5d2b",
   "metadata": {},
   "source": [
    "### **8. The Complete Encoder Layer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16904fed",
   "metadata": {},
   "source": [
    "Now let's see the complete Encoder Layer with all components:\n",
    "\n",
    "```python\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        # Sublayer 1: Multi-Head Self-Attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Sublayer 2: Feed-Forward Network\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Sublayer 1: Self-Attention + Add & Norm\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # Residual + LayerNorm\n",
    "        \n",
    "        # Sublayer 2: Feed-Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))  # Residual + LayerNorm\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "**Data Flow:**\n",
    "\n",
    "```\n",
    "Input x\n",
    "   â†“\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â†“                          â”‚\n",
    "[Multi-Head Self-Attention]   â”‚\n",
    "   â†“                          â”‚\n",
    "[Dropout]                     â”‚\n",
    "   â†“                          â”‚\n",
    "   + â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (Residual Connection)\n",
    "   â†“\n",
    "[Layer Norm]\n",
    "   â†“\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â†“                          â”‚\n",
    "[Feed-Forward Network]        â”‚\n",
    "   â†“                          â”‚\n",
    "[Dropout]                     â”‚\n",
    "   â†“                          â”‚\n",
    "   + â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (Residual Connection)\n",
    "   â†“\n",
    "[Layer Norm]\n",
    "   â†“\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8419c76",
   "metadata": {},
   "source": [
    "### **9. The Complete Decoder Layer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd02702",
   "metadata": {},
   "source": [
    "The Decoder Layer is more complex with **three sublayers**:\n",
    "\n",
    "```python\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        # Sublayer 1: Masked Multi-Head Self-Attention\n",
    "        self.masked_self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Sublayer 2: Multi-Head Cross-Attention\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Sublayer 3: Feed-Forward Network\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # Sublayer 1: Masked Self-Attention + Add & Norm\n",
    "        attn_output = self.masked_self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Sublayer 2: Cross-Attention + Add & Norm\n",
    "        # Query from Decoder, Key & Value from Encoder!\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # Sublayer 3: Feed-Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "**Data Flow:**\n",
    "\n",
    "```\n",
    "Input x (from previous decoder layer or embeddings)\n",
    "   â†“\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â†“                          â”‚\n",
    "[Masked Self-Attention]       â”‚\n",
    "   (with causal mask)         â”‚\n",
    "   â†“                          â”‚\n",
    "   + â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†“\n",
    "[Layer Norm]\n",
    "   â†“\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â†“                          â”‚\n",
    "[Cross-Attention]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "   Q: from decoder            â”‚\n",
    "   K,V: from encoder_output   â”‚\n",
    "   â†“                          â”‚\n",
    "   + â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†“\n",
    "[Layer Norm]\n",
    "   â†“\n",
    "   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â†“                          â”‚\n",
    "[Feed-Forward Network]        â”‚\n",
    "   â†“                          â”‚\n",
    "   + â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â†“\n",
    "[Layer Norm]\n",
    "   â†“\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ebf2f",
   "metadata": {},
   "source": [
    "### **10. Encoder vs Decoder: Key Differences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ff02e",
   "metadata": {},
   "source": [
    "Let's summarize the key differences:\n",
    "\n",
    "| Aspect | Encoder | Decoder |\n",
    "|--------|---------|--------|\n",
    "| **Number of Sublayers** | 2 | 3 |\n",
    "| **Self-Attention Type** | Bidirectional (see all) | Masked (see only past) |\n",
    "| **Cross-Attention** | âŒ None | âœ… Yes (to encoder output) |\n",
    "| **Causal Mask** | âŒ Not needed | âœ… Required |\n",
    "| **Purpose** | Understand input | Generate output |\n",
    "| **When Q, K, V...** | All from same source | Self-attn: same, Cross-attn: Q from target |\n",
    "\n",
    "**Visual Comparison:**\n",
    "\n",
    "```\n",
    "ENCODER LAYER                    DECODER LAYER\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Self-Attention    â”‚          â”‚ Masked Self-Attentionâ”‚\n",
    "â”‚  (Bidirectional)   â”‚          â”‚    (Causal)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“                               â†“\n",
    "    [Add & Norm]                    [Add & Norm]\n",
    "         â†“                               â†“\n",
    "                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                â”‚  Cross-Attention   â”‚\n",
    "                                â”‚ (to Encoder output)â”‚\n",
    "                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                         â†“\n",
    "                                    [Add & Norm]\n",
    "         â†“                               â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Feed-Forward     â”‚          â”‚   Feed-Forward     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“                               â†“\n",
    "    [Add & Norm]                    [Add & Norm]\n",
    "         â†“                               â†“\n",
    "      Output                          Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99d0a4",
   "metadata": {},
   "source": [
    "### **11. Real-World Applications: Which Architecture to Use?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40005d6",
   "metadata": {},
   "source": [
    "Different tasks use different combinations of Encoder and Decoder:\n",
    "\n",
    "#### **Encoder-Only Models**\n",
    "\n",
    "**Examples:** BERT, RoBERTa, ALBERT\n",
    "\n",
    "**Use Cases:**\n",
    "- Text classification\n",
    "- Named Entity Recognition\n",
    "- Sentiment analysis\n",
    "- Question answering (extractive)\n",
    "\n",
    "**Why?** These tasks need **understanding** but not **generation**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Decoder-Only Models**\n",
    "\n",
    "**Examples:** GPT, GPT-2, GPT-3, GPT-4, LLaMA, Claude\n",
    "\n",
    "**Use Cases:**\n",
    "- Text generation\n",
    "- Code generation  \n",
    "- Conversational AI (ChatGPT!)\n",
    "- Story writing\n",
    "\n",
    "**Why?** These tasks focus on **generating** text autoregressively.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Encoder-Decoder Models**\n",
    "\n",
    "**Examples:** Original Transformer, T5, BART, mBART\n",
    "\n",
    "**Use Cases:**\n",
    "- Machine translation\n",
    "- Summarization\n",
    "- Question answering (generative)\n",
    "- Text-to-SQL\n",
    "\n",
    "**Why?** These tasks need to **understand input** AND **generate different output**.\n",
    "\n",
    "---\n",
    "\n",
    "| Model Type | Encoder | Decoder | Best For |\n",
    "|------------|---------|---------|----------|\n",
    "| **Encoder-Only** | âœ… | âŒ | Understanding tasks |\n",
    "| **Decoder-Only** | âŒ | âœ… | Generation tasks |\n",
    "| **Encoder-Decoder** | âœ… | âœ… | Sequence-to-sequence tasks |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f58478",
   "metadata": {},
   "source": [
    "### **12. Summary: What We Learned Today**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f96d7",
   "metadata": {},
   "source": [
    "Congratulations! You now understand the architecture of Transformers at a conceptual level! ğŸ‰\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "âœ… **Encoder**: Processes input and creates rich contextual representations\n",
    "- Uses bidirectional self-attention (sees all words)\n",
    "- Two sublayers: Self-Attention + Feed-Forward\n",
    "\n",
    "âœ… **Decoder**: Generates output sequence autoregressively\n",
    "- Uses masked self-attention (sees only past)\n",
    "- Uses cross-attention to look at encoder output\n",
    "- Three sublayers: Masked Self-Attention + Cross-Attention + Feed-Forward\n",
    "\n",
    "âœ… **Supporting Components**:\n",
    "- **Positional Encoding**: Adds word order information\n",
    "- **Feed-Forward Network**: Adds non-linear transformation\n",
    "- **Layer Normalization**: Stabilizes training\n",
    "- **Residual Connections**: Enables deep networks\n",
    "\n",
    "âœ… **Architecture Variants**:\n",
    "- Encoder-Only (BERT): Understanding tasks\n",
    "- Decoder-Only (GPT): Generation tasks\n",
    "- Encoder-Decoder (T5): Sequence-to-sequence tasks\n",
    "\n",
    "---\n",
    "\n",
    "**What's Next?**\n",
    "\n",
    "In **Part 2**, we'll implement the **Encoder Layer** from scratch in PyTorch!\n",
    "\n",
    "In **Part 3**, we'll implement the **Decoder Layer** with cross-attention!\n",
    "\n",
    "Get ready to build! ğŸ”§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cb924",
   "metadata": {},
   "source": [
    "### **13. Quick Quiz: Test Your Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd036d4",
   "metadata": {},
   "source": [
    "Before moving on, let's test your understanding!\n",
    "\n",
    "**Question 1:** Why does the Decoder use masked self-attention while the Encoder doesn't?\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answer</summary>\n",
    "\n",
    "The Decoder generates output **autoregressively** (one token at a time). During training, we must prevent it from \"cheating\" by seeing future tokens it shouldn't know yet. The Encoder, on the other hand, is just understanding the input â€“ it should see the entire input at once.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2:** In cross-attention, where do Query, Key, and Value come from?\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answer</summary>\n",
    "\n",
    "- **Query**: From the Decoder (\"What am I looking for?\")\n",
    "- **Key & Value**: From the Encoder output (\"What's in the source?\")\n",
    "\n",
    "This allows the Decoder to \"look at\" the source sequence while generating the target.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 3:** Why do we need positional encodings?\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answer</summary>\n",
    "\n",
    "Self-attention is **permutation invariant** â€“ it treats \"The cat sat\" the same as \"sat cat The\". Positional encodings inject information about word order so the model knows which word comes first, second, etc.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 4:** What is the purpose of residual connections (the \"Add\" in \"Add & Norm\")?\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answer</summary>\n",
    "\n",
    "Residual connections provide a \"gradient highway\" that allows gradients to flow freely during backpropagation. This enables training of very deep networks (6+ layers) without vanishing gradients.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Question 5:** Would you use an Encoder-only, Decoder-only, or Encoder-Decoder model for machine translation?\n",
    "\n",
    "<details>\n",
    "<summary>Click for Answer</summary>\n",
    "\n",
    "**Encoder-Decoder** model! Translation requires understanding the source language (Encoder) AND generating the target language (Decoder). Examples: Original Transformer, T5, mBART.\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
