{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12246525",
   "metadata": {},
   "source": [
    "### Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7ac8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d09ec6",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77adf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tiny shakespeare dataset\n",
    "dataset = \"tiny_shakespeare.txt\"\n",
    "\n",
    "# Load the dataset into a string\n",
    "with open(dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f8ba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b80fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Print the first 500 characters\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d6e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Check all unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b54a39",
   "metadata": {},
   "source": [
    "### Building Tokenization Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98d3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping from characters to integers\n",
    "\n",
    "stoi = { ch: i for i, ch in enumerate(chars)}\n",
    "itos = { i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64eecb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47c7f4",
   "metadata": {},
   "source": [
    "### Checking out Tokenizer used by OpenAI for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a8ed81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking vocab size for GPT 2 using tiktoken\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0134865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 4178, 612]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"hii there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d4da0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hii there'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([71, 4178, 612])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd99255",
   "metadata": {},
   "source": [
    "### Encoding entire Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "003b88a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4d802",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e1359d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "# Split the Data into Train and Validation Sets\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8ba90",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56bfa8",
   "metadata": {},
   "source": [
    "We dont feed entire data to transformer all at once as it will be computationally expensive. Therefore we break the data into chunks and feed them one by one.\n",
    "\n",
    "These chunks have some max length called. Lets call it block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcd4aba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1] # This chunk has multiple examples packed into it. There are 8 examples here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5469bc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is 47\n",
      "When input is tensor([18, 47]) the target is 56\n",
      "When input is tensor([18, 47, 56]) the target is 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc0ab4",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f9c82b",
   "metadata": {},
   "source": [
    "After chunking, we need to batch the chunks into batches of size `batch_size`. Batching keeps the Gpus busy and allows for faster training. So in a batch, we have `batch_size` number of chunks, all training at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9dc0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of input-target pairs\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0dadd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db76edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([24]) target:  tensor(43)\n",
      "when input is  tensor([24, 43]) target:  tensor(58)\n",
      "when input is  tensor([24, 43, 58]) target:  tensor(5)\n",
      "when input is  tensor([24, 43, 58,  5]) target:  tensor(57)\n",
      "when input is  tensor([24, 43, 58,  5, 57]) target:  tensor(1)\n",
      "when input is  tensor([24, 43, 58,  5, 57,  1]) target:  tensor(46)\n",
      "when input is  tensor([24, 43, 58,  5, 57,  1, 46]) target:  tensor(43)\n",
      "when input is  tensor([24, 43, 58,  5, 57,  1, 46, 43]) target:  tensor(39)\n",
      "when input is  tensor([44]) target:  tensor(53)\n",
      "when input is  tensor([44, 53]) target:  tensor(56)\n",
      "when input is  tensor([44, 53, 56]) target:  tensor(1)\n",
      "when input is  tensor([44, 53, 56,  1]) target:  tensor(58)\n",
      "when input is  tensor([44, 53, 56,  1, 58]) target:  tensor(46)\n",
      "when input is  tensor([44, 53, 56,  1, 58, 46]) target:  tensor(39)\n",
      "when input is  tensor([44, 53, 56,  1, 58, 46, 39]) target:  tensor(58)\n",
      "when input is  tensor([44, 53, 56,  1, 58, 46, 39, 58]) target:  tensor(1)\n",
      "when input is  tensor([52]) target:  tensor(58)\n",
      "when input is  tensor([52, 58]) target:  tensor(1)\n",
      "when input is  tensor([52, 58,  1]) target:  tensor(58)\n",
      "when input is  tensor([52, 58,  1, 58]) target:  tensor(46)\n",
      "when input is  tensor([52, 58,  1, 58, 46]) target:  tensor(39)\n",
      "when input is  tensor([52, 58,  1, 58, 46, 39]) target:  tensor(58)\n",
      "when input is  tensor([52, 58,  1, 58, 46, 39, 58]) target:  tensor(1)\n",
      "when input is  tensor([52, 58,  1, 58, 46, 39, 58,  1]) target:  tensor(46)\n",
      "when input is  tensor([25]) target:  tensor(17)\n",
      "when input is  tensor([25, 17]) target:  tensor(27)\n",
      "when input is  tensor([25, 17, 27]) target:  tensor(10)\n",
      "when input is  tensor([25, 17, 27, 10]) target:  tensor(0)\n",
      "when input is  tensor([25, 17, 27, 10,  0]) target:  tensor(21)\n",
      "when input is  tensor([25, 17, 27, 10,  0, 21]) target:  tensor(1)\n",
      "when input is  tensor([25, 17, 27, 10,  0, 21,  1]) target:  tensor(54)\n",
      "when input is  tensor([25, 17, 27, 10,  0, 21,  1, 54]) target:  tensor(39)\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(\"when input is \", context, \"target: \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfda27e",
   "metadata": {},
   "source": [
    "### Simplest NN: Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53052b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        # x is (B, T) tensor of indices.\n",
    "        logits = self.token_embedding_table(x) # (B, T, C) = (4, 8, 65)\n",
    "        \n",
    "        # Loss\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = torch.functional.F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        # x is (B, T) tensor of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(x)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim = -1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            x = torch.cat((x, idx_next), dim = 1) # (B, T+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be11035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.7405, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b67aecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((1, 1), dtype = torch.long) # Since idx 0 is a new line character\n",
    "out = m.generate(x, max_new_tokens = 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5f279",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "185ed163",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c23db8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.456190347671509\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # Backpropagate and update weights\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Reset the gradients\n",
    "    optimizer.zero_grad(set_to_none = True) # Set to None instead of zero to free up memory\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00122c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BRS:\n",
      "\n",
      "THAnrt t fa boun-s trconnou\n",
      "\n",
      "No: ENGUS:\n",
      "tepare ofo.'s ne:\n",
      "We Prellothe, s;\n",
      "NE: t an re, belono\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((1, 1), dtype = torch.long) # Since idx 0 is a new line character\n",
    "out = m.generate(x, max_new_tokens = 100)\n",
    "print(decode(out[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
