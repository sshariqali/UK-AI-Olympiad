{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3e79ee",
   "metadata": {},
   "source": [
    "### **Overview: What We'll Build**\n",
    "\n",
    "In this notebook, we'll implement the complete Encoder from scratch. We'll build it step by step:\n",
    "\n",
    "| Steps | Component |\n",
    "|------|-----------|\n",
    "| **1** | Positional Encoding |\n",
    "| **2** | Position-wise Feed-Forward Network |\n",
    "| **3** | Complete Encoder Layer |\n",
    "| **4** | Encoder Stack + Testing |\n",
    "\n",
    "Let's start with **Positional Encoding**! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df3c90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Step 1: Positional Encoding**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430efa57",
   "metadata": {},
   "source": [
    "### **1.1 Why Do We Need Positional Encoding?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43843a0f",
   "metadata": {},
   "source": [
    "#### **The Problem: Self-Attention Has No Sense of Order**\n",
    "\n",
    "Remember how self-attention works? It computes relationships between all pairs of words using dot products:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**The Issue:** This operation is **permutation invariant** ‚Äì it doesn't care about word order!\n",
    "\n",
    "**Demonstration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3bf370",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Set random seed for reproducibility\u001b[39;00m\n\u001b[32m      9\u001b[39m torch.manual_seed(\u001b[32m42\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask = None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch_size, seq_len, d_k)\n",
    "        K: Key tensor (batch_size, seq_len, d_k)\n",
    "        V: Value tensor (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask (batch_size, seq_len, seq_len) or (seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len, d_v)\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Get dimension for scaling\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1 & 2: Compute scaled scores\n",
    "    # Q: (batch, seq_len, d_k)\n",
    "    # K.transpose: (batch, d_k, seq_len)\n",
    "    # scores: (batch, seq_len, seq_len)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 4: Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357566c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention(Q, K, V):\n",
    "    \"\"\"Simple scaled dot-product attention\"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, V), weights\n",
    "\n",
    "# Create word embeddings (without position info)\n",
    "# Sentence: \"cat sat mat\" - 3 words, 4-dimensional embeddings\n",
    "embeddings = {\n",
    "    'cat': torch.tensor([1.0, 0.0, 0.5, 0.2]),\n",
    "    'sat': torch.tensor([0.0, 1.0, 0.3, 0.8]),\n",
    "    'mat': torch.tensor([0.5, 0.0, 1.0, 0.1])\n",
    "}\n",
    "\n",
    "# Original order: \"cat sat mat\"\n",
    "sentence1 = torch.stack([embeddings['cat'], embeddings['sat'], embeddings['mat']]).unsqueeze(0)\n",
    "\n",
    "# Shuffled order: \"mat cat sat\"\n",
    "sentence2 = torch.stack([embeddings['mat'], embeddings['cat'], embeddings['sat']]).unsqueeze(0)\n",
    "\n",
    "print(\"Original sentence: 'cat sat mat'\")\n",
    "print(f\"Shape: {sentence1.shape}\")\n",
    "print()\n",
    "print(\"Shuffled sentence: 'mat cat sat'\")\n",
    "print(f\"Shape: {sentence2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbf623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention for both orders\n",
    "output1, weights1 = simple_attention(sentence1, sentence1, sentence1)\n",
    "output2, weights2 = simple_attention(sentence2, sentence2, sentence2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ATTENTION WEIGHTS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Original: 'cat sat mat' ---\")\n",
    "print(\"Attention weights:\")\n",
    "print(weights1.squeeze().detach().numpy().round(3))\n",
    "\n",
    "print(\"\\n--- Shuffled: 'mat cat sat' ---\")\n",
    "print(\"Attention weights:\")\n",
    "print(weights2.squeeze().detach().numpy().round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY OBSERVATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNotice how the attention pattern for 'cat' is the SAME\")\n",
    "print(\"regardless of its position in the sentence!\")\n",
    "print(\"\\nüö® The model has NO IDEA about word order!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9e2a9",
   "metadata": {},
   "source": [
    "#### **Why This Is a Problem**\n",
    "\n",
    "Word order is **crucial** for understanding language:\n",
    "\n",
    "| Sentence | Meaning |\n",
    "|----------|--------|\n",
    "| \"The dog bit the man\" | üêï Dog is the attacker |\n",
    "| \"The man bit the dog\" | üë® Man is the attacker |\n",
    "\n",
    "Same words, completely different meanings! Without positional information, the model would treat these identically.\n",
    "\n",
    "**Solution:** Add position information to the embeddings using **Positional Encoding**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838fe47",
   "metadata": {},
   "source": [
    "### **1.2 The Positional Encoding Formula**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29437dec",
   "metadata": {},
   "source": [
    "The original Transformer paper uses **sine and cosine functions** of different frequencies:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $pos$ = position in the sequence (0, 1, 2, ...)\n",
    "- $i$ = dimension index (0, 1, 2, ..., $d_{model}/2$)\n",
    "- $d_{model}$ = embedding dimension (e.g., 512)\n",
    "\n",
    "**Breaking It Down:**\n",
    "\n",
    "| Component | Meaning |\n",
    "|-----------|--------|\n",
    "| $pos$ | Which position in the sequence (0th word, 1st word, etc.) |\n",
    "| $i$ | Which dimension of the encoding |\n",
    "| $10000^{2i/d_{model}}$ | Wavelength that increases with dimension |\n",
    "| Even dimensions (2i) | Use sine |\n",
    "| Odd dimensions (2i+1) | Use cosine |\n",
    "\n",
    "**Intuition:** Each dimension captures position at a different \"frequency\":\n",
    "- Low dimensions (small $i$): High frequency, captures fine-grained positions\n",
    "- High dimensions (large $i$): Low frequency, captures coarse positions\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://machinelearningmastery.com/wp-content/uploads/2022/01/PE3.png\" width=\"600\"/>\n",
    "  <p><i>Positional Encoding: Each column is a position, each row is a dimension</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f837ea",
   "metadata": {},
   "source": [
    "### **1.3 Step-by-Step Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c3989",
   "metadata": {},
   "source": [
    "Let's build the Positional Encoding step by step to understand each part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57380ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_seq_len = 10  # Maximum sequence length\n",
    "d_model = 8       # Embedding dimension (small for visualization)\n",
    "\n",
    "print(f\"Creating positional encoding for:\")\n",
    "print(f\"  - Max sequence length: {max_seq_len}\")\n",
    "print(f\"  - Embedding dimension: {d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810533fa",
   "metadata": {},
   "source": [
    "#### **Step 1a: Create Position Indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create position indices: [0, 1, 2, ..., max_seq_len-1]\n",
    "# Shape: (max_seq_len, 1)\n",
    "position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "print(\"Position indices (pos):\")\n",
    "print(position.T)  # Transpose for horizontal display\n",
    "print(f\"\\nShape: {position.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e8765",
   "metadata": {},
   "source": [
    "#### **Step 1b: Create the Division Term**\n",
    "\n",
    "We need to compute $10000^{2i/d_{model}}$ for each dimension $i$.\n",
    "\n",
    "For numerical stability, we compute this as:\n",
    "$$10000^{2i/d_{model}} = e^{2i \\cdot \\ln(10000) / d_{model}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0480e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dimension indices for even dimensions: [0, 2, 4, 6, ...]\n",
    "# We only need d_model/2 values since each i covers 2 dimensions (sin and cos)\n",
    "div_term_indices = torch.arange(0, d_model, 2).float()\n",
    "print(f\"Dimension indices (2i): {div_term_indices.numpy()}\")\n",
    "\n",
    "# Compute the division term: 10000^(2i/d_model)\n",
    "# Using exp(log) for numerical stability\n",
    "div_term = torch.exp(div_term_indices * (-np.log(10000.0) / d_model))\n",
    "\n",
    "print(f\"\\nDivision terms (1/10000^(2i/d_model)):\")\n",
    "print(div_term.numpy().round(6))\n",
    "\n",
    "print(f\"\\nShape: {div_term.shape}\")\n",
    "print(f\"\\nNote: These decrease as dimension increases (lower frequency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcffe94",
   "metadata": {},
   "source": [
    "#### **Step 1c: Compute Sine and Cosine Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the positional encoding matrix\n",
    "# Shape: (max_seq_len, d_model)\n",
    "pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "# Compute position * div_term for all positions and dimensions\n",
    "# position: (max_seq_len, 1)\n",
    "# div_term: (d_model/2,)\n",
    "# Result: (max_seq_len, d_model/2)\n",
    "angles = position * div_term\n",
    "\n",
    "print(\"Angles (pos * div_term):\")\n",
    "print(angles.numpy().round(3))\n",
    "print(f\"\\nShape: {angles.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sine to even indices (0, 2, 4, ...)\n",
    "pe[:, 0::2] = torch.sin(angles)\n",
    "\n",
    "# Apply cosine to odd indices (1, 3, 5, ...)\n",
    "pe[:, 1::2] = torch.cos(angles)\n",
    "\n",
    "print(\"Positional Encoding Matrix:\")\n",
    "print(pe.numpy().round(3))\n",
    "print(f\"\\nShape: {pe.shape}\")\n",
    "print(\"\\nRows = Positions (0 to 9)\")\n",
    "print(\"Columns = Dimensions (0 to 7)\")\n",
    "print(\"Even columns = sin, Odd columns = cos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5c60d",
   "metadata": {},
   "source": [
    "### **1.4 Visualizing Positional Encodings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b37fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger positional encoding for better visualization\n",
    "max_seq_len_viz = 100\n",
    "d_model_viz = 64\n",
    "\n",
    "position_viz = torch.arange(0, max_seq_len_viz, dtype=torch.float).unsqueeze(1)\n",
    "div_term_viz = torch.exp(torch.arange(0, d_model_viz, 2).float() * (-np.log(10000.0) / d_model_viz))\n",
    "\n",
    "pe_viz = torch.zeros(max_seq_len_viz, d_model_viz)\n",
    "pe_viz[:, 0::2] = torch.sin(position_viz * div_term_viz)\n",
    "pe_viz[:, 1::2] = torch.cos(position_viz * div_term_viz)\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pe_viz.numpy(), cmap='RdBu_r', center=0, \n",
    "            xticklabels=8, yticklabels=10)\n",
    "plt.xlabel('Embedding Dimension', fontsize=12)\n",
    "plt.ylabel('Position in Sequence', fontsize=12)\n",
    "plt.title('Positional Encoding Visualization\\n(Red = Positive, Blue = Negative)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"1. Left side (low dimensions): High frequency patterns - changes rapidly with position\")\n",
    "print(\"2. Right side (high dimensions): Low frequency patterns - changes slowly\")\n",
    "print(\"3. Each position has a UNIQUE pattern!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84de7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual dimensions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "dimensions_to_plot = [0, 1, 30, 31]  # Low and high frequency dimensions\n",
    "positions = np.arange(max_seq_len_viz)\n",
    "\n",
    "for idx, dim in enumerate(dimensions_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = pe_viz[:, dim].numpy()\n",
    "    \n",
    "    ax.plot(positions, values, 'b-', linewidth=2)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Position', fontsize=10)\n",
    "    ax.set_ylabel('Encoding Value', fontsize=10)\n",
    "    \n",
    "    func_type = 'sin' if dim % 2 == 0 else 'cos'\n",
    "    freq_type = 'HIGH' if dim < 16 else 'LOW'\n",
    "    ax.set_title(f'Dimension {dim} ({func_type}) - {freq_type} Frequency', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Positional Encoding: Different Frequencies at Different Dimensions', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Insight:\")\n",
    "print(\"- Low dimensions (0, 1): Oscillate rapidly - distinguish nearby positions\")\n",
    "print(\"- High dimensions (30, 31): Oscillate slowly - capture global position\")\n",
    "print(\"- Together, they create a UNIQUE fingerprint for each position!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e01ee69",
   "metadata": {},
   "source": [
    "### **1.5 The Complete PositionalEncoding Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e411a6",
   "metadata": {},
   "source": [
    "Now let's wrap everything into a proper `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding using sine and cosine functions.\n",
    "    \n",
    "    Adds positional information to input embeddings so the model\n",
    "    can distinguish between different positions in the sequence.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Embedding dimension (must match input embedding size)\n",
    "        max_seq_len: Maximum sequence length to support\n",
    "        dropout: Dropout probability (default: 0.1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        # Shape: (max_seq_len, d_model)\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        # Position indices: [0, 1, 2, ..., max_seq_len-1]\n",
    "        # Shape: (max_seq_len, 1)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Division term: 10000^(2i/d_model) computed as exp for stability\n",
    "        # Shape: (d_model/2,)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension: (1, max_seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but should be saved with model)\n",
    "        # Buffers are tensors that are part of the module but not trainable\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of same shape with positional encoding added\n",
    "        \"\"\"\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        # self.pe: (1, max_seq_len, d_model)\n",
    "        # We slice pe to match the actual sequence length\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Add positional encoding (broadcasts across batch dimension)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        \n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"‚úÖ PositionalEncoding class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ad036",
   "metadata": {},
   "source": [
    "### **1.6 Testing the PositionalEncoding Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "\n",
    "# Create the positional encoding module\n",
    "pos_encoder = PositionalEncoding(d_model=d_model, max_seq_len=100, dropout=0.0)\n",
    "\n",
    "# Create dummy embeddings (simulating word embeddings)\n",
    "embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"Input embeddings:\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Sample values (batch 0, position 0): {embeddings[0, 0, :4].numpy().round(3)}\")\n",
    "\n",
    "# Apply positional encoding\n",
    "output = pos_encoder(embeddings)\n",
    "\n",
    "print(\"\\nOutput (embeddings + positional encoding):\")\n",
    "print(f\"  Shape: {output.shape}\")\n",
    "print(f\"  Sample values (batch 0, position 0): {output[0, 0, :4].detach().numpy().round(3)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Positional encoding applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that different positions get different encodings\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFYING UNIQUE POSITION ENCODINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the positional encodings for first 5 positions\n",
    "pe_values = pos_encoder.pe[0, :5, :8].numpy().round(3)  # First 5 positions, first 8 dims\n",
    "\n",
    "print(\"\\nPositional encodings for positions 0-4 (first 8 dimensions):\")\n",
    "print()\n",
    "for pos in range(5):\n",
    "    print(f\"Position {pos}: {pe_values[pos]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Each position has a unique encoding pattern!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c460551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's verify that attention can distinguish positions\n",
    "print(\"=\" * 60)\n",
    "print(\"ATTENTION WITH POSITIONAL ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Same word embeddings as before\n",
    "embeddings = {\n",
    "    'cat': torch.tensor([1.0, 0.0, 0.5, 0.2]),\n",
    "    'sat': torch.tensor([0.0, 1.0, 0.3, 0.8]),\n",
    "    'mat': torch.tensor([0.5, 0.0, 1.0, 0.1])\n",
    "}\n",
    "\n",
    "# Create positional encoder for d_model=4\n",
    "pos_enc_small = PositionalEncoding(d_model=4, max_seq_len=10, dropout=0.0)\n",
    "\n",
    "# Original order: \"cat sat mat\"\n",
    "sentence1 = torch.stack([embeddings['cat'], embeddings['sat'], embeddings['mat']]).unsqueeze(0)\n",
    "\n",
    "# Shuffled order: \"mat cat sat\"  \n",
    "sentence2 = torch.stack([embeddings['mat'], embeddings['cat'], embeddings['sat']]).unsqueeze(0)\n",
    "\n",
    "# Add positional encodings\n",
    "sentence1_with_pos = pos_enc_small(sentence1)\n",
    "sentence2_with_pos = pos_enc_small(sentence2)\n",
    "\n",
    "# Compute attention\n",
    "output1, weights1 = simple_attention(sentence1_with_pos, sentence1_with_pos, sentence1_with_pos)\n",
    "output2, weights2 = simple_attention(sentence2_with_pos, sentence2_with_pos, sentence2_with_pos)\n",
    "\n",
    "print(\"\\n--- Original: 'cat sat mat' (with positional encoding) ---\")\n",
    "print(\"Attention weights:\")\n",
    "print(weights1.squeeze().detach().numpy().round(3))\n",
    "\n",
    "print(\"\\n--- Shuffled: 'mat cat sat' (with positional encoding) ---\")\n",
    "print(\"Attention weights:\")\n",
    "print(weights2.squeeze().detach().numpy().round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY OBSERVATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ Now the attention patterns are DIFFERENT!\")\n",
    "print(\"‚úÖ The model can distinguish word ORDER!\")\n",
    "print(\"‚úÖ Same words at different positions ‚Üí different representations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53affdaf",
   "metadata": {},
   "source": [
    "### **1.7 Why Sine and Cosine? The Clever Math**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b57c335",
   "metadata": {},
   "source": [
    "You might wonder: Why use sine and cosine specifically? There are several elegant reasons:\n",
    "\n",
    "#### **Reason 1: Unique Encoding for Each Position**\n",
    "\n",
    "The combination of sines and cosines at different frequencies creates a unique \"fingerprint\" for each position.\n",
    "\n",
    "#### **Reason 2: Bounded Values**\n",
    "\n",
    "Sine and cosine are always between -1 and 1, which is numerically stable:\n",
    "$$-1 \\leq \\sin(x), \\cos(x) \\leq 1$$\n",
    "\n",
    "#### **Reason 3: Relative Position Information**\n",
    "\n",
    "This is the cleverest part! For any fixed offset $k$, we can express $PE_{pos+k}$ as a linear function of $PE_{pos}$:\n",
    "\n",
    "$$\\sin(pos + k) = \\sin(pos)\\cos(k) + \\cos(pos)\\sin(k)$$\n",
    "$$\\cos(pos + k) = \\cos(pos)\\cos(k) - \\sin(pos)\\sin(k)$$\n",
    "\n",
    "This means the model can easily learn to attend to **relative positions** (e.g., \"the word 3 positions back\")!\n",
    "\n",
    "#### **Reason 4: Extrapolation to Longer Sequences**\n",
    "\n",
    "Since sine and cosine are continuous functions, the encoding naturally extends to any sequence length, even longer than seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate relative position property\n",
    "print(\"=\" * 60)\n",
    "print(\"RELATIVE POSITION DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get positional encodings for positions 0, 3, and 10, 13 (same relative offset of 3)\n",
    "pe_large = PositionalEncoding(d_model=64, max_seq_len=100, dropout=0.0)\n",
    "\n",
    "pe_0 = pe_large.pe[0, 0, :8].numpy()\n",
    "pe_3 = pe_large.pe[0, 3, :8].numpy()\n",
    "pe_10 = pe_large.pe[0, 10, :8].numpy()\n",
    "pe_13 = pe_large.pe[0, 13, :8].numpy()\n",
    "\n",
    "# Compute differences\n",
    "diff_0_to_3 = pe_3 - pe_0\n",
    "diff_10_to_13 = pe_13 - pe_10\n",
    "\n",
    "print(\"\\nDifference between position 0 and 3:\")\n",
    "print(diff_0_to_3.round(4))\n",
    "\n",
    "print(\"\\nDifference between position 10 and 13:\")\n",
    "print(diff_10_to_13.round(4))\n",
    "\n",
    "print(\"\\nüéØ The differences are SIMILAR (not identical due to different frequencies)!\")\n",
    "print(\"This allows the model to learn relative position relationships.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
