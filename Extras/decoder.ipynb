{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c51e897",
   "metadata": {},
   "source": [
    "### **9. The Complete Decoder Layer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79ebd5",
   "metadata": {},
   "source": [
    "The Decoder Layer is more complex with **three sublayers**:\n",
    "\n",
    "```python\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        # Sublayer 1: Masked Multi-Head Self-Attention\n",
    "        self.masked_self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Sublayer 2: Multi-Head Cross-Attention\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Sublayer 3: Feed-Forward Network\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # Sublayer 1: Masked Self-Attention + Add & Norm\n",
    "        attn_output = self.masked_self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Sublayer 2: Cross-Attention + Add & Norm\n",
    "        # Query from Decoder, Key & Value from Encoder!\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # Sublayer 3: Feed-Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "**Data Flow:**\n",
    "\n",
    "```\n",
    "Input x (from previous decoder layer or embeddings)\n",
    "   ↓\n",
    "   ├──────────────────────────┐\n",
    "   ↓                          │\n",
    "[Masked Self-Attention]       │\n",
    "   (with causal mask)         │\n",
    "   ↓                          │\n",
    "   + ←────────────────────────┘\n",
    "   ↓\n",
    "[Layer Norm]\n",
    "   ↓\n",
    "   ├──────────────────────────┐\n",
    "   ↓                          │\n",
    "[Cross-Attention]─────────────┤\n",
    "   Q: from decoder            │\n",
    "   K,V: from encoder_output   │\n",
    "   ↓                          │\n",
    "   + ←────────────────────────┘\n",
    "   ↓\n",
    "[Layer Norm]\n",
    "   ↓\n",
    "   ├──────────────────────────┐\n",
    "   ↓                          │\n",
    "[Feed-Forward Network]        │\n",
    "   ↓                          │\n",
    "   + ←────────────────────────┘\n",
    "   ↓\n",
    "[Layer Norm]\n",
    "   ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bca6a5",
   "metadata": {},
   "source": [
    "### **10. Encoder vs Decoder: Key Differences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1c79d",
   "metadata": {},
   "source": [
    "Let's summarize the key differences:\n",
    "\n",
    "| Aspect | Encoder | Decoder |\n",
    "|--------|---------|--------|\n",
    "| **Number of Sublayers** | 2 | 3 |\n",
    "| **Self-Attention Type** | Bidirectional (see all) | Masked (see only past) |\n",
    "| **Cross-Attention** | ❌ None | ✅ Yes (to encoder output) |\n",
    "| **Causal Mask** | ❌ Not needed | ✅ Required |\n",
    "| **Purpose** | Understand input | Generate output |\n",
    "| **When Q, K, V...** | All from same source | Self-attn: same, Cross-attn: Q from target |\n",
    "\n",
    "**Visual Comparison:**\n",
    "\n",
    "```\n",
    "ENCODER LAYER                    DECODER LAYER\n",
    "\n",
    "┌────────────────────┐          ┌────────────────────┐\n",
    "│  Self-Attention    │          │ Masked Self-Attention│\n",
    "│  (Bidirectional)   │          │    (Causal)         │\n",
    "└────────────────────┘          └────────────────────┘\n",
    "         ↓                               ↓\n",
    "    [Add & Norm]                    [Add & Norm]\n",
    "         ↓                               ↓\n",
    "                                ┌────────────────────┐\n",
    "                                │  Cross-Attention   │\n",
    "                                │ (to Encoder output)│\n",
    "                                └────────────────────┘\n",
    "                                         ↓\n",
    "                                    [Add & Norm]\n",
    "         ↓                               ↓\n",
    "┌────────────────────┐          ┌────────────────────┐\n",
    "│   Feed-Forward     │          │   Feed-Forward     │\n",
    "└────────────────────┘          └────────────────────┘\n",
    "         ↓                               ↓\n",
    "    [Add & Norm]                    [Add & Norm]\n",
    "         ↓                               ↓\n",
    "      Output                          Output\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
