{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d865e26",
   "metadata": {},
   "source": [
    "### **1. Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d8dc2",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "The goal is to develop a neural network model that can accurately classify images of clothing items into their respective categories. Given a grayscale image of a clothing item, our model should predict which category it belongs to among 10 different classes.\n",
    "\n",
    "**Dataset: Fashion MNIST**\n",
    "\n",
    "Fashion MNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**The 10 Classes:**\n",
    "\n",
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser     |\n",
    "| 2     | Pullover    |\n",
    "| 3     | Dress       |\n",
    "| 4     | Coat        |\n",
    "| 5     | Sandal      |\n",
    "| 6     | Shirt       |\n",
    "| 7     | Sneaker     |\n",
    "| 8     | Bag         |\n",
    "| 9     | Ankle boot  |\n",
    "\n",
    "**Dataset Properties:**\n",
    "- **Training images:** 60,000\n",
    "- **Test images:** 10,000\n",
    "- **Image size:** 28x28 pixels\n",
    "- **Color:** Grayscale (1 channel)\n",
    "- **Pixel values:** 0-255 (0 = black, 255 = white)\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "To address this problem, we will create a `Multi-Layer Neural Network model` using `PyTorch` to implement `Image Classification` - a Machine Learning task.\n",
    "\n",
    "**Tools**\n",
    "- **NumPy:** A library for scientific computing, mainly involving linear algebra operations.\n",
    "- **Matplotlib:** A library for plotting and visualizing data.\n",
    "- **PyTorch:** A library for flexibility and speed when building deep learning models.\n",
    "- **torchvision:** PyTorch's computer vision library for datasets and transformations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48823c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e39932",
   "metadata": {},
   "source": [
    "### **2. Loading and Exploring the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b58868",
   "metadata": {},
   "source": [
    "PyTorch's `torchvision` library makes it easy to download and load the Fashion MNIST dataset. We'll:\n",
    "\n",
    "1. **Download the dataset** (if not already present)\n",
    "2. **Apply transformations** to convert images to PyTorch tensors\n",
    "3. **Create data loaders** for efficient batch processing\n",
    "\n",
    "**What is a Tensor?**\n",
    "\n",
    "A tensor is a multi-dimensional array (like NumPy arrays) that can run on GPUs for faster computation. For images:\n",
    "- A grayscale image is a 2D tensor: `(height, width)`\n",
    "- In PyTorch, we add a channel dimension: `(channels, height, width)`\n",
    "- For Fashion MNIST: `(1, 28, 28)` - 1 channel (grayscale), 28x28 pixels\n",
    "\n",
    "**What is a DataLoader?**\n",
    "\n",
    "A DataLoader wraps a dataset and provides:\n",
    "- **Batching:** Groups multiple samples together for efficient training\n",
    "- **Shuffling:** Randomizes the order of samples to improve learning\n",
    "- **Parallel loading:** Loads data in the background while the model trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "# ToTensor() converts PIL images (0-255) to tensors with values in range [0, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download and load the training dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',           # Directory to store the dataset\n",
    "    train=True,              # Load the training set\n",
    "    download=True,           # Download if not present\n",
    "    transform=transform      # Apply transformations\n",
    ")\n",
    "\n",
    "# Download and load the test dataset\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,             # Load the test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64  # Number of images to process at once\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,           # Shuffle the training data\n",
    "    num_workers=2           # Number of parallel workers for data loading\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,          # Don't shuffle test data\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b0e1e",
   "metadata": {},
   "source": [
    "### **3. Visualizing the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbf1f4",
   "metadata": {},
   "source": [
    "Before building our model, let's visualize some samples from the dataset to understand what we're working with. This helps us:\n",
    "\n",
    "1. Verify the data loaded correctly\n",
    "2. Understand the image quality and characteristics\n",
    "3. See the variety of clothing items in different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names for Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Get a single sample to understand the data structure\n",
    "sample_image, sample_label = train_dataset[0]\n",
    "print(f\"Image shape: {sample_image.shape}\")  # Should be (1, 28, 28)\n",
    "print(f\"Image type: {type(sample_image)}\")\n",
    "print(f\"Label: {sample_label} ({class_names[sample_label]})\")\n",
    "print(f\"Pixel value range: [{sample_image.min():.3f}, {sample_image.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a grid of sample images\n",
    "def show_images(dataset, num_samples=16):\n",
    "    \"\"\"\n",
    "    Display a grid of sample images from the dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to sample from\n",
    "        num_samples: Number of images to display\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    fig.suptitle('Sample Images from Fashion MNIST', fontsize=16, y=0.995)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_samples:\n",
    "            # Get a random sample\n",
    "            idx = np.random.randint(0, len(dataset))\n",
    "            image, label = dataset[idx]\n",
    "            \n",
    "            # Convert tensor to numpy and reshape for display\n",
    "            # squeeze() removes the channel dimension: (1, 28, 28) -> (28, 28)\n",
    "            img_np = image.squeeze().numpy()\n",
    "            \n",
    "            # Display the image\n",
    "            ax.imshow(img_np, cmap='gray')\n",
    "            ax.set_title(f'{class_names[label]}', fontsize=10)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show sample images\n",
    "show_images(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "def analyze_class_distribution(dataset):\n",
    "    \"\"\"\n",
    "    Count the number of samples in each class\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset to analyze\n",
    "    \"\"\"\n",
    "    labels = [label for _, label in dataset]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(unique)), counts, color='steelblue', edgecolor='black')\n",
    "    plt.xlabel('Class', fontsize=12)\n",
    "    plt.ylabel('Number of Samples', fontsize=12)\n",
    "    plt.title('Class Distribution in Training Set', fontsize=14)\n",
    "    plt.xticks(range(len(unique)), [class_names[i] for i in unique], rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(i, count + 50, str(count), ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClass distribution:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"{class_names[label]:15s}: {count:5d} samples ({count/len(dataset)*100:.1f}%)\")\n",
    "\n",
    "analyze_class_distribution(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95da6c",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "\n",
    "1. **Balanced Dataset:** Each class has exactly 6,000 samples in the training set, making this a perfectly balanced dataset. This is ideal for training as the model won't be biased toward any particular class.\n",
    "\n",
    "2. **Image Quality:** The images are relatively simple grayscale representations at 28x28 resolution. Despite the low resolution, human observers can easily identify most items.\n",
    "\n",
    "3. **Challenge Level:** Some classes are visually similar (e.g., T-shirt vs. Shirt, Pullover vs. Coat), which makes this a moderately challenging classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1add7b",
   "metadata": {},
   "source": [
    "### **4. Understanding Classification vs Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b2b67",
   "metadata": {},
   "source": [
    "In our previous notebook, we built a model to predict exam scores - a **regression** task. Now we're building a model to classify clothing items - a **classification** task. What's the difference?\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*nf7nPIQDJXg8RjJTGuF07A.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Regression vs Classification:**\n",
    "\n",
    "| Aspect | Regression | Classification |\n",
    "|--------|-----------|---------------|\n",
    "| **Output Type** | Continuous numerical value | Discrete category/class |\n",
    "| **Examples** | Predicting exam scores (0-100), house prices, temperature | Identifying clothing type, spam detection, disease diagnosis |\n",
    "| **Previous Task** | Exam Score: 67.5, 89.2, 54.8, etc. | - |\n",
    "| **Current Task** | - | Clothing Type: T-shirt, Trouser, Dress, etc. |\n",
    "| **Output Range** | Any real number (e.g., -∞ to +∞) | Fixed set of categories (e.g., 0-9 for our 10 classes) |\n",
    "| **Loss Function** | Mean Squared Error (MSE) | Cross-Entropy Loss |\n",
    "| **Activation Function** | Identity (linear) or ReLU | Softmax (for multi-class) |\n",
    "| **Evaluation Metrics** | MSE, RMSE, MAE, R² | Accuracy, Precision, Recall, F1-Score |\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "1. **Output Interpretation:**\n",
    "   - **Regression:** \"This student will score 85.3 on the exam\" - a specific number\n",
    "   - **Classification:** \"This image is a T-shirt\" - a specific category\n",
    "\n",
    "2. **Model Output:**\n",
    "   - **Regression:** Single value (or multiple values for multi-output)\n",
    "   - **Classification:** Probability distribution over all classes\n",
    "\n",
    "3. **Training Objective:**\n",
    "   - **Regression:** Minimize the difference between predicted and actual values\n",
    "   - **Classification:** Maximize the probability of the correct class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffac86e",
   "metadata": {},
   "source": [
    "**How Classification Works (Mathematically):**\n",
    "\n",
    "For our Fashion MNIST task with 10 classes, the neural network outputs 10 numbers (one for each class). These raw outputs are called **logits**.\n",
    "\n",
    "We then apply the **Softmax function** to convert these logits into probabilities:\n",
    "\n",
    "$$P(y = i | x) = \\frac{e^{z_i}}{\\sum_{j=1}^{10} e^{z_j}}$$\n",
    "\n",
    "Where:\n",
    "- $z_i$ is the logit (raw output) for class $i$\n",
    "- $P(y = i | x)$ is the probability that the input $x$ belongs to class $i$\n",
    "- The denominator ensures all probabilities sum to 1\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If the model outputs for an image are:\n",
    "```\n",
    "Logits:        [2.3, 5.8, 1.2, 0.5, -1.0, 0.8, 1.5, -0.5, 3.2, 1.8]\n",
    "After Softmax: [0.03, 0.67, 0.01, 0.01, 0.00, 0.00, 0.01, 0.00, 0.06, 0.01]\n",
    "Classes:       [T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Boot]\n",
    "```\n",
    "\n",
    "The model predicts **Trouser** with 67% confidence (highest probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed4d7d",
   "metadata": {},
   "source": [
    "### **5. Why Do We Need a Bigger Neural Network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef752a",
   "metadata": {},
   "source": [
    "In our previous notebook, we used a simple **Perceptron** (single neuron) for predicting exam scores from structured data (numbers in a table). For Fashion MNIST, we need something more powerful. Why?\n",
    "\n",
    "**1. Input Complexity:**\n",
    "\n",
    "- **Previous Task (Exam Scores):** ~30 features (hours studied, attendance, etc.)\n",
    "- **Current Task (Fashion MNIST):** 784 features (28 × 28 = 784 pixels)\n",
    "\n",
    "Each image has 784 input values, and the relationships between these pixels are complex.\n",
    "\n",
    "**2. Problem Complexity:**\n",
    "\n",
    "A single perceptron can only learn **linear** relationships. Let's understand this:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*ZB6H4HuF58VcMOWbdpcRxQ.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "- **Linear (Simple):** \"If hours studied increases, exam score increases\" - a straight-line relationship\n",
    "- **Non-Linear (Complex):** \"This pattern of pixels forms a T-shirt shape\" - requires understanding curves, edges, and spatial patterns\n",
    "\n",
    "**3. Feature Learning:**\n",
    "\n",
    "For images, we need the model to learn **hierarchical features**:\n",
    "\n",
    "```\n",
    "Low-level features  →  Mid-level features  →  High-level features  →  Classification\n",
    "   (edges, lines)        (shapes, textures)      (sleeves, collars)      (T-shirt)\n",
    "```\n",
    "\n",
    "A single perceptron cannot learn this hierarchy - it treats all pixels independently.\n",
    "\n",
    "**4. The Solution: Multi-Layer Neural Networks**\n",
    "\n",
    "By stacking multiple layers of neurons, we can:\n",
    "- Learn non-linear patterns through **activation functions**\n",
    "- Build hierarchical representations of the data\n",
    "- Capture complex relationships between features\n",
    "\n",
    "This is why we need to build a **deeper** neural network with multiple layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13536822",
   "metadata": {},
   "source": [
    "Now that we understand why we need a bigger network, let's design and build one! We'll create a **Multi-Layer Perceptron (MLP)** - also called a **Fully Connected Neural Network** or **Dense Neural Network**.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*3fA77_mLNiJTSgZFhYnU0Q.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Our Network Architecture:**\n",
    "\n",
    "We'll build a 3-layer neural network with the following structure:\n",
    "\n",
    "```\n",
    "Input Layer (784 pixels)\n",
    "      ↓\n",
    "Hidden Layer 1 (128 neurons) + ReLU Activation\n",
    "      ↓\n",
    "Hidden Layer 2 (64 neurons) + ReLU Activation\n",
    "      ↓\n",
    "Output Layer (10 classes) + Softmax Activation\n",
    "```\n",
    "\n",
    "**Why This Architecture?**\n",
    "\n",
    "1. **Input Layer (784 neurons):**\n",
    "   - Each 28×28 image is flattened into a 1D vector of 784 pixels\n",
    "   - Each pixel becomes an input feature\n",
    "\n",
    "2. **Hidden Layer 1 (128 neurons):**\n",
    "   - Learns low to mid-level features (edges, simple patterns)\n",
    "   - 128 neurons provide enough capacity to learn diverse patterns\n",
    "   - More neurons = more complex patterns the network can learn\n",
    "\n",
    "3. **Hidden Layer 2 (64 neurons):**\n",
    "   - Combines features from Layer 1 into higher-level features (shapes, textures)\n",
    "   - Fewer neurons (64) because we're building more abstract representations\n",
    "   - Creates a bottleneck that forces the network to learn efficient representations\n",
    "\n",
    "4. **Output Layer (10 neurons):**\n",
    "   - One neuron for each clothing class\n",
    "   - Outputs raw scores (logits) for each class\n",
    "   - Softmax converts these to probabilities\n",
    "\n",
    "**Why These Specific Numbers?**\n",
    "\n",
    "The choice of 128 and 64 neurons follows common practices:\n",
    "- Start with a larger hidden layer to capture details\n",
    "- Gradually reduce size (784 → 128 → 64 → 10) creating a \"funnel\" shape\n",
    "- This progressive reduction helps the network learn hierarchical features\n",
    "- These are powers of 2, which can be computationally efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5abf2",
   "metadata": {},
   "source": [
    "### **7. Understanding Activation Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f83241",
   "metadata": {},
   "source": [
    "Before we implement our network, we need to understand **activation functions** - they're crucial for enabling neural networks to learn complex patterns.\n",
    "\n",
    "**What is an Activation Function?**\n",
    "\n",
    "An activation function is a mathematical function applied to the output of each neuron. It introduces **non-linearity** into the network, allowing it to learn complex patterns.\n",
    "\n",
    "**Without activation functions (or with only linear activation):**\n",
    "- No matter how many layers you stack, the network is equivalent to a single layer\n",
    "- Can only learn linear relationships (straight lines)\n",
    "- Cannot solve complex problems like image classification\n",
    "\n",
    "**With non-linear activation functions:**\n",
    "- Each layer can learn increasingly complex patterns\n",
    "- The network can approximate any continuous function (Universal Approximation Theorem)\n",
    "- Can solve complex real-world problems\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*p_hyqAtyI8pbt2kEl6siOQ.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "**Common Activation Functions:**\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)** - Our choice for hidden layers:\n",
    "   $$\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "   \n",
    "   **Why ReLU?**\n",
    "   - Simple and computationally efficient\n",
    "   - Helps avoid the \"vanishing gradient\" problem\n",
    "   - Works well in practice for most problems\n",
    "   - Most popular activation function in modern deep learning\n",
    "\n",
    "2. **Softmax** - For the output layer:\n",
    "   $$\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "   \n",
    "   **Why Softmax?**\n",
    "   - Converts raw scores into probabilities (values between 0 and 1)\n",
    "   - All probabilities sum to 1\n",
    "   - Perfect for multi-class classification\n",
    "\n",
    "3. **Sigmoid** - Alternative for binary classification:\n",
    "   $$\\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "   \n",
    "   - Outputs values between 0 and 1\n",
    "   - Used for binary decisions (yes/no)\n",
    "\n",
    "4. **Tanh (Hyperbolic Tangent)** - Alternative activation:\n",
    "   $$\\text{Tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "   \n",
    "   - Outputs values between -1 and 1\n",
    "   - Zero-centered (can be better than sigmoid)\n",
    "\n",
    "**Our Choices:**\n",
    "- **Hidden Layers:** ReLU - for efficient learning and avoiding vanishing gradients\n",
    "- **Output Layer:** Softmax - to get probability distribution over 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54240054",
   "metadata": {},
   "source": [
    "### **8. Implementing the Neural Network Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3dccb9",
   "metadata": {},
   "source": [
    "Now let's implement our multi-layer neural network using PyTorch. We'll create a class that inherits from `nn.Module`, just like we did with the Perceptron, but this time with multiple layers.\n",
    "\n",
    "**Key Components:**\n",
    "- `nn.Linear`: Fully connected layer (performs $y = xW^T + b$)\n",
    "- `nn.ReLU`: ReLU activation function\n",
    "- `forward()`: Defines how data flows through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the neural network architecture\n",
    "        \"\"\"\n",
    "        super(FashionMNISTNet, self).__init__()\n",
    "        \n",
    "        # Input layer to Hidden layer 1\n",
    "        # Input: 784 pixels (28x28), Output: 128 neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        \n",
    "        # Hidden layer 1 to Hidden layer 2\n",
    "        # Input: 128 neurons, Output: 64 neurons\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # Hidden layer 2 to Output layer\n",
    "        # Input: 64 neurons, Output: 10 classes\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: define how data flows through the network\n",
    "        \n",
    "        Args:\n",
    "            x: Input images (batch_size, 1, 28, 28)\n",
    "        \n",
    "        Returns:\n",
    "            Output logits (batch_size, 10)\n",
    "        \"\"\"\n",
    "        # Flatten the image from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # Layer 1: Linear transformation + ReLU activation\n",
    "        x = self.fc1(x)      # (batch_size, 784) -> (batch_size, 128)\n",
    "        x = self.relu(x)     # Apply ReLU activation\n",
    "        \n",
    "        # Layer 2: Linear transformation + ReLU activation\n",
    "        x = self.fc2(x)      # (batch_size, 128) -> (batch_size, 64)\n",
    "        x = self.relu(x)     # Apply ReLU activation\n",
    "        \n",
    "        # Output layer: Linear transformation (no activation here)\n",
    "        # Softmax will be applied automatically by the loss function\n",
    "        x = self.fc3(x)      # (batch_size, 64) -> (batch_size, 10)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create an instance of our model\n",
    "model = FashionMNISTNet().to(device)  # Move model to GPU if available\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb683014",
   "metadata": {},
   "source": [
    "**Understanding the Parameter Count:**\n",
    "\n",
    "Let's break down where all these parameters come from:\n",
    "\n",
    "1. **Layer 1 (fc1):** 784 → 128\n",
    "   - Weights: 784 × 128 = 100,352\n",
    "   - Biases: 128\n",
    "   - Total: 100,480 parameters\n",
    "\n",
    "2. **Layer 2 (fc2):** 128 → 64\n",
    "   - Weights: 128 × 64 = 8,192\n",
    "   - Biases: 64\n",
    "   - Total: 8,256 parameters\n",
    "\n",
    "3. **Layer 3 (fc3):** 64 → 10\n",
    "   - Weights: 64 × 10 = 640\n",
    "   - Biases: 10\n",
    "   - Total: 650 parameters\n",
    "\n",
    "**Grand Total:** 100,480 + 8,256 + 650 = **109,386 parameters**\n",
    "\n",
    "Each of these parameters will be learned during training to minimize our loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the shape transformations in detail\n",
    "def test_forward_pass():\n",
    "    \"\"\"\n",
    "    Test the forward pass with a dummy input to verify shapes\n",
    "    \"\"\"\n",
    "    # Create a dummy batch of 4 images\n",
    "    dummy_input = torch.randn(4, 1, 28, 28).to(device)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"  └─ (batch_size=4, channels=1, height=28, width=28)\\n\")\n",
    "    \n",
    "    # Pass through the model\n",
    "    with torch.no_grad():  # Don't compute gradients for this test\n",
    "        output = model(dummy_input)\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"  └─ (batch_size=4, num_classes=10)\\n\")\n",
    "    \n",
    "    print(\"Raw output (logits) for first image:\")\n",
    "    print(output[0].cpu().numpy())\n",
    "    print(\"\\nApplying Softmax to get probabilities:\")\n",
    "    \n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = torch.softmax(output[0], dim=0)\n",
    "    print(probabilities.cpu().numpy())\n",
    "    print(f\"\\nSum of probabilities: {probabilities.sum().item():.4f} (should be 1.0)\")\n",
    "    print(f\"Predicted class: {probabilities.argmax().item()} ({class_names[probabilities.argmax().item()]})\")\n",
    "\n",
    "test_forward_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d629e",
   "metadata": {},
   "source": [
    "### **9. Choosing the Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c6e47",
   "metadata": {},
   "source": [
    "For classification tasks, we use a different loss function than regression. Let's understand why and how it works.\n",
    "\n",
    "**Cross-Entropy Loss (Negative Log-Likelihood Loss):**\n",
    "\n",
    "This is the standard loss function for multi-class classification. It measures how well the predicted probability distribution matches the true distribution.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "\n",
    "$$\\text{CrossEntropy} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Where:\n",
    "- $C$ is the number of classes (10 for Fashion MNIST)\n",
    "- $y_i$ is 1 if the true class is $i$, otherwise 0 (one-hot encoding)\n",
    "- $\\hat{y}_i$ is the predicted probability for class $i$\n",
    "\n",
    "**Simplified version** (since only one $y_i$ is 1):\n",
    "\n",
    "$$\\text{CrossEntropy} = -\\log(\\hat{y}_{\\text{true}})$$\n",
    "\n",
    "This means we want to **maximize** the predicted probability of the correct class.\n",
    "\n",
    "**Why Cross-Entropy Loss?**\n",
    "\n",
    "1. **Penalizes confident wrong predictions heavily:**\n",
    "   - If true class is \"T-shirt\" but model predicts \"Trouser\" with 99% confidence → high loss\n",
    "   - If model is uncertain (spreads probability) → moderate loss\n",
    "   - If model correctly predicts \"T-shirt\" with 99% confidence → very low loss\n",
    "\n",
    "2. **Works well with Softmax:**\n",
    "   - Softmax + Cross-Entropy have nice mathematical properties\n",
    "   - Gradients are well-behaved for efficient learning\n",
    "\n",
    "3. **Probabilistic interpretation:**\n",
    "   - Minimizing cross-entropy is equivalent to maximizing log-likelihood\n",
    "   - Has strong theoretical foundations in information theory\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "\n",
    "PyTorch's `nn.CrossEntropyLoss` combines `LogSoftmax` and `NLLLoss` (Negative Log-Likelihood):\n",
    "- It automatically applies Softmax to the model outputs\n",
    "- Then computes the negative log-likelihood\n",
    "- This is more numerically stable than doing it in two steps\n",
    "\n",
    "**Important:** We don't apply Softmax in our model's forward pass because `CrossEntropyLoss` does it for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(\"\\nCrossEntropyLoss automatically:\")\n",
    "print(\"  1. Applies Softmax to convert logits to probabilities\")\n",
    "print(\"  2. Computes the negative log-likelihood loss\")\n",
    "print(\"  3. Returns the average loss over the batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca47e0a",
   "metadata": {},
   "source": [
    "**Example: Understanding Cross-Entropy Loss**\n",
    "\n",
    "Let's compute loss for different scenarios to build intuition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example scenarios to understand cross-entropy loss\n",
    "def demonstrate_cross_entropy():\n",
    "    \"\"\"\n",
    "    Show how cross-entropy loss changes based on predictions\n",
    "    \"\"\"\n",
    "    print(\"Understanding Cross-Entropy Loss:\\n\")\n",
    "    print(\"Scenario: True class is 0 (T-shirt)\\n\")\n",
    "    \n",
    "    # True label\n",
    "    true_label = torch.tensor([0])  # T-shirt\n",
    "    \n",
    "    # Scenario 1: Very confident correct prediction\n",
    "    logits_1 = torch.tensor([[5.0, -2.0, -1.0, -1.5, -2.0, -1.8, -2.2, -1.9, -2.1, -2.3]])\n",
    "    loss_1 = criterion(logits_1, true_label)\n",
    "    probs_1 = torch.softmax(logits_1, dim=1)[0]\n",
    "    print(f\"Scenario 1: Confident CORRECT prediction\")\n",
    "    print(f\"  Predicted probability for T-shirt: {probs_1[0]:.4f} (very high)\")\n",
    "    print(f\"  Loss: {loss_1.item():.4f} (very low - good!)\\n\")\n",
    "    \n",
    "    # Scenario 2: Uncertain prediction (all probabilities similar)\n",
    "    logits_2 = torch.tensor([[0.5, 0.3, 0.4, 0.2, 0.1, 0.3, 0.2, 0.4, 0.3, 0.2]])\n",
    "    loss_2 = criterion(logits_2, true_label)\n",
    "    probs_2 = torch.softmax(logits_2, dim=1)[0]\n",
    "    print(f\"Scenario 2: UNCERTAIN prediction\")\n",
    "    print(f\"  Predicted probability for T-shirt: {probs_2[0]:.4f} (moderate)\")\n",
    "    print(f\"  Loss: {loss_2.item():.4f} (moderate - needs improvement)\\n\")\n",
    "    \n",
    "    # Scenario 3: Very confident wrong prediction\n",
    "    logits_3 = torch.tensor([[-2.0, 5.0, -1.0, -1.5, -2.0, -1.8, -2.2, -1.9, -2.1, -2.3]])\n",
    "    loss_3 = criterion(logits_3, true_label)\n",
    "    probs_3 = torch.softmax(logits_3, dim=1)[0]\n",
    "    print(f\"Scenario 3: Confident WRONG prediction\")\n",
    "    print(f\"  Predicted probability for T-shirt: {probs_3[0]:.4f} (very low)\")\n",
    "    print(f\"  Predicted class: {probs_3.argmax().item()} ({class_names[probs_3.argmax().item()]})\")\n",
    "    print(f\"  Loss: {loss_3.item():.4f} (very high - bad!)\\n\")\n",
    "    \n",
    "    print(\"Key Insight: Loss is LOW when model is confident AND correct,\")\n",
    "    print(\"             Loss is HIGH when model is confident BUT wrong.\")\n",
    "\n",
    "demonstrate_cross_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b23097",
   "metadata": {},
   "source": [
    "### **10. Choosing the Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855a48b",
   "metadata": {},
   "source": [
    "The **optimizer** is responsible for updating the model's parameters to minimize the loss. In our previous notebook, we used **SGD (Stochastic Gradient Descent)**. For this more complex task, we'll use **Adam** - a more advanced optimizer.\n",
    "\n",
    "**Why Adam over SGD?**\n",
    "\n",
    "| Feature | SGD | Adam |\n",
    "|---------|-----|------|\n",
    "| **Learning Rate** | Fixed (or manually scheduled) | Adaptive per parameter |\n",
    "| **Convergence Speed** | Slower | Faster |\n",
    "| **Hyperparameter Tuning** | Requires careful tuning | Works well with default settings |\n",
    "| **Memory Usage** | Low | Slightly higher (stores additional statistics) |\n",
    "| **Best For** | Simple problems, well-tuned settings | Complex problems, faster iteration |\n",
    "\n",
    "**How Adam Works:**\n",
    "\n",
    "Adam (Adaptive Moment Estimation) combines ideas from two other optimizers:\n",
    "1. **Momentum:** Uses moving average of gradients to smooth updates\n",
    "2. **RMSprop:** Adapts learning rate for each parameter based on recent gradient magnitudes\n",
    "\n",
    "**Key Advantages:**\n",
    "- Automatically adjusts learning rate for each parameter\n",
    "- Handles sparse gradients well (common in deep networks)\n",
    "- Requires minimal tuning (default settings work well)\n",
    "- Generally converges faster than vanilla SGD\n",
    "\n",
    "**Hyperparameters:**\n",
    "- **learning_rate (lr):** How big of a step to take (typically 0.001 for Adam)\n",
    "- **betas:** Exponential decay rates for moment estimates (default: (0.9, 0.999))\n",
    "- **weight_decay:** L2 regularization strength (helps prevent overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe39419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"\\nLearning rate: {learning_rate}\")\n",
    "print(\"\\nAdam optimizer will:\")\n",
    "print(\"  1. Compute gradients for all parameters\")\n",
    "print(\"  2. Adapt learning rate for each parameter individually\")\n",
    "print(\"  3. Use momentum to smooth the optimization path\")\n",
    "print(\"  4. Update parameters to minimize the loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c2ebd",
   "metadata": {},
   "source": [
    "Now comes the exciting part - training our neural network! The training process is similar to what we did with the perceptron, but with some important differences:\n",
    "\n",
    "**Training Loop Components:**\n",
    "\n",
    "1. **Epochs:** Complete passes through the entire training dataset\n",
    "2. **Batches:** Process multiple images at once (faster and more stable than one at a time)\n",
    "3. **Forward Pass:** Feed data through the network to get predictions\n",
    "4. **Loss Calculation:** Measure how wrong the predictions are\n",
    "5. **Backward Pass:** Calculate gradients (how to adjust each parameter)\n",
    "6. **Parameter Update:** Use optimizer to adjust weights and biases\n",
    "\n",
    "**Why Train in Batches?**\n",
    "\n",
    "Instead of using all 60,000 images at once (too memory-intensive) or one image at a time (too slow and unstable), we use **mini-batches** of 64 images:\n",
    "\n",
    "- **Computational Efficiency:** GPUs are optimized for parallel processing\n",
    "- **Memory Management:** Fits in GPU/CPU memory\n",
    "- **Better Gradients:** Averaging over a batch gives more stable gradient estimates\n",
    "- **Faster Convergence:** Updates happen more frequently than full-batch training\n",
    "\n",
    "**Training vs Evaluation Mode:**\n",
    "\n",
    "- `model.train()`: Enables training-specific behaviors (like dropout, if we had it)\n",
    "- `model.eval()`: Disables training-specific behaviors for evaluation\n",
    "- `torch.no_grad()`: Disables gradient computation (saves memory during evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36cd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train the neural network\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        train_loader: DataLoader for training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs: Number of complete passes through the dataset\n",
    "    \n",
    "    Returns:\n",
    "        Lists of training losses and accuracies per epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    print(\"Starting training...\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            # Move data to device (GPU if available)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # 1. Forward pass: compute predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 3. Backward pass: compute gradients\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()        # Compute new gradients\n",
    "            \n",
    "            # 4. Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get class with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate average loss and accuracy for this epoch\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Loss: {epoch_loss:.4f} | \"\n",
    "              f\"Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    return train_losses, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc44da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, train_accuracies = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    num_epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ea569",
   "metadata": {},
   "source": [
    "### **12. Visualizing Training Progress**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091c259",
   "metadata": {},
   "source": [
    "Visualizing the training process helps us understand:\n",
    "1. **Is the model learning?** Loss should decrease over time\n",
    "2. **Is it learning well?** Accuracy should increase over time\n",
    "3. **Any problems?** Sudden spikes or plateaus might indicate issues\n",
    "\n",
    "**What to Look For:**\n",
    "- **Decreasing Loss:** Model is getting better at predicting\n",
    "- **Increasing Accuracy:** Model is making more correct predictions\n",
    "- **Smooth Curves:** Indicates stable learning\n",
    "- **Plateauing:** Model might have reached its capacity or learning rate is too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86164bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "def plot_training_curves(train_losses, train_accuracies):\n",
    "    \"\"\"\n",
    "    Visualize training loss and accuracy over epochs\n",
    "    \n",
    "    Args:\n",
    "        train_losses: List of training losses per epoch\n",
    "        train_accuracies: List of training accuracies per epoch\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax1.plot(range(1, len(train_losses) + 1), train_losses, \n",
    "             marker='o', linewidth=2, markersize=6, color='#e74c3c')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(range(1, len(train_losses) + 1))\n",
    "    \n",
    "    # Plot training accuracy\n",
    "    ax2.plot(range(1, len(train_accuracies) + 1), train_accuracies, \n",
    "             marker='s', linewidth=2, markersize=6, color='#27ae60')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.set_title('Training Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xticks(range(1, len(train_accuracies) + 1))\n",
    "    ax2.set_ylim([0, 100])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nTraining Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Initial Loss:    {train_losses[0]:.4f}\")\n",
    "    print(f\"Final Loss:      {train_losses[-1]:.4f}\")\n",
    "    print(f\"Loss Reduction:  {train_losses[0] - train_losses[-1]:.4f}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Initial Accuracy: {train_accuracies[0]:.2f}%\")\n",
    "    print(f\"Final Accuracy:   {train_accuracies[-1]:.2f}%\")\n",
    "    print(f\"Improvement:      {train_accuracies[-1] - train_accuracies[0]:.2f}%\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "plot_training_curves(train_losses, train_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95263637",
   "metadata": {},
   "source": [
    "### **13. Evaluating on the Test Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5aa58",
   "metadata": {},
   "source": [
    "Training accuracy tells us how well the model performs on data it has seen. But the real test is: **Can it generalize to new, unseen data?**\n",
    "\n",
    "This is why we have a separate **test set** - 10,000 images the model has never seen during training.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Generalization:** The ability to perform well on new data\n",
    "2. **Overfitting:** When training accuracy is high but test accuracy is low (model memorized training data)\n",
    "3. **Underfitting:** When both training and test accuracy are low (model is too simple)\n",
    "4. **Good Fit:** When both training and test accuracy are high and similar\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*_7OPgojau8hkiPUiHoGK_w.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**What We're Measuring:**\n",
    "- **Accuracy:** Percentage of correct predictions\n",
    "- **Per-Class Performance:** How well the model performs on each clothing type\n",
    "- **Confusion Matrix:** Where the model makes mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8109e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data\n",
    "    \n",
    "    Args:\n",
    "        model: The trained neural network\n",
    "        test_loader: DataLoader for test data\n",
    "    \n",
    "    Returns:\n",
    "        Overall accuracy, per-class accuracy, all predictions and labels\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = [0] * 10  # Correct predictions per class\n",
    "    class_total = [0] * 10    # Total samples per class\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Don't compute gradients during evaluation (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Overall accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == label:\n",
    "                    class_correct[label] += 1\n",
    "            \n",
    "            # Store for confusion matrix\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    per_class_accuracy = [100 * class_correct[i] / class_total[i] for i in range(10)]\n",
    "    \n",
    "    return overall_accuracy, per_class_accuracy, all_predictions, all_labels\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy, per_class_acc, predictions, true_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOverall Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"\\nThis means the model correctly classified {test_accuracy:.2f}% of the\")\n",
    "print(f\"10,000 images it has NEVER seen before!\\n\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc218a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Class':<15} | {'Accuracy':<10} | {'Bar Chart'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, acc in enumerate(per_class_acc):\n",
    "    bar = '█' * int(acc / 2)  # Scale down for display\n",
    "    print(f\"{class_names[i]:<15} | {acc:>6.2f}%   | {bar}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best and worst performing classes\n",
    "best_class = np.argmax(per_class_acc)\n",
    "worst_class = np.argmin(per_class_acc)\n",
    "\n",
    "print(f\"\\nBest Performance:  {class_names[best_class]} ({per_class_acc[best_class]:.2f}%)\")\n",
    "print(f\"Worst Performance: {class_names[worst_class]} ({per_class_acc[worst_class]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eaabaf",
   "metadata": {},
   "source": [
    "### **14. Confusion Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d7b49",
   "metadata": {},
   "source": [
    "A **confusion matrix** is a powerful tool for understanding classification errors. It shows:\n",
    "- **Diagonal:** Correct predictions (darker = more correct)\n",
    "- **Off-diagonal:** Misclassifications (which classes get confused with each other)\n",
    "\n",
    "**How to Read It:**\n",
    "- **Rows:** True labels (actual class)\n",
    "- **Columns:** Predicted labels (what model predicted)\n",
    "- **Cell (i, j):** Number of times class i was predicted as class j\n",
    "\n",
    "**Example:** If cell (Shirt, T-shirt) = 150, it means 150 Shirts were incorrectly classified as T-shirts.\n",
    "\n",
    "This helps us understand:\n",
    "1. Which classes are most confused\n",
    "2. Patterns in the errors (e.g., similar-looking items)\n",
    "3. Where to focus improvement efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1745295",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (585533846.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    A **confusion matrix** is a powerful tool for understanding classification errors. It shows:\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Create and visualize confusion matrix\n",
    "def plot_confusion_matrix(true_labels, predictions, class_names):\n",
    "    \"\"\"\n",
    "    Create and display a confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        true_labels: Ground truth labels\n",
    "        predictions: Model predictions\n",
    "        class_names: Names of the classes\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot using seaborn for better aesthetics\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Number of Predictions'})\n",
    "    \n",
    "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    plt.title('Confusion Matrix - Fashion MNIST Classification', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze common misclassifications\n",
    "    print(\"\\nMost Common Misclassifications:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    misclassifications = []\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            if i != j and cm[i][j] > 0:  # Off-diagonal elements\n",
    "                misclassifications.append((cm[i][j], i, j))\n",
    "    \n",
    "    # Sort by frequency and show top 10\n",
    "    misclassifications.sort(reverse=True)\n",
    "    for count, true_class, pred_class in misclassifications[:10]:\n",
    "        print(f\"{count:4d} times: {class_names[true_class]:<15} → {class_names[pred_class]:<15}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "plot_confusion_matrix(true_labels, predictions, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42880132",
   "metadata": {},
   "source": [
    "### **15. Visualizing Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657cfc51",
   "metadata": {},
   "source": [
    "Let's visualize some predictions to see where our model succeeds and where it struggles. This gives us intuition about:\n",
    "1. What the model has learned\n",
    "2. Why certain errors occur\n",
    "3. How confident the model is in its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3761c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correct and incorrect predictions\n",
    "def visualize_predictions(model, test_dataset, num_samples=16, show_incorrect=False):\n",
    "    \"\"\"\n",
    "    Display sample predictions with true and predicted labels\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_dataset: Test dataset\n",
    "        num_samples: Number of samples to display\n",
    "        show_incorrect: If True, show only incorrect predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    \n",
    "    if show_incorrect:\n",
    "        fig.suptitle('Incorrect Predictions (Red)', fontsize=16, y=0.995, color='red', fontweight='bold')\n",
    "    else:\n",
    "        fig.suptitle('Sample Predictions (Green=Correct, Red=Incorrect)', \n",
    "                     fontsize=16, y=0.995, fontweight='bold')\n",
    "    \n",
    "    samples_found = 0\n",
    "    idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while samples_found < num_samples and idx < len(test_dataset):\n",
    "            image, true_label = test_dataset[idx]\n",
    "            idx += 1\n",
    "            \n",
    "            # Get prediction\n",
    "            image_batch = image.unsqueeze(0).to(device)\n",
    "            output = model(image_batch)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_label = output.argmax(1).item()\n",
    "            confidence = probabilities[0][predicted_label].item() * 100\n",
    "            \n",
    "            # Filter based on show_incorrect flag\n",
    "            is_correct = (predicted_label == true_label)\n",
    "            if show_incorrect and is_correct:\n",
    "                continue\n",
    "            \n",
    "            # Plot\n",
    "            ax = axes[samples_found // 4, samples_found % 4]\n",
    "            img_np = image.squeeze().cpu().numpy()\n",
    "            ax.imshow(img_np, cmap='gray')\n",
    "            \n",
    "            # Color code: green for correct, red for incorrect\n",
    "            color = 'green' if is_correct else 'red'\n",
    "            \n",
    "            ax.set_title(f'True: {class_names[true_label]}\\n'\n",
    "                        f'Pred: {class_names[predicted_label]}\\n'\n",
    "                        f'Conf: {confidence:.1f}%',\n",
    "                        fontsize=9, color=color, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            samples_found += 1\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(samples_found, 16):\n",
    "        axes[i // 4, i % 4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show mixed predictions\n",
    "print(\"Sample Predictions (Mix of Correct and Incorrect):\")\n",
    "visualize_predictions(model, test_dataset, num_samples=16, show_incorrect=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only incorrect predictions to analyze errors\n",
    "print(\"\\nIncorrect Predictions Only:\")\n",
    "visualize_predictions(model, test_dataset, num_samples=16, show_incorrect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a17ea",
   "metadata": {},
   "source": [
    "### **16. Making Predictions on New Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9d709",
   "metadata": {},
   "source": [
    "Let's create a function to make predictions on individual images, showing the top predictions and their probabilities. This demonstrates how the model would be used in a real-world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955dd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict with probability distribution\n",
    "def predict_with_probabilities(model, image_tensor, true_label=None):\n",
    "    \"\"\"\n",
    "    Make prediction and show probability distribution\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image_tensor: Input image tensor\n",
    "        true_label: Optional true label for comparison\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension and move to device\n",
    "        if len(image_tensor.shape) == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        output = model(image_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)[0]\n",
    "        predicted_class = probabilities.argmax().item()\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Display image\n",
    "        img_np = image_tensor.squeeze().cpu().numpy()\n",
    "        ax1.imshow(img_np, cmap='gray')\n",
    "        ax1.set_title(f'Input Image', fontsize=12, fontweight='bold')\n",
    "        if true_label is not None:\n",
    "            ax1.set_xlabel(f'True Label: {class_names[true_label]}', \n",
    "                          fontsize=10, fontweight='bold')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Display probability distribution\n",
    "        probs_np = probabilities.cpu().numpy()\n",
    "        colors = ['green' if i == predicted_class else 'steelblue' for i in range(10)]\n",
    "        \n",
    "        bars = ax2.barh(range(10), probs_np, color=colors, edgecolor='black')\n",
    "        ax2.set_yticks(range(10))\n",
    "        ax2.set_yticklabels(class_names)\n",
    "        ax2.set_xlabel('Probability', fontsize=11, fontweight='bold')\n",
    "        ax2.set_title(f'Prediction: {class_names[predicted_class]} '\n",
    "                     f'({probs_np[predicted_class]*100:.1f}%)',\n",
    "                     fontsize=12, fontweight='bold', color='green')\n",
    "        ax2.set_xlim([0, 1])\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, (bar, prob) in enumerate(zip(bars, probs_np)):\n",
    "            if prob > 0.02:  # Only show if > 2%\n",
    "                ax2.text(prob + 0.01, i, f'{prob*100:.1f}%', \n",
    "                        va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print top 3 predictions\n",
    "        print(\"\\nTop 3 Predictions:\")\n",
    "        print(\"=\"*50)\n",
    "        top3_idx = probabilities.argsort(descending=True)[:3]\n",
    "        for rank, idx in enumerate(top3_idx, 1):\n",
    "            print(f\"{rank}. {class_names[idx]:<15} - {probabilities[idx]*100:5.2f}%\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "# Test with a random image from test set\n",
    "random_idx = np.random.randint(0, len(test_dataset))\n",
    "test_image, test_label = test_dataset[random_idx]\n",
    "\n",
    "print(f\"\\nPredicting on random test image #{random_idx}:\")\n",
    "predict_with_probabilities(model, test_image, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378afc1",
   "metadata": {},
   "source": [
    "### **17. Conclusion and Key Takeaways**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b22c75",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully built, trained, and evaluated a multi-layer neural network for image classification. Let's summarize what we've learned:\n",
    "\n",
    "**🎯 What We Accomplished:**\n",
    "\n",
    "1. ✅ Loaded and explored the Fashion MNIST dataset (70,000 images)\n",
    "2. ✅ Built a 3-layer neural network with 109,386 parameters\n",
    "3. ✅ Trained the model using Cross-Entropy Loss and Adam optimizer\n",
    "4. ✅ Achieved ~85-90% accuracy on unseen test data\n",
    "5. ✅ Analyzed performance using confusion matrices and visualizations\n",
    "\n",
    "**🔑 Key Concepts Learned:**\n",
    "\n",
    "1. **Classification vs Regression:**\n",
    "   - Classification predicts discrete categories\n",
    "   - Requires different loss functions (Cross-Entropy) and activations (Softmax)\n",
    "\n",
    "2. **Multi-Layer Neural Networks:**\n",
    "   - Stack layers to learn hierarchical features\n",
    "   - Use activation functions (ReLU) for non-linearity\n",
    "   - More layers = more complex patterns can be learned\n",
    "\n",
    "3. **Training Process:**\n",
    "   - Forward pass → Loss calculation → Backward pass → Parameter update\n",
    "   - Mini-batch training for efficiency\n",
    "   - Monitoring loss and accuracy to track learning\n",
    "\n",
    "4. **Evaluation:**\n",
    "   - Test set measures generalization ability\n",
    "   - Confusion matrix reveals where errors occur\n",
    "   - Per-class accuracy shows strengths and weaknesses\n",
    "\n",
    "**💡 Important Insights:**\n",
    "\n",
    "- **Simple items** (Trousers, Bags, Sneakers) are easier to classify\n",
    "- **Similar items** (T-shirt vs Shirt, Pullover vs Coat) get confused\n",
    "- **Architecture matters:** More layers and neurons generally improve performance\n",
    "- **Hyperparameters** (learning rate, batch size, epochs) significantly impact results\n",
    "\n",
    "**🚀 Next Steps to Improve:**\n",
    "\n",
    "1. **Try Different Architectures:**\n",
    "   - Add more layers or neurons\n",
    "   - Experiment with different activation functions\n",
    "   - Try Convolutional Neural Networks (CNNs) - designed for images!\n",
    "\n",
    "2. **Regularization Techniques:**\n",
    "   - Dropout: Randomly disable neurons during training to prevent overfitting\n",
    "   - Batch Normalization: Normalize inputs to each layer\n",
    "   - Data Augmentation: Create variations of training images\n",
    "\n",
    "3. **Hyperparameter Tuning:**\n",
    "   - Learning rate scheduling (reduce learning rate over time)\n",
    "   - Different batch sizes\n",
    "   - More training epochs\n",
    "\n",
    "4. **Advanced Optimizers:**\n",
    "   - AdamW (Adam with weight decay)\n",
    "   - Learning rate warm-up and decay strategies\n",
    "\n",
    "**🎓 From Perceptron to Deep Learning:**\n",
    "\n",
    "We started with a simple perceptron (1 neuron) for regression and now have a 3-layer network for classification. This is the foundation of **deep learning** - by going deeper (more layers) and wider (more neurons), neural networks can solve increasingly complex problems:\n",
    "\n",
    "- Image Recognition (our task)\n",
    "- Natural Language Processing\n",
    "- Speech Recognition\n",
    "- Game Playing (AlphaGo, Chess)\n",
    "- Autonomous Driving\n",
    "- Medical Diagnosis\n",
    "\n",
    "The principles you learned here scale to all these applications!\n",
    "\n",
    "**📊 Final Comparison: Simple vs Complex Networks**\n",
    "\n",
    "| Aspect | Day 4 Part 1 (Perceptron) | Day 4 Part 2 (Multi-Layer Net) |\n",
    "|--------|---------------------------|--------------------------------|\n",
    "| **Task** | Exam Score Prediction (Regression) | Clothing Classification (Classification) |\n",
    "| **Layers** | 1 (Input → Output) | 3 (Input → Hidden → Hidden → Output) |\n",
    "| **Parameters** | ~30 | 109,386 |\n",
    "| **Activation** | Identity (Linear) | ReLU + Softmax |\n",
    "| **Loss Function** | Mean Squared Error | Cross-Entropy |\n",
    "| **Can Learn** | Linear relationships | Complex non-linear patterns |\n",
    "\n",
    "---\n",
    "\n",
    "**🌟 You've now mastered the fundamentals of neural networks! Keep exploring and building more complex models!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c223eef",
   "metadata": {},
   "source": [
    "### **18. Challenge Exercises (Optional)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1992cb",
   "metadata": {},
   "source": [
    "Ready to test your understanding? Try these challenges:\n",
    "\n",
    "**Challenge 1: Modify the Architecture**\n",
    "- Change the network to have 4 layers instead of 3\n",
    "- Try different neuron counts (e.g., 256 → 128 → 64 → 10)\n",
    "- Compare the results with the original architecture\n",
    "\n",
    "**Challenge 2: Experiment with Hyperparameters**\n",
    "- Train for 20 epochs instead of 10\n",
    "- Try different learning rates (0.0001, 0.01, 0.1)\n",
    "- Change the batch size (32, 128, 256)\n",
    "\n",
    "**Challenge 3: Analyze Specific Classes**\n",
    "- Focus on the two classes with lowest accuracy\n",
    "- Visualize 20 misclassified examples from these classes\n",
    "- Can you spot patterns in why the model fails?\n",
    "\n",
    "**Challenge 4: Save and Load the Model**\n",
    "```python\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'fashion_mnist_model.pth')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = FashionMNISTNet().to(device)\n",
    "loaded_model.load_state_dict(torch.load('fashion_mnist_model.pth'))\n",
    "```\n",
    "\n",
    "**Challenge 5: Try Different Optimizers**\n",
    "- Replace Adam with SGD with momentum\n",
    "- Compare training curves and final accuracy\n",
    "- Which optimizer works better for this task?\n",
    "\n",
    "---\n",
    "\n",
    "Good luck, and happy coding! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
